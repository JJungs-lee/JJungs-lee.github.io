var store = [{
        "title": "Finding offset of a struct elemnet in c",
        "excerpt":"If it is a simple structure, the position and size of member variables can be intuitively known.  // Differences depending on the compiler. struct Sample {   int sample1;    // size 4   short sample2;  // size 2   double sample3; // size 8 }  However, in complex structures it is difficult to calculate every day. So in that case, you can use the macro. The macro is offsetof. You can define it as below and use it.  #define OFFSETOF(type, member) &amp;((type *)0)-&gt;member)   But standard libray or kernel has a header file(stddef.h).   This macro already defined, and see the man page to use it.   OFFSETOF(3)           Linux Programmer's Manual           OFFSETOF(3)  NAME        offsetof - offset of a structure member  SYNOPSIS        #include &lt;stddef.h&gt;         size_t offsetof(type, member);  DESCRIPTION        The  macro  offsetof()  returns the offset of the field member         from the start of the structure type.         This macro is useful because the sizes of the fields  that         compose a structure can vary across implementations, and         compilers may insert different numbers of padding bytes between         fields. Consequently, an element's offset is not necessarily         given by the sum of the sizes of the previous elements.         A compiler error will result if member is not aligned to a byte         bound‐ary (i.e., it is a bit field).  RETURN VALUE        offsetof()  returns  the  offset  of  the given member within         the given type, in units of bytes.  CONFORMING TO        POSIX.1-2001, POSIX.1-2008, C89, C99.  EXAMPLE        On a Linux/i386 system, when compiled using the default gcc(1)         options, the program below produces the following output:            $ ./a.out           offsets: i=0; c=4; d=8 a=16           sizeof(struct s)=16     Program source        #include &lt;stddef.h&gt;        #include &lt;stdio.h&gt;        #include &lt;stdlib.h&gt;         int main(void)        {            struct s {                int i;                char c;                double d;                char a[];            };             /* Output is compiler dependent */            printf(\"offsets: i=%zd; c=%zd; d=%zd a=%zd\\n\",                    offsetof(struct s, i), offsetof(struct s, c),                    offsetof(struct s, d), offsetof(struct s, a));            printf(\"sizeof(struct s)=%zd\\n\", sizeof(struct s));             exit(EXIT_SUCCESS);        }  COLOPHON        This page is part of release 4.04 of the Linux man-pages project.        A description of the project, information about reporting bugs,        and the latest version of this page, can be found at        http://www.kernel.org/doc/man-pages/.  GNU                           2015-08-08                     OFFSETOF(3)   In the kernel, it is declared in the following path(include/linux/stddef.h), and It is used a lot in the kernel.   /* SPDX-License-Identifier: GPL-2.0 */ #ifndef _LINUX_STDDEF_H #define _LINUX_STDDEF_H  #include &lt;uapi/linux/stddef.h&gt;  #undef NULL #define NULL ((void *)0)  enum { \tfalse\t= 0, \ttrue\t= 1 };  #undef offsetof #ifdef __compiler_offsetof #define offsetof(TYPE, MEMBER)\t__compiler_offsetof(TYPE, MEMBER) #else #define offsetof(TYPE, MEMBER)\t((size_t)&amp;((TYPE *)0)-&gt;MEMBER) #endif  /**  * sizeof_field(TYPE, MEMBER)  *  * @TYPE: The structure containing the field of interest  * @MEMBER: The field to return the size of  */ #define sizeof_field(TYPE, MEMBER) sizeof((((TYPE *)0)-&gt;MEMBER))  /**  * offsetofend(TYPE, MEMBER)  *  * @TYPE: The type of the structure  * @MEMBER: The member within the structure to get the end offset of  */ #define offsetofend(TYPE, MEMBER) \\ \t(offsetof(TYPE, MEMBER)\t+ sizeof_field(TYPE, MEMBER))  #endif   Let’s do nice programming using the macro.  ","categories": ["kernel"],
        "tags": ["C","C++","offsetof"],
        "url": "https://jjungs-lee.github.io//kernel/2020-05-03-finding%20offset%20of%20a%20struct%20elemnet%20in%20c/",
        "teaser":null},{
        "title": "How to make kernel debugging easy?",
        "excerpt":"When we analyze the kernel, there are many times when we want to know the behavior of kernel code.  For example, who called the function, what value the variable has, what happens to the call stack, etc.   So I’m going to write down a few things and how to use them.   printk  When we first learn a programming language, we create a program that printing Hello world.   The C language uses printf and the Java language uses System.out.println.  These are called STL(Standard Template Library) output.  It’s the most basic part and the most powerful.  In the kernel, you can print through a function called printk. You can find out more by entering the link.   Integer types  Below is a table of the type and format. C language developer would have seen it often. The kernel’s printf does not support %n.  For obvious reasons, floating point formats (%e,%f, %g, %a) are also not recognized.  Use of any unsupported specifier or length qualifier results in a WARN and early return from vsnprintf.  printk(\"test: sector number/total blocks: %llu/%llu\\n\",         (unsigned long long)sector, (unsigned long long)blockcount);                  If variable is of Type       use printk format specifier                       int       %d or %x                 unsigned int       %u or %x                 long       %ld or %lx                 unsigned long       %ld or %lx                 long long       %lld or %llx                 unsigned long long       %llu or %llx                 size_t       %zu or %zx                 ssize_t       %zd or %zx                 s32       %d or %x                 u32       %u or %x                 s64       %lld or %llx                 u64       %llu or %llx           However, it has become a stronger format than this.  You can print pointer Address, functions and symbol/function pointer etc. Debugging is much easier if you know this format.   Pointer Types / Unmodified Addresses  Pointers printed without a specifier extension (i.e unadorned %p) are hashed to give a unique identifier without leaking kernel addresses to user space.  On 64 bit machines the first 32 bits are zeroed. If you really want the address use %px.   Please consider whether or not you are leaking sensitive information about the Kernel layout in memory before printing pointers with %px.  %px is functionally equivalent to %lx. %px is preferred to %lx because it is more uniquely grep’able.  If, in the future, we need to modify the way the Kernel handles printing pointers it will be nice to be able to find the call sites.   %p\tabcdef12 or 00000000abcdef12 %px\t01234567 or 0123456789abcdef   Symbols/Function Pointers  %pf\tversatile_init %pF\tversatile_init+0x0/0x110 %ps\tversatile_init %pS\tversatile_init+0x0/0x110 %pSR\tversatile_init+0x9/0x110         (with __builtin_extract_return_addr() translation) %pB\tprev_fn_of_versatile_init+0x88/0x88  The F and f specifiers are for printing function pointers, for example, f-&gt;func, &amp;gettimeofday.  They have the same result as S and s specifiers. But they do an extra conversion on ia64, ppc64 and parisc64 architectures where the function pointers are actually function descriptors.   The S and s specifiers can be used for printing symbols from direct addresses,  for example, __builtin_return_address(0), (void *)regs-&gt;ip. They result in the symbol name with (S) or without (s) offsets.  If KALLSYMS are disabled then the symbol address is printed instead.   The B specifier results in the symbol name with offsets and should be used when printing stack backtraces.  The specifier takes into consideration the effect of compiler optimisations which may occur when tail-calls are used and marked with the noreturn GCC attribute.   printk(\"Going to call: %pF\\n\", gettimeofday); printk(\"Going to call: %pF\\n\", p-&gt;func);  // #define _RET_IP_  (unsigned long)__builtin_return_address(0) printk(\"%s: called from %pS\\n\", __func__, (void *)_RET_IP_); printk(\"%s: called from %pS\\n\", __func__,                 (void *)__builtin_return_address(0));  printk(\"Faulted at %pS\\n\", (void *)regs-&gt;ip); printk(\" %s%pB\\n\", (reliable ? \"\" : \"? \"), (void *)*stack);   Kernel Pointers   %pK\t01234567 or 0123456789abcdef   For printing kernel pointers which should be hidden from unprivileged users. The behaviour of %pK depends on the kptr_restrict sysctl - see Documentation/sysctl/kernel.txt for more details.   Struct Resources  %pr\t[mem 0x60000000-0x6fffffff flags 0x2200] or \t[mem 0x0000000060000000-0x000000006fffffff flags 0x2200] %pR\t[mem 0x60000000-0x6fffffff pref] or \t[mem 0x0000000060000000-0x000000006fffffff pref]   For printing struct resources. The R and r specifiers result in a printed resource with (R) or without (r) a decoded flags member. Passed by reference.   dump_stack  In addition to printk, the kernel provides the ability to show kernel behavior through kernel logs.  In other words, if you call the dump_stack() function supported by the kernel, you can view the call-stack as a kernel log. Using the dump_stack() function is simple.  Just add the dump_stack() function to the code you want to see the call stack in the kernel log.   To call the dump_stack() function, you need to add the “linux/kernel.h” header file at the top of the code as follows:   #include &lt;linux/kernel.h&gt;   Let’s look at the declaration section of the dump_stack() function.  /**  * dump_stack - dump the current task information and its stack trace  *  * Architectures can override this implementation by implementing its own  */ asmlinkage __visible void dump_stack(void);   Both the argument and return value types are void.  Just add the dump_stack() function anywhere in the kernel source code.   CPU: 1 PID: 1022 Comm: cat Tainted: G         C        4.19.127-v7+ #2 Hardware name: BCM2835 [&lt;80112018&gt;] (unwind_backtrace) from [&lt;8010d364&gt;] (show_stack+0x20/0x24) [&lt;8010d364&gt;] (show_stack) from [&lt;80851ab4&gt;] (dump_stack+0xd8/0x11c) [&lt;80851ab4&gt;] (dump_stack) from [&lt;80719280&gt;] (jhs_debug_read+0x18/0x20) [&lt;80719280&gt;] (jhs_debug_read) from [&lt;803484a8&gt;] (proc_reg_read+0x70/0x98) [&lt;803484a8&gt;] (proc_reg_read) from [&lt;802d0628&gt;] (__vfs_read+0x48/0x16c) [&lt;802d0628&gt;] (__vfs_read) from [&lt;802d07e8&gt;] (vfs_read+0x9c/0x164) [&lt;802d07e8&gt;] (vfs_read) from [&lt;802d0e38&gt;] (ksys_read+0x74/0xe8) [&lt;802d0e38&gt;] (ksys_read) from [&lt;802d0ec4&gt;] (sys_read+0x18/0x1c) [&lt;802d0ec4&gt;] (sys_read) from [&lt;80101000&gt;] (ret_fast_syscall+0x0/0x28)   panic  If there is a problem with the kernel, the kernel calls the panic function. And it prints various information such as call-stack and register, and prints reboot or panic message on the screen.   To call the panic() function, you need to add the “linux/kernel.h” header file at the top of the code as follows:   #include &lt;linux/kernel.h&gt;   Let’s look at the declaration section of the panic() function.  /**  *\tpanic - halt the system  *\t@fmt: The text string to print  *  *\tDisplay a message, then perform cleanups.  *  *\tThis function never returns.  */ void panic(const char *fmt, ...)  Because kernel internal operation are vast, it can be difficult to see the behavior of the code I added. In that case, put panic() after the code you want to debug, panic the device. And extract the RAM dump or log for analysis.   The annoying thing is that you have to build it with a panic code and work carefully.  ","categories": ["kernel"],
        "tags": ["debugging","printk","Pointers","dump_stack","panic"],
        "url": "https://jjungs-lee.github.io//kernel/2020-07-09-How%20to%20make%20kernel%20debugging%20easy/",
        "teaser":null},{
        "title": "RUST : 0. Overview",
        "excerpt":"I will post a summary of RUST   Plz follow the below link. Thx :D   link     Getting Started   Programming a Guessing Game   3. Common Programming Concepts   4. Understanding Ownership   5. Using Structs to Structure Related Data   6. Enums and Pattern Matching   7. Managing Growing Projects with Packages, Crates, and Modules   8. Common Collections   9. Error Handling   10. Generic Types, Traits, and Lifetimes   11. Writing Automated Tests   12. An I/O Project: Building a Command Line Program   13. Functional Language Features: Iterators and Closures   More about Cargo and Crates.io   Smart Pointers   Fearless Concurrency   Object Oriented Programming Features of Rust   Patterns and Matching   Advanced Features   Final Project: Building a Multithreaded Web Server   Appendix    [Reference]   eng ver : https://doc.rust-lang.org/book/title-page.html   kor ver : https://rinthel.github.io/rust-lang-book-ko  ","categories": ["RUST Language"],
        "tags": ["Summery","Overview"],
        "url": "https://jjungs-lee.github.io//rust/0.Overview",
        "teaser":null},{
        "title": "RUST : 3. Common Programming Concepts",
        "excerpt":"Variables and Mutability  Default variables are immutable.  If you want mutable variable, can make them mutable by adding mut in front of the variable name.  fn main() {     let mut x = 5;     println!(\"The value of x is: {}\", x);     x = 6;     println!(\"The value of x is: {}\", x); }  First, you aren’t allowed to use mut with constants. Constants aren’t just immutable by default—they’re always immutable.  You declare constants using the const keyword instead of the let keyword, and the type of the value must be annotated.  const MAX_POINTS: u32 = 100_000;  Note: Rust’s naming convention for constants is to use all uppercase with underscores between words, and underscores can be inserted in numeric literals to improve readability   Data Types  Keep in mind that Rust is a statically typed language, which means that it must know the types of all variables at compile time.   Table 3-1: Integer Types in Rust                  Length       Signed       Unsigned                       8-bit       i8       u8                 16-bit       i16       u16                 32-bit       i32       u32                 64-bit       i64       u64                 arch       isize       usize           Table 3-2:Integer Literals in Rust                  Number literals       Example                       Decimal       98_222                 Hex       0xff                 Octal       0o77                 Binary       0b1111_0000                 Byte (u8 only)       b’A’           // Floating-Point(default type is f64) let x = 2.0; // f64 let y: f32 = 3.0; // f32  // Numeric Operations let sum = 5 + 10; let difference = 95.5 - 4.3; let product = 4 * 30; let quotient = 56.7 / 32.2; let remainder = 43 % 5;  // Boolean let t = true; let f: bool = false; // with explicit type annotation  // Character let c = 'z'; let z = 'ℤ'; let heart_eyed_cat = '😻';  // Tuple let tup = (500, 6.4, 1); let (x, y, z) = tup; println!(\"The value of y is: {}\", y);  // Access a tuple element directly by using a period(.) // followed by the index of the value we want to access let x: (i32, f64, u8) = (500, 6.4, 1); let five_hundred = x.0; let six_point_four = x.1; let one = x.2;  // array let a = [1, 2, 3, 4, 5]; let first = a[0]; let second = a[1];   Functions  fn keyword, which allows you to declare new functions.  Rust doesn’t care where you define your functions, only that they’re defined somewhere.  fn main() {     println!(\"Hello, world!\");      another_function(); }  fn another_function() {     println!(\"Another function.\"); }  Parameters  fn main() {     another_function(5, 6); }  fn another_function(x: i32, y: i32) {     println!(\"The value of x is: {}\", x);     println!(\"The value of y is: {}\", y); }   Function Bodies Contain Statements and Expressions  Statements are instructions that perform some action and do not return a value.   Expressions evaluate to a resulting value.  fn main() {     let y = 6;  // This is Statemnets      // Error - (let y = 6) statement does not return a value     let x = (let y = 6);  }  Note: This is different from what happens in other languages, such as C and Ruby, where the assignment returns the value of the assignment.  In those languages, you can write x = y = 6 and have both x and y have the value 6; that is not the case in Rust.   Expressions can be part of statements: the 6 in the statement let y = 6; is an expression that evaluates to the value 6.  Calling a function is an expression. Calling a macro is an expression. The block that we use to create new scopes, {}, is an expression, for example:  fn main() {     let x = 5;      let y = { //like this         let x = 3;         x + 1     };      println!(\"The value of y is: {}\", y); }  This expression:  {     let x = 3;     x + 1 }  Note: the x + 1 line without a semicolon at the end, which is unlike most of the lines you’ve seen so far.  Expressions do not include ending semicolons. If you add a semicolon to the end of an expression, you turn it into a statement, which will then not return a value. Keep this in mind as you explore function return values and expressions next.   Functions with Return Values  We don’t name return values, but we do declare their type after an arrow (-&gt;) In Rust, the return value of the function is synonymous with the value of the final expression in the block of the body of a function.  You can return early from a function by using the return keyword and specifying a value, but most functions return the last expression implicitly.  // perfectly valid function in Rust fn five() -&gt; i32 {     5  }  fn main() {     let x = five();      println!(\"The value of x is: {}\", x); }   fn main() {     let x = plus_one(5);      println!(\"The value of x is: {}\", x); }  fn plus_one(x: i32) -&gt; i32 {     x + 1     // If place a semicolon at the end of the line x + 1,     // changing it from an expression to a statement,      // we’ll get an error.     // like this -&gt; error[E0308]: mismatched types }   Comments  A simple comment:      // So we’re doing something complicated here,      // long enough that we need     // multiple lines of comments to do it!     // Whew! Hopefully, this comment will     // explain what’s going on.     let lucky_number = 7; // I’m feeling lucky today.  Rust also has another kind of comment, documentation comments, which we’ll discuss in the “Publishing a Crate to Crates.io” section of Chapter 14.   Control Flow  The most common constructs that let you control the flow of execution of Rust code are if expressions and loops.  if Expressions  fn main() {     let number = 3;      if number &lt; 5 {         rintln!(\"condition was true\");     } else {         println!(\"condition was false\");     } }   It’s also worth noting that the condition in this code must be a bool. Rust will not automatically try to convert non-Boolean types to a Boolean.  fn main() {     let number = 3;      if number {   //Change to \"if number != 0 {\"         println!(\"number was three\");     } }   You can have multiple conditions by combining if and else in an else if expression.  fn main() {     let number = 6;      if number % 4 == 0 {         println!(\"number is divisible by 4\");     } else if number % 3 == 0 {         println!(\"number is divisible by 3\");     } else if number % 2 == 0 {         println!(\"number is divisible by 2\");     } else {         println!(\"number is not divisible by 4, 3, or 2\");     } }   Because if is an expression, we can use it on the right side of a let statement, as in below.  fn main() {     let condition = true;     let number = if condition {         5     } else {         6     };      println!(\"The value of number is: {}\", number); }   loop Expressions  The loop keyword tells Rust to execute a block of code over and over again forever or until you explicitly tell it to stop.   Most terminals support a keyboard shortcut, ctrl-c, to interrupt a program that is stuck in a continual loop. The symbol ^C represents where you pressed ctrl-c. You may or may not see the word again! printed after the ^C, depending on where the code was in the loop when it received the interrupt signal.  fn main() {     loop {         println!(\"again!\");     } }   // Returning Values from Loops fn main() {     let mut counter = 0;      let result = loop {         counter += 1;          if counter == 10 {           break counter * 2;         }       };      println!(\"The result is {}\", result); }  while Expressions  While the condition is true, the loop runs. When the condition ceases to be true, the program calls break, stopping the loop. This loop type could be implemented using a combination of loop, if, else, and break; you could try that now in a program, if you’d like.  fn main() {     let mut number = 3;      while number != 0 {         println!(\"{}!\", number);          number -= 1;     }      println!(\"LIFTOFF!!!\"); }   for Expressions  You could use the while construct to loop over the elements of a collection, such as an array.  fn main() {     let a = [10, 20, 30, 40, 50];     let mut index = 0;      while index &lt; 5 {         println!(\"the value is: {}\", a[index]);          index = index + 1;     } }  But this approach is error prone; we could cause the program to panic if the index length is incorrect. It’s also slow, because the compiler adds runtime code to perform the conditional check on every element on every iteration through the loop. As a more concise alternative, you can use a for loop and execute some code for each item in a collection.  fn main() {     let a = [10, 20, 30, 40, 50];      for element in a.iter() {         println!(\"the value is: {}\", element);     } }   Here’s what the countdown would look like using a for loop and another method we’ve not yet talked about, rev, to reverse the range:  fn main() {     for number in (1..4).rev() {         println!(\"{}!\", number);     }       println!(\"LIFTOFF!!!\"); }  ","categories": ["RUST Language"],
        "tags": ["concepts","valriable"],
        "url": "https://jjungs-lee.github.io//rust/3.Common-Programming-Concepts",
        "teaser":null},{
        "title": "RUST : 4. Understanding Ownership",
        "excerpt":"Ownership is Rust’s most unique feature, and it enables Rust to make memory safety guarantees without needing a garbage collector.   What Is Ownership?  Some languages have garbage collection that constantly looks for no longer used memory as the program runs; in other languages, the programmer must explicitly allocate and free the memory. Rust uses a third approach:     memory is managed through a system of ownership with a set of rules that the compiler checks at compile time.    Ownership Rules     Each value in Rust has a variable that’s called its owner.   There can only be one owner at a time.   When the owner goes out of scope, the value will be dropped.   Variable Scope  The variable s refers to a string literal, where the value of the string is hardcoded into the text of our program. The variable is valid from the point at which it’s declared until the end of the current scope.  let s = \"hello\";  scope is the range within a program for which an item is valid.  {                    // s is not valid here, it’s not yet declared   let s = \"hello\";   // s is valid from this point forward   // do stuff with s }                    // this scope is now over, and s is no longer valid  Listing 4-1: A variable and the scope in which it is valid  There are two important points in time here:     When s comes into scope, it is valid.   It remains valid until it goes out of scope.   The String Type  we need a data type that is more complex than the ones we covered in the “Data Types” section of Chapter 3. The types covered previously are all stored on the stack and popped off the stack when their scope is over, but we want to look at data that is stored on the heap and explore how Rust knows when to clean up that data.   For example, what if we want to take user input and store it? For these situations, Rust has a second string type, String. This type is allocated on the heap and as such is able to store an amount of text that is unknown to us at compile time. You can create a String from a string literal using the from function, like so:  let s = String::from(\"hello\");  The double colon (::) is an operator that allows us to namespace this particular from function under the String type rather than using some sort of name like string_from.   Why can String be mutated but literals cannot? The difference is how these two types deal with memory.   Memory and Allocation  With the String type, in order to support a mutable, growable piece of text, we need to allocate an amount of memory on the heap, unknown at compile time, to hold the contents. This means:     The memory must be requested from the operating system at runtime.   We need a way of returning this memory to the operating system when we’re done with our String.   That first part is done by us: when we call String::from, its implementation requests the memory it needs.   However, the second part is different. In languages with a garbage collector (GC), the GC keeps track and cleans up memory that isn’t being used anymore, and we don’t need to think about it. Without a GC, it’s our responsibility to identify when memory is no longer being used and call code to explicitly return it, just as we did to request it. Doing this correctly has historically been a difficult programming problem. If we forget, we’ll waste memory. If we do it too early, we’ll have an invalid variable. If we do it twice, that’s a bug too. We need to pair exactly one allocate with exactly one free.  {     let s = String::from(\"hello\"); // s is valid from this point forward     // do stuff with s }                                  // this scope is now over, and s is no                                    // longer valid  There is a natural point at which we can return the memory our String needs to the operating system: when s goes out of scope. When a variable goes out of scope, Rust calls a special function for us. This function is called drop, and it’s where the author of String can put the code to return the memory. Rust calls drop automatically at the closing curly bracket.   Note: In C++, this pattern of deallocating resources at the end of an item’s lifetime is sometimes called Resource Acquisition Is Initialization (RAII). The drop function in Rust will be familiar to you if you’ve used RAII patterns.   Ways Variables and Data Interact: Move  let s1 = String::from(\"hello\"); let s2 = s1;  A String is made up of three parts, shown on the left: a pointer to the memory that holds the contents of the string, a length, and a capacity. This group of data is stored on the stack. On the right is the memory on the heap that holds the contents.     Figure 4-1: Representation in memory of a String holding the value “hello” bound to s1   When we assign s1 to s2, the String data is copied, meaning we copy the pointer, the length, and the capacity that are on the stack. We do not copy the data on the heap that the pointer refers to.     Figure 4-2: Representation in memory of the variable s2 that has a copy of the pointer, length, and capacity of s1   The representation does not look like Figure 4-3, which is what memory would look like if Rust instead copied the heap data as well. If Rust did this, the operation s2 = s1 could be very expensive in terms of runtime performance if the data on the heap were large.     Figure 4-3: Another possibility for what s2 = s1 might do if Rust copied the heap data as well   Earlier, we said that when a variable goes out of scope, Rust automatically calls the drop function and cleans up the heap memory for that variable. But Figure 4-2 shows both data pointers pointing to the same location. This is a problem: when s2 and s1 go out of scope, they will both try to free the same memory. This is known as a double free error and is one of the memory safety bugs we mentioned previously. To ensure memory safety, there’s one more detail to what happens in this situation in Rust.   let s1 = String::from(\"hello\"); let s2 = s1;  println!(\"{}, world!\", s1);  You’ll get an error like this because Rust prevents you from using the invalidated reference:  error[E0382]: use of moved value: `s1`  --&gt; src/main.rs:5:28   | 3 |     let s2 = s1;   |         -- value moved here 4 | 5 |     println!(\"{}, world!\", s1);   |                            ^^ value used here after move   |   = note: move occurs because `s1` has type `std::string::String`,   which does not implement the `Copy` trait  with other languages, the concept of copying the pointer, length, and capacity without copying the data probably sounds like making a shallow copy. But because Rust also invalidates the first variable, instead of being called a shallow copy, it’s known as a move. That solves our problem! With only s2 valid, when it goes out of scope, it alone will free the memory, and we’re done. Rust will never automatically create “deep” copies of your data.     Figure 4-4: Representation in memory after s1 has been invalidated   Ways Variables and Data Interact: Clone  If we do want to deeply copy the heap data of the String, not just the stack data, we can use a common method called clone. This methods are a common feature in many programming languages, you’ve probably seen them before. This works just fine and explicitly produces the behavior shown in Figure 4-3, where the heap data does get copied.  let s1 = String::from(\"hello\"); let s2 = s1.clone();  println!(\"s1 = {}, s2 = {}\", s1, s2);   Stack-Only Data: Copy  This code seems to contradict what we just learned: we don’t have a call to clone, but x is still valid and wasn’t moved into y.   The reason is that types such as integers that have a known size at compile time are stored entirely on the stack, so copies of the actual values are quick to make. That means there’s no reason we would want to prevent x from being valid after we create the variable y. In other words, there’s no difference between deep and shallow copying here, so calling clone wouldn’t do anything different from the usual shallow copying and we can leave it out.  let x = 5; let y = x;  println!(\"x = {}, y = {}\", x, y);  Rust has a special annotation called the Copy trait that we can place on types like integers that are stored on the stack (we’ll talk more about traits in Chapter 10). So what types are Copy?     All the integer types, such as u32.   The Boolean type, bool, with values true and false.   All the floating point types, such as f64.   The character type, char.   Tuples, if they only contain types that are also Copy. For example, (i32, i32) is Copy, but (i32, String) is not.   Ownership and Functions  The semantics for passing a value to a function are similar to those for assigning a value to a variable. Passing a variable to a function will move or copy, just as assignment does. If we tried to use s after the call to takes_ownership, Rust would throw a compile-time error. These static checks protect us from mistakes.  fn main() {     let s = String::from(\"hello\");  // s comes into scope      takes_ownership(s);             // s's value moves into the function...                                     // ... and so is no longer valid here      let x = 5;                      // x comes into scope      makes_copy(x);                  // x would move into the function,                                     // but i32 is Copy, so it’s okay to still                                     // use x afterward  } // Here, x goes out of scope, then s. But because s's value was moved, nothing   // special happens.  fn takes_ownership(some_string: String) { // some_string comes into scope     println!(\"{}\", some_string); } // Here, some_string goes out of scope and `drop` is called. The backing   // memory is freed.  fn makes_copy(some_integer: i32) { // some_integer comes into scope     println!(\"{}\", some_integer); } // Here, some_integer goes out of scope. Nothing special happens.  Listing 4-3: Functions with ownership and scope annotated   Return Values and Scope  Returning values can also transfer ownership. Listing 4-4 is an example with similar annotations to those in Listing 4-3.  fn main() {     let s1 = gives_ownership();         // gives_ownership moves its return                                         // value into s1      let s2 = String::from(\"hello\");     // s2 comes into scope      let s3 = takes_and_gives_back(s2);  // s2 is moved into                                         // takes_and_gives_back, which also                                         // moves its return value into s3 } // Here, s3 goes out of scope and is dropped. s2 goes out of scope but was   // moved, so nothing happens. s1 goes out of scope and is dropped.  fn gives_ownership() -&gt; String {             // gives_ownership will move its                                              // return value into the function                                              // that calls it      let some_string = String::from(\"hello\"); // some_string comes into scope      some_string                              // some_string is returned and                                              // moves out to the calling                                              // function }  // takes_and_gives_back will take a String and return one fn takes_and_gives_back(a_string: String) -&gt; String { // a_string comes into                                                       // scope      a_string  // a_string is returned and moves out to the calling function }  Listing 4-4: Transferring ownership of return values   The ownership of a variable follows the same pattern every time: assigning a value to another variable moves it. When a variable that includes data on the heap goes out of scope, the value will be cleaned up by drop unless the data has been moved to be owned by another variable.   Taking ownership and then returning ownership with every function is a bit tedious. What if we want to let a function use a value but not take ownership? It’s quite annoying that anything we pass in also needs to be passed back if we want to use it again, in addition to any data resulting from the body of the function that we might want to return as well.   It’s possible to return multiple values using a tuple, as shown in Listing 4-5.  fn main() {     let s1 = String::from(\"hello\");      let (s2, len) = calculate_length(s1);      println!(\"The length of '{}' is {}.\", s2, len); }  fn calculate_length(s: String) -&gt; (String, usize) {     let length = s.len(); // len() returns the length of a String.      (s, length) }  Listing 4-5: Returning ownership of parameters  But this is too much ceremony and a lot of work for a concept that should be common. Luckily for us, Rust has a feature for this concept, called references.   References and Borrowing  Here is how you would define and use a calculate_length function that has a reference to an object as a parameter instead of taking ownership of the value:  fn main() {     let s1 = String::from(\"hello\");      let len = calculate_length(&amp;s1);      println!(\"The length of '{}' is {}.\", s1, len); }  fn calculate_length(s: &amp;String) -&gt; usize {     s.len() }  First, notice that all the tuple code in the variable declaration and the function return value is gone. Second, note that we pass &amp;s1 into calculate_length and, in its definition, we take &amp;String rather than String.   These ampersands are references, and they allow you to refer to some value without taking ownership of it     Figure 4-5: A diagram of &amp;String s pointing at String s1  Note: The opposite of referencing by using &amp; is dereferencing, which is accomplished with the dereference operator, *. We’ll see some uses of the dereference operator in Chapter 8 and discuss details of dereferencing in Chapter 15.   let s1 = String::from(\"hello\");  let len = calculate_length(&amp;s1);  The &amp;s1 syntax lets us create a reference that refers to the value of s1 but does not own it. Because it does not own it, the value it points to will not be dropped when the reference goes out of scope.   Likewise, the signature of the function uses &amp; to indicate that the type of the parameter s is a reference. Let’s add some explanatory annotations:  fn calculate_length(s: &amp;String) -&gt; usize { // s is a reference to a String     s.len() } // Here, s goes out of scope. But because it does not have ownership of what   // it refers to, nothing happens.```  The scope in which the variable s is valid is the same as any function parameter’s scope, but we don’t drop what the reference points to when it goes out of scope because we don’t have ownership. When functions have references as parameters instead of the actual values, we won’t need to return the values in order to give back ownership, because we never had ownership.   We call having references as function parameters borrowing. As in real life, if a person owns something, you can borrow it from them. When you’re done, you have to give it back.   So what happens if we try to modify something we’re borrowing? Try the code. Spoiler alert: it doesn’t work!  fn main() {     let s = String::from(\"hello\");      change(&amp;s); }  fn change(some_string: &amp;String) {     some_string.push_str(\", world\"); }  error[E0596]: cannot borrow immutable borrowed content `*some_string` as mutable  --&gt; error.rs:8:5   | 7 | fn change(some_string: &amp;String) {   |                        ------- use `&amp;mut String` here to make mutable 8 |     some_string.push_str(\", world\");   |     ^^^^^^^^^^^ cannot borrow as mutable  Listing 4-6: Attempting to modify a borrowed value  Just as variables are immutable by default, so are references. We’re not allowed to modify something we have a reference to.   Mutable References  We can fix the error in the code from Listing 4-6 with just a small tweak  fn main() {     let mut s = String::from(\"hello\");      change(&amp;mut s); }  fn change(some_string: &amp;mut String) {     some_string.push_str(\", world\"); }  First, we had to change s to be mut. Then we had to create a mutable reference with &amp;mut s and accept a mutable reference with some_string: &amp;mut String.   But mutable references have one big restriction: you can have only one mutable reference to a particular piece of data in a particular scope. This code will fail:  let mut s = String::from(\"hello\");  let r1 = &amp;mut s; let r2 = &amp;mut s;  println!(\"{}, {}\", r1, r2);  error[E0499]: cannot borrow `s` as mutable more than once at a time  --&gt; src/main.rs:5:14   | 4 |     let r1 = &amp;mut s;   |              ------ first mutable borrow occurs here 5 |     let r2 = &amp;mut s;   |              ^^^^^^ second mutable borrow occurs here 6 | 7 |     println!(\"{}, {}\", r1, r2);   |                        -- first borrow later used here  This restriction allows for mutation but in a very controlled fashion. It’s something that new Rustaceans struggle with, because most languages let you mutate whenever you’d like.   The benefit of having this restriction is that Rust can prevent data races at compile time. A data race is similar to a race condition and happens when these three behaviors occur:     Two or more pointers access the same data at the same time.   At least one of the pointers is being used to write to the data.   There’s no mechanism being used to synchronize access to the data.   Data races cause undefined behavior and can be difficult to diagnose and fix when you’re trying to track them down at runtime; Rust prevents this problem from happening because it won’t even compile code with data races!   As always, we can use curly brackets to create a new scope, allowing for multiple mutable references, just not simultaneous ones:   let mut s = String::from(\"hello\");  {     let r1 = &amp;mut s; } // r1 goes out of scope here, so we can make a new reference with no problems.  let r2 = &amp;mut s;   A similar rule exists for combining mutable and immutable references. This code results in an error:  let mut s = String::from(\"hello\");  let r1 = &amp;s; // no problem let r2 = &amp;s; // no problem let r3 = &amp;mut s; // BIG PROBLEM  println!(\"{}, {}, and {}\", r1, r2, r3);  error[E0502]: cannot borrow `s` as mutable because it is also borrowed as immutable  --&gt; src/main.rs:6:14   | 4 |     let r1 = &amp;s; // no problem   |              -- immutable borrow occurs here 5 |     let r2 = &amp;s; // no problem 6 |     let r3 = &amp;mut s; // BIG PROBLEM   |              ^^^^^^ mutable borrow occurs here 7 | 8 |     println!(\"{}, {}, and {}\", r1, r2, r3);   |                                -- immutable borrow later used here  Whew! We also cannot have a mutable reference while we have an immutable one. Users of an immutable reference don’t expect the values to suddenly change out from under them! However, multiple immutable references are okay because no one who is just reading the data has the ability to affect anyone else’s reading of the data.   Note that a reference’s scope starts from where it is introduced and continues through the last time that reference is used. For instance, this code will compile because the last usage of the immutable references occurs before the mutable reference is introduced:  let mut s = String::from(\"hello\");  let r1 = &amp;s; // no problem let r2 = &amp;s; // no problem println!(\"{} and {}\", r1, r2); // r1 and r2 are no longer used after this point  let r3 = &amp;mut s; // no problem println!(\"{}\", r3);  The scopes of the immutable references r1 and r2 end after the println! where they are last used, which is before the mutable reference r3 is created. These scopes don’t overlap, so this code is allowed.   Even though borrowing errors may be frustrating at times, remember that it’s the Rust compiler pointing out a potential bug early (at compile time rather than at runtime) and showing you exactly where the problem is. Then you don’t have to track down why your data isn’t what you thought it was.   Dangling References  In languages with pointers, it’s easy to erroneously create a dangling pointer, a pointer that references a location in memory that may have been given to someone else, by freeing some memory while preserving a pointer to that memory. In Rust, by contrast, the compiler guarantees that references will never be dangling references: if you have a reference to some data, the compiler will ensure that the data will not go out of scope before the reference to the data does.   fn main() {     let reference_to_nothing = dangle(); }  fn dangle() -&gt; &amp;String {     let s = String::from(\"hello\");      &amp;s }  error[E0106]: missing lifetime specifier  --&gt; dangle.rs:5:16   | 5 | fn dangle() -&gt; &amp;String {   |                ^^^^^^^   |   = help: this function's return type contains a borrowed value, but there is no     value for it to be borrowed from   = help: consider giving it a 'static lifetime  error: aborting due to previous error  This error message refers to a feature we haven’t covered yet: lifetimes. We’ll discuss lifetimes in detail in Chapter 10. But, if you disregard the parts about lifetimes, the message does contain the key to why this code is a problem:     this function’s return type contains a borrowed value, but there is no value for it to be borrowed from.    Let’s take a closer look at exactly what’s happening at each stage of our dangle code:  fn dangle() -&gt; &amp;String { // dangle returns a reference to a String      let s = String::from(\"hello\"); // s is a new String      &amp;s // we return a reference to the String, s } // Here, s goes out of scope, and is dropped. Its memory goes away.   // Danger!  Because s is created inside dangle, when the code of dangle is finished, s will be deallocated. But we tried to return a reference to it. That means this reference would be pointing to an invalid String. That’s no good! Rust won’t let us do this.   The solution here is to return the String directly:  fn no_dangle() -&gt; String {     let s = String::from(\"hello\");      s }  This works without any problems. Ownership is moved out, and nothing is deallocated.   The Rules of References  Let’s recap what we’ve discussed about references:     At any given time, you can have either one mutable reference or any number of immutable references.   References must always be valid.   The Slice Type  Another data type that does not have ownership is the slice. Slices let you reference a contiguous sequence of elements in a collection rather than the whole collection.   Here’s a small programming problem: write a function that takes a string and returns the first word it finds in that string. If the function doesn’t find a space in the string, the whole string must be one word, so the entire string should be returned.   Let’s think about the signature of this function:  fn first_word(s: &amp;String) -&gt; ?   This function, first_word, has a &amp;String as a parameter. We don’t want ownership, so this is fine. But what should we return? We don’t really have a way to talk about part of a string. However, we could return the index of the end of the word. Let’s try that, as shown in Listing 4-7.  fn main() {     fn first_word(s: &amp;String) -&gt; usize {         let bytes = s.as_bytes();          for (i, &amp;item) in bytes.iter().enumerate() {             if item == b' ' {                 return i;             }         }          s.len()     } }  Listing 4-7: The first_word function that returns a byte index value into the String parameter   Because we need to go through the String element by element and check whether a value is a space, we’ll convert our String to an array of bytes using the as_bytes method:  let bytes = s.as_bytes();  Next, we create an iterator over the array of bytes using the iter method:  for (i, &amp;item) in bytes.iter().enumerate() {  We’ll discuss iterators in more detail in Chapter 13. For now, know that iter is a method that returns each element in a collection and that enumerate wraps the result of iter and returns each element as part of a tuple instead. The first element of the tuple returned from enumerate is the index, and the second element is a reference to the element. This is a bit more convenient than calculating the index ourselves.   Because the enumerate method returns a tuple, we can use patterns to destructure that tuple, just like everywhere else in Rust. So in the for loop, we specify a pattern that has i for the index in the tuple and &amp;item for the single byte in the tuple. Because we get a reference to the element from .iter().enumerate(), we use &amp; in the pattern.   Inside the for loop, we search for the byte that represents the space by using the byte literal syntax. If we find a space, we return the position. Otherwise, we return the length of the string by using s.len():      if item == b' ' {         return i;     } } s.len()  We now have a way to find out the index of the end of the first word in the string, but there’s a problem. We’re returning a usize on its own, but it’s only a meaningful number in the context of the &amp;String. In other words, because it’s a separate value from the String, there’s no guarantee that it will still be valid in the future. Consider the program in Listing 4-8 that uses the first_word function from Listing 4-7.  fn main() {     let mut s = String::from(\"hello world\");      let word = first_word(&amp;s); // word will get the value 5      s.clear(); // this empties the String, making it equal to \"\"      // word still has the value 5 here, but there's no more string that     // we could meaningfully use the value 5 with. word is now totally invalid! }  Listing 4-8: Storing the result from calling the first_word function and then changing the String contents   This program compiles without any errors and would also do so if we used word after calling s.clear(). Because word isn’t connected to the state of s at all, word still contains the value 5. We could use that value 5 with the variable s to try to extract the first word out, but this would be a bug because the contents of s have changed since we saved 5 in word.   Having to worry about the index in word getting out of sync with the data in s is tedious and error prone! Managing these indices is even more brittle if we write a second_word function. Its signature would have to look like this:   fn second_word(s: &amp;String) -&gt; (usize, usize) {  Now we’re tracking a starting and an ending index, and we have even more values that were calculated from data in a particular state but aren’t tied to that state at all. We now have three unrelated variables floating around that need to be kept in sync.   Luckily, Rust has a solution to this problem: string slices.   String Slices  A string slice is a reference to part of a String, and it looks like this:  fn main() {     let s = String::from(\"hello world\");      let hello = &amp;s[0..5];     let world = &amp;s[6..11]; }  This is similar to taking a reference to the whole String but with the extra [0..5] bit. Rather than a reference to the entire String, it’s a reference to a portion of the String.   We can create slices using a range within brackets by specifying [starting_index..ending_index], where starting_index is the first position in the slice and ending_index is one more than the last position in the slice. Internally, the slice data structure stores the starting position and the length of the slice, which corresponds to ending_index minus starting_index. So in the case of let world = &amp;s[6..11];, world would be a slice that contains a pointer to the 7th byte (counting from 1) of s with a length value of 5.   Figure 4-6: String slice referring to part of a String   With Rust’s .. range syntax, if you want to start at the first index (zero), you can drop the value before the two periods. In other words, these are equal:  let s = String::from(\"hello\");  let slice = &amp;s[0..2]; let slice = &amp;s[..2];  You can also drop both values to take a slice of the entire string. So these are equal:  let s = String::from(\"hello\");  let len = s.len();  let slice = &amp;s[0..len]; let slice = &amp;s[..];   Note: String slice range indices must occur at valid UTF-8 character boundaries. If you attempt to create a string slice in the middle of a multibyte character, your program will exit with an error. For the purposes of introducing string slices, we are assuming ASCII only in this section; a more thorough discussion of UTF-8 handling is in the “Storing UTF-8 Encoded Text with Strings” section of Chapter 8.   With all this information in mind, let’s rewrite first_word to return a slice. The type that signifies “string slice” is written as &amp;str:  fn first_word(s: &amp;String) -&gt; &amp;str {     let bytes = s.as_bytes();      for (i, &amp;item) in bytes.iter().enumerate() {         if item == b' ' {             return &amp;s[0..i];         }     }      &amp;s[..] }  We get the index for the end of the word in the same way as we did in Listing 4-7, by looking for the first occurrence of a space. When we find a space, we return a string slice using the start of the string and the index of the space as the starting and ending indices.   Now when we call first_word, we get back a single value that is tied to the underlying data. The value is made up of a reference to the starting point of the slice and the number of elements in the slice.   Returning a slice would also work for a second_word function:  fn second_word(s: &amp;String) -&gt; &amp;str {  We now have a straightforward API that’s much harder to mess up, because the compiler will ensure the references into the String remain valid. Remember the bug in the program in Listing 4-8, when we got the index to the end of the first word but then cleared the string so our index was invalid? That code was logically incorrect but didn’t show any immediate errors. The problems would show up later if we kept trying to use the first word index with an emptied string. Slices make this bug impossible and let us know we have a problem with our code much sooner. Using the slice version of first_word will throw a compile-time error:  fn main() {     let mut s = String::from(\"hello world\");      let word = first_word(&amp;s);      s.clear(); // Error!      println!(\"the first word is: {}\", word); }  error[E0502]: cannot borrow `s` as mutable because it is also borrowed as immutable   --&gt; src/main.rs:18:5    | 16 |     let word = first_word(&amp;s);    |                           -- immutable borrow occurs here 17 | 18 |     s.clear(); // error!    |     ^^^^^^^^^ mutable borrow occurs here 19 | 20 |     println!(\"the first word is: {}\", word);    |                                       ---- immutable borrow later used here  Recall from the borrowing rules that if we have an immutable reference to something, we cannot also take a mutable reference. Because clear needs to truncate the String, it needs to get a mutable reference. Rust disallows this, and compilation fails. Not only has Rust made our API easier to use, but it has also eliminated an entire class of errors at compile time!   String Literals Are Slices  The type of s here is &amp;str: it’s a slice pointing to that specific point of the binary. This is also why string literals are immutable; &amp;str is an immutable reference.  let s = \"Hello, world!\";   String Slices as Parameters  Knowing that you can take slices of literals and String values leads us to one more improvement on first_word, and that’s its signature:  fn first_word(s: &amp;String) -&gt; &amp;str {  A more experienced Rustacean would write the signature shown in Listing 4-9 instead because it allows us to use the same function on both &amp;String values and &amp;str values.   fn first_word(s: &amp;str) -&gt; &amp;str {  Listing 4-9: Improving the first_word function by using a string slice for the type of the s parameter   If we have a string slice, we can pass that directly. If we have a String, we can pass a slice of the entire String. Defining a function to take a string slice instead of a reference to a String makes our API more general and useful without losing any functionality:   fn main() {     let my_string = String::from(\"hello world\");      // first_word works on slices of `String`s     let word = first_word(&amp;my_string[..]);      let my_string_literal = \"hello world\";      // first_word works on slices of string literals     let word = first_word(&amp;my_string_literal[..]);      // Because string literals *are* string slices already,     // this works too, without the slice syntax!     let word = first_word(my_string_literal); }   Other Slices  String slices, as you might imagine, are specific to strings. But there’s a more general slice type, too. Consider this array:   let a = [1, 2, 3, 4, 5];  Just as we might want to refer to a part of a string, we might want to refer to part of an array. We’d do so like this:   let a = [1, 2, 3, 4, 5];  let slice = &amp;a[1..3];  This slice has the type &amp;[i32]. It works the same way as string slices do, by storing a reference to the first element and a length. You’ll use this kind of slice for all sorts of other collections. We’ll discuss these collections in detail when we talk about vectors in Chapter 8.   Summary  The concepts of ownership, borrowing, and slices ensure memory safety in Rust programs at compile time. The Rust language gives you control over your memory usage in the same way as other systems programming languages, but having the owner of data automatically clean up that data when the owner goes out of scope means you don’t have to write and debug extra code to get this control.   Ownership affects how lots of other parts of Rust work, so we’ll talk about these concepts further throughout the rest of the book. Let’s move on to Chapter 5 and look at grouping pieces of data together in a struct.  ","categories": ["RUST Language"],
        "tags": ["Ownership","Unique feature","Memory safety"],
        "url": "https://jjungs-lee.github.io//rust/4.Understanding-Ownership",
        "teaser":null},{
        "title": "RUST : 5. Using structs to structure related data",
        "excerpt":"A struct, or structure, is a custom data type that lets you name and package together multiple related values that make up a meaningful group.   Defining and Instantiating Structs  Structs are similar to tuples, which were discussed in Chapter 3. Like tuples, the pieces of a struct can be different types. Unlike with tuples, you’ll name each piece of data so it’s clear what the values mean. As a result of these names, structs are more flexible than tuples.   To define a struct, we enter the keyword struct and name the entire struct  struct User {     username: String,     email: String,     sign_in_count: u64,     active: bool, }  Listing 5-1: A User struct definition   To use a struct after we’ve defined it, we create an instance of that struct by specifying concrete values for each of the fields.  We create an instance by stating the name of the struct and then add curly brackets containing key: value pairs, where the keys are the names of the fields and the values are the data we want to store in those fields. We don’t have to specify the fields in the same order in which we declared them in the struct.  let user1 = User {     email: String::from(\"someone@example.com\"),     username: String::from(\"someusername123\"),     active: true,     sign_in_count: 1, };  Listing 5-2: Creating an instance of the User struct   To get a specific value from a struct, we can use dot(.) notation. If we wanted just this user’s email address, we could use user1.email wherever we wanted to use this value. If the instance is mutable, we can change a value by using the dot(.) notation and assigning into a particular field. Listing 5-3 shows how to change the value in the email field of a mutable User instance.  let mut user1 = User {     email: String::from(\"someone@example.com\"),     username: String::from(\"someusername123\"),     active: true,     sign_in_count: 1, };  user1.email = String::from(\"anotheremail@example.com\");  Listing 5-3: Changing the value in the email field of a User instance   Listing 5-4 shows a build_user function that returns a User instance with the given email and username. The active field gets the value of true, and the sign_in_count gets a value of 1.  // JHS) maybe this function similar constructor in C++ fn build_user(email: String, username: String) -&gt; User {     User {         email: email,         username: username,         active: true,         sign_in_count: 1,     } }  Listing 5-4: A build_user function that takes an email and username and returns a User instance   Using the Field Init Shorthand when Variables and Fields Have the Same Name  Because the parameter names and the struct field names are exactly the same in Listing 5-4, we can use the field init shorthand syntax to rewrite build_user so that it behaves exactly the same but doesn’t have the repetition of email and username, as shown in Listing 5-5.  fn build_user(email: String, username: String) -&gt; User {     User {         email,         username,         active: true,         sign_in_count: 1,     } }  Listing 5-5: A build_user function that uses field init shorthand because the email and username parameters have the same name as struct fields   Creating Instances From Other Instances With Struct Update Syntax  It’s often useful to create a new instance of a struct that uses most of an old instance’s values but changes some. You’ll do this using struct update syntax. First, Listing 5-6 shows how we create a new User instance in user2 without the update syntax. We set new values for email and username but otherwise use the same values from user1 that we created in Listing 5-2.  let user2 = User {     email: String::from(\"another@example.com\"),     username: String::from(\"anotherusername567\"),     active: user1.active,     sign_in_count: user1.sign_in_count, };  Listing 5-6: Creating a new User instance using some of the values from user1  Using struct update syntax, we can achieve the same effect with less code, as shown in Listing 5-7. The syntax .. specifies that the remaining fields not explicitly set should have the same value as the fields in the given instance. The code in Listing 5-7 also creates an instance in user2 that has a different value for email and username but has the same values for the active and sign_in_count fields from user1.  let user2 = User {     email: String::from(\"another@example.com\"),     username: String::from(\"anotherusername567\"),     ..user1 };  Listing 5-7: Using struct update syntax to set new email and username values for a User instance but use the rest of the values from the fields of the instance in the user1 variable   Using Tuple Structs without Named Fields to Create Different Types  You can also define structs that look similar to tuples, called tuple structs. Tuple structs have the added meaning the struct name provides but don’t have names associated with their fields; rather, they just have the types of the fields. Tuple structs are useful when you want to give the whole tuple a name and make the tuple be a different type from other tuples, and naming each field as in a regular struct would be verbose or redundant.   To define a tuple struct, start with the struct keyword and the struct name followed by the types in the tuple.  struct Color(i32, i32, i32); struct Point(i32, i32, i32);  let black = Color(0, 0, 0); let origin = Point(0, 0, 0);  Tuple struct instances behave like tuples: you can destructure them into their individual pieces, you can use a . followed by the index to access an individual value, and so on.   Unit-Like Structs Without Any Fields  You can also define structs that don’t have any fields! These are called unit-like structs because they behave similarly to (), the unit type. Unit-like structs can be useful in situations in which you need to implement a trait on some type but don’t have any data that you want to store in the type itself. We’ll discuss traits in Chapter 10.   Ownership of Struct Data  In the User struct definition in Listing 5-1, we used the owned String type rather than the &amp;str string slice type. This is a deliberate choice because we want instances of this struct to own all of its data and for that data to be valid for as long as the entire struct is valid.   It’s possible for structs to store references to data owned by something else, but to do so requires the use of lifetimes, a Rust feature that we’ll discuss in Chapter 10. Lifetimes ensure that the data referenced by a struct is valid for as long as the struct is.  Let’s say you try to store a reference in a struct without specifying lifetimes, like this, which won’t work:   In Chapter 10, we’ll discuss how to fix these errors so you can store references in structs, but for now, we’ll fix errors like these using owned types like String instead of references like &amp;str.  // This code does not compile! struct User {     username: &amp;str,     email: &amp;str,     sign_in_count: u64,     active: bool, }  fn main() {     let user1 = User {         email: \"someone@example.com\",         username: \"someusername123\",         active: true,         sign_in_count: 1,     }; }  // The compiler will complain that it needs lifetime specifiers: error[E0106]: missing lifetime specifier --&gt;   | 2 |     username: &amp;str,   |               ^ expected lifetime parameter  error[E0106]: missing lifetime specifier --&gt;   | 3 |     email: &amp;str,   |            ^ expected lifetime parameter   An Example Program Using Structs  Let’s write a program that calculates the area of a rectangle. We’ll start with single variables, and then refactor the program until we’re using structs instead.  fn main() {     let length1 = 50;     let width1 = 30;      println!(         \"The area of the rectangle is {} square pixels.\",         area(length1, width1)     ); }  fn area(length: u32, width: u32) -&gt; u32 {     length * width }  Listing 5-8: Calculating the area of a rectangle specified by separate width and height variables  The issue with this code is evident in the signature of area:   Even though Listing 5-8 works and figures out the area of the rectangle by calling the area function with each dimension, we can do better.  fn area(width: u32, height: u32) -&gt; u32 {  The area function is supposed to calculate the area of one rectangle, but the function we wrote has two parameters. The parameters are related, but that’s not expressed anywhere in our program. It would be more readable and more manageable to group width and height together. We’ve already discussed one way we might do that in “The Tuple Type” section of Chapter 3: by using tuples.   Refactoring with Tuples  Listing 5-9 shows another version of our program that uses tuples.  fn main() {     let rect1 = (30, 50);      println!(         \"The area of the rectangle is {} square pixels.\",         area(rect1)     ); }  fn area(dimensions: (u32, u32)) -&gt; u32 {     dimensions.0 * dimensions.1 }  Listing 5-9: Specifying the width and height of the rectangle with a tuple  In one way, this program is better. Tuples let us add a bit of structure, and we’re now passing just one argument. But in another way, this version is less clear: tuples don’t name their elements, so our calculation has become more confusing because we have to index into the parts of the tuple.   It doesn’t matter if we mix up width and height for the area calculation, but if we want to draw the rectangle on the screen, it would matter! We would have to keep in mind that width is the tuple index 0 and height is the tuple index 1. If someone else worked on this code, they would have to figure this out and keep it in mind as well. It would be easy to forget or mix up these values and cause errors, because we haven’t conveyed the meaning of our data in our code.   Refactoring with Structs: Adding More Meaning  We use structs to add meaning by labeling the data. We can transform the tuple we’re using into a data type with a name for the whole as well as names for the parts, as shown in Listing 5-10.  struct Rectangle {     width: u32,     height: u32, }  fn main() {     let rect1 = Rectangle { width: 30, height: 50 };      println!(         \"The area of the rectangle is {} square pixels.\",         area(&amp;rect1)     ); }  fn area(rectangle: &amp;Rectangle) -&gt; u32 {     rectangle.width * rectangle.height }  Listing 5-10: Defining a Rectangle struct  Here we’ve defined a struct and named it Rectangle. Inside the curly brackets, we defined the fields as width and height, both of which have type u32. Then in main, we created a particular instance of Rectangle that has a width of 30 and a height of 50.   Our area function is now defined with one parameter, which we’ve named rectangle, whose type is an immutable borrow of a struct Rectangle instance. As mentioned in Chapter 4, we want to borrow the struct rather than take ownership of it. This way, main retains its ownership and can continue using rect1, which is the reason we use the &amp; in the function signature and where we call the function.   The area function accesses the width and height fields of the Rectangle instance. Our function signature for area now says exactly what we mean: calculate the area of Rectangle, using its width and height fields. This conveys that the width and height are related to each other, and it gives descriptive names to the values rather than using the tuple index values of 0 and 1. This is a win for clarity.   Adding Useful Functionality with Derived Traits  It’d be nice to be able to print an instance of Rectangle while we’re debugging our program and see the values for all its fields. Listing 5-11 tries using the println! macro as we have used in previous chapters. This won’t work, however.  struct Rectangle {     length: u32,     width: u32, }  fn main() {     let rect1 = Rectangle { length: 50, width: 30 };      println!(\"rect1 is {}\", rect1);      // When we compile this code, we get an error with this core message:     // error[E0277]: `Rectangle` doesn't implement `std::fmt::Display` }  Listing 5-11: Attempting to print a Rectangle instance   The println! macro can do many kinds of formatting, and by default, the curly brackets tell println! to use formatting known as Display: output intended for direct end user consumption. The primitive types we’ve seen so far implement Display by default, because there’s only one way you’d want to show a 1 or any other primitive type to a user. But with structs, the way println! should format the output is less clear because there are more display possibilities: Do you want commas or not? Do you want to print the curly brackets? Should all the fields be shown? Due to this ambiguity, Rust doesn’t try to guess what we want, and structs don’t have a provided implementation of Display.  = help: the trait `std::fmt::Display` is not implemented for `Rectangle` = note: in format strings you may be able to use `{:?}`          (or {:#?} for pretty-print) instead  Let’s try it! The println! macro call will now look like println!(\"rect1 is {:?}\", rect1);. Putting the specifier :? inside the curly brackets tells println! we want to use an output format called Debug. The Debug trait enables us to print our struct in a way that is useful for developers so we can see its value while we’re debugging our code.   Compile the code with this change. Drat! We still get an error:  error[E0277]: `Rectangle` doesn't implement `std::fmt::Debug`  But again, the compiler gives us a helpful note:  = help: the trait `std::fmt::Debug` is not implemented for `Rectangle` = note: add `#[derive(Debug)]` or manually implement `std::fmt::Debug`  Rust does include functionality to print out debugging information, but we have to explicitly opt in to make that functionality available for our struct. To do that, we add the annotation #[derive(Debug)] just before the struct definition, as shown in Listing 5-12.  #[derive(Debug)] struct Rectangle {     width: u32,     height: u32, }  fn main() {     let rect1 = Rectangle { width: 30, height: 50 };      println!(\"rect1 is {:?}\", rect1); }  Listing 5-12: Adding the annotation to derive the Debug trait and printing the Rectangle instance using debug formatting  Now when we run the program, we won’t get any errors, and we’ll see the following output:  rect1 is Rectangle { width: 30, height: 50 }   Nice! It’s not the prettiest output, but it shows the values of all the fields for this instance, which would definitely help during debugging. When we have larger structs, it’s useful to have output that’s a bit easier to read; in those cases, we can use {:#?}** instead of {:?} in the println! string. When we use the {:#?} style in the example, the output will look like this:  rect1 is Rectangle {     width: 30,     height: 50 }  Rust has provided a number of traits for us to use with the derive annotation that can add useful behavior to our custom types. Those traits and their behaviors are listed in Appendix C. We’ll cover how to implement these traits with custom behavior as well as how to create your own traits in Chapter 10.   Our area function is very specific: it only computes the area of rectangles. It would be helpful to tie this behavior more closely to our Rectangle struct, because it won’t work with any other type. Let’s look at how we can continue to refactor this code by turning the area function into an area method defined on our Rectangle type.   Method Syntax  Methods are similar to functions: they’re declared with the fn keyword and their name, they can have parameters and a return value, and they contain some code that is run when they’re called from somewhere else. However, methods are different from functions in that they’re defined within the context of a struct (or an enum or a trait object, which we cover in Chapters 6 and 17, respectively), and their first parameter is always self, which represents the instance of the struct the method is being called on.   Defining Methods  Let’s change the area function that has a Rectangle instance as a parameter and instead make an area method defined on the Rectangle struct, as shown in Listing 5-13.  #[derive(Debug)] struct Rectangle {     width: u32,     height: u32, }  impl Rectangle {     fn area(&amp;self) -&gt; u32 {         self.width * self.height     } }  fn main() {     let rect1 = Rectangle { width: 30, height: 50 };      println!(         \"The area of the rectangle is {} square pixels.\",         rect1.area()     ); }  Listing 5-13: Defining an area method on the Rectangle struct  To define the function within the context of Rectangle, we start an impl (implementation) block. Then we move the area function within the impl curly brackets and change the first (and in this case, only) parameter to be self in the signature and everywhere within the body. In main, where we called the area function and passed rect1 as an argument, we can instead use method syntax to call the area method on our Rectangle instance. The method syntax goes after an instance: we add a dot(.) followed by the method name, parentheses, and any arguments.   In the signature for area, we use &amp;self instead of rectangle: &amp;Rectangle because Rust knows the type of self is Rectangle due to this method’s being inside the impl Rectangle context. Note that we still need to use the &amp; before self, just as we did in &amp;Rectangle. Methods can take ownership of self, borrow self immutably as we’ve done here, or borrow self mutably, just as they can any other parameter.   We’ve chosen &amp;self here for the same reason we used &amp;Rectangle in the function version: we don’t want to take ownership, and we just want to read the data in the struct, not write to it. If we wanted to change the instance that we’ve called the method on as part of what the method does, we’d use &amp;mut self as the first parameter. Having a method that takes ownership of the instance by using just self as the first parameter is rare; this technique is usually used when the method transforms self into something else and you want to prevent the caller from using the original instance after the transformation.   The main benefit of using methods instead of functions, in addition to using method syntax and not having to repeat the type of self in every method’s signature, is for organization. We’ve put all the things we can do with an instance of a type in one impl block rather than making future users of our code search for capabilities of Rectangle in various places in the library we provide.   Where’s the -&gt; Operator?   In C and C++, two different operators are used for calling methods: you use . if you’re calling a method on the object directly and -&gt; if you’re calling the method on a pointer to the object and need to dereference the pointer first. In other words, if object is a pointer, object-&gt;something() is similar to (*object).something().   Rust doesn’t have an equivalent to the -&gt; operator; instead, Rust has a feature called automatic referencing and dereferencing. Calling methods is one of the few places in Rust that has this behavior.   Here’s how it works: when you call a method with object.something(), Rust automatically adds in &amp;, &amp;mut, or * so object matches the signature of the method. In other words, the following are the same:   p1.distance(&amp;p2);  (&amp;p1).distance(&amp;p2);  The first one looks much cleaner. This automatic referencing behavior works because methods have a clear receiver—the type of self. Given the receiver and name of a method, Rust can figure out definitively whether the method is reading (&amp;self), mutating (&amp;mut self), or consuming (self). The fact that Rust makes borrowing implicit for method receivers is a big part of making ownership ergonomic in practice.   Methods with More Parameters  Let’s practice using methods by implementing a second method on the Rectangle struct. This time, we want an instance of Rectangle to take another instance of Rectangle and return true if the second Rectangle can fit completely within self; otherwise it should return false. That is, we want to be able to write the program shown in Listing 5-14, once we’ve defined the can_hold method.  fn main() {     let rect1 = Rectangle { length: 50, width: 30 };     let rect2 = Rectangle { length: 40, width: 10 };     let rect3 = Rectangle { length: 45, width: 60 };      println!(\"Can rect1 hold rect2? {}\", rect1.can_hold(&amp;rect2));     println!(\"Can rect1 hold rect3? {}\", rect1.can_hold(&amp;rect3)); }  Listing 5-14: Using the as-yet-unwritten can_hold method  And the expected output would look like the following, because both dimensions of rect2 are smaller than the dimensions of rect1 but rect3 is wider than rect1:  Can rect1 hold rect2? true Can rect1 hold rect3? false  We know we want to define a method, so it will be within the impl Rectangle block. The method name will be can_hold, and it will take an immutable borrow of another Rectangle as a parameter. We can tell what the type of the parameter will be by looking at the code that calls the method: rect1.can_hold(&amp;rect2) passes in &amp;rect2, which is an immutable borrow to rect2, an instance of Rectangle. This makes sense because we only need to read rect2 (rather than write, which would mean we’d need a mutable borrow), and we want main to retain ownership of rect2 so we can use it again after calling the can_hold method. The return value of can_hold will be a Boolean, and the implementation will check whether the width and height of self are both greater than the width and height of the other Rectangle, respectively. Let’s add the new can_hold method to the impl block from Listing 5-13, shown in Listing 5-15.  impl Rectangle {     fn area(&amp;self) -&gt; u32 {         self.width * self.height     }      fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool {         self.width &gt; other.width &amp;&amp; self.height &gt; other.height     } }  Listing 5-15: Implementing the can_hold method on Rectangle that takes another Rectangle instance as a parameter   Associated Functions  Another useful feature of impl blocks is that we’re allowed to define functions within impl blocks that don’t take self as a parameter. These are called associated functions because they’re associated with the struct. They’re still functions, not methods, because they don’t have an instance of the struct to work with. You’ve already used the String::from associated function.   Associated functions are often used for constructors that will return a new instance of the struct.  impl Rectangle {     fn square(size: u32) -&gt; Rectangle {         Rectangle { length: size, width: size }     } }  To call this associated function, we use the :: syntax with the struct name; let sq = Rectangle::square(3); is an example. This function is namespaced by the struct: the :: syntax is used for both associated functions and namespaces created by modules. We’ll discuss modules in Chapter 7.   Multiple impl Blocks  Each struct is allowed to have multiple impl blocks. For example, Listing 5-15 is equivalent to the code shown in Listing 5-16, which has each method in its own impl block.  impl Rectangle {     fn area(&amp;self) -&gt; u32 {         self.width * self.height     } }  impl Rectangle {     fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool {         self.width &gt; other.width &amp;&amp; self.height &gt; other.height     } }  Listing 5-16: Rewriting Listing 5-15 using multiple impl blocks  There’s no reason to separate these methods into multiple impl blocks here, but this is valid syntax. We’ll see a case in which multiple impl blocks are useful in Chapter 10, where we discuss generic types and traits.   Summary  Structs let you create custom types that are meaningful for your domain. By using structs, you can keep associated pieces of data connected to each other and name each piece to make your code clear. Methods let you specify the behavior that instances of your structs have, and associated functions let you namespace functionality that is particular to your struct without having an instance available.   But structs aren’t the only way you can create custom types: let’s turn to Rust’s enum feature to add another tool to your toolbox.  ","categories": ["RUST Language"],
        "tags": ["Structure","Struct"],
        "url": "https://jjungs-lee.github.io//rust/5.Using-Structs-to-Structure-Related-Data",
        "teaser":null},{
        "title": "RUST : 6. Enums and Pattern Matching",
        "excerpt":"Enums allow you to define a type by enumerating its possible variants.  Next, we’ll explore a particularly useful enum, called Option, which expresses that a value can be either something or nothing.  Then we’ll look at how pattern matching in the match expression makes it easy to run different code for different values of an enum.  Finally, we’ll cover how the if let construct is another convenient and concise idiom available to you to handle enums in your code.   Rust’s enums are most similar to algebraic data types in functional languages, such as F#, OCaml, and Haskell.   Defining an Enum  Let’s look at a situation we might want to express in code and see why enums are useful and more appropriate than structs in this case.  Say we need to work with IP addresses. Currently, two major standards are used for IP addresses: version four and version six. we can enumerate all possible variants, which is where enumeration gets its name.   enum IpAddrKind {     V4,     V6, }  Enum Values  We can create instances of each of the two variants of IpAddrKind like this:  let four = IpAddrKind::V4; let six = IpAddrKind::V6;  Note that the variants of the enum are namespaced under its identifier, and we use a double colon to separate the two.  The reason this is useful is that now both values IpAddrKind::V4 and IpAddrKind::V6 are of the same type: IpAddrKind. We can then, for instance, define a function that takes any IpAddrKind:  fn route(ip_kind: IpAddrKind) { }  And we can call this function with either variant:  route(IpAddrKind::V4); route(IpAddrKind::V6);  Using enums has even more advantages. Thinking more about our IP address type, at the moment we don’t have a way to store the actual IP address data; we only know what kind it is. Given that you just learned about structs in Chapter 5, you might tackle this problem as shown in Listing 6-1.  enum IpAddrKind {     V4,     V6, }  struct IpAddr {     kind: IpAddrKind,     address: String, }  let home = IpAddr {     kind: IpAddrKind::V4,     address: String::from(\"127.0.0.1\"), };  let loopback = IpAddr {     kind: IpAddrKind::V6,     address: String::from(\"::1\"), };  Listing 6-1: Storing the data and IpAddrKind variant of an IP address using a struct  Here, we’ve defined a struct IpAddr that has two fields: a kind field that is of type IpAddrKind (the enum we defined previously) and an address field of type String. We have two instances of this struct. The first, home, has the value IpAddrKind::V4 as its kind with associated address data of 127.0.0.1. The second instance, loopback, has the other variant of IpAddrKind as its kind value, V6, and has address ::1 associated with it. We’ve used a struct to bundle the kind and address values together, so now the variant is associated with the value.   We can represent the same concept in a more concise way using just an enum, rather than an enum inside a struct, by putting data directly into each enum variant. This new definition of the IpAddr enum says that both V4 and V6 variants will have associated String values:  enum IpAddr {     V4(String),     V6(String), }  let home = IpAddr::V4(String::from(\"127.0.0.1\"));  let loopback = IpAddr::V6(String::from(\"::1\"));  We attach data to each variant of the enum directly, so there is no need for an extra struct.   There’s another advantage to using an enum rather than a struct: each variant can have different types and amounts of associated data. Version four type IP addresses will always have four numeric components that will have values between 0 and 255. If we wanted to store V4 addresses as four u8 values but still express V6 addresses as one String value, we wouldn’t be able to with a struct. Enums handle this case with ease:  enum IpAddr {     V4(u8, u8, u8, u8),     V6(String), }  let home = IpAddr::V4(127, 0, 0, 1);  let loopback = IpAddr::V6(String::from(\"::1\"));  We’ve shown several different ways to define data structures to store version four and version six IP addresses. However, as it turns out, wanting to store IP addresses and encode which kind they are is so common that the standard library has a definition we can use! It has the exact enum and variants that we’ve defined and used, but it embeds the address data inside the variants in the form of two different structs, which are defined differently for each variant:  struct Ipv4Addr {     // --snip-- }  struct Ipv6Addr {     // --snip-- }  enum IpAddr {     V4(Ipv4Addr),     V6(Ipv6Addr), }  This code illustrates that you can put any kind of data inside an enum variant: strings, numeric types, or structs, for example.You can even include another enum!   Note that even though the standard library contains a definition for IpAddr, we can still create and use our own definition without conflict because we haven’t brought the standard library’s definition into our scope. We’ll talk more about bringing types into scope in Chapter 7.   Let’s look at another example of an enum in Listing 6-2: this one has a wide variety of types embedded in its variants.  enum Message {     Quit,     Move { x: i32, y: i32 },     Write(String),     ChangeColor(i32, i32, i32), }  Listing 6-2: A Message enum whose variants each store different amounts and types of values  This enum has four variants with different types:      Quit has no data associated with it at all.   Move includes an anonymous struct inside it.   Write includes a single String.   ChangeColor includes three i32 values.   Defining an enum with variants such as the ones in Listing 6-2 is similar to defining different kinds of struct definitions, except the enum doesn’t use the struct keyword and all the variants are grouped together under the Message type. The following structs could hold the same data that the preceding enum variants hold:  struct QuitMessage; // unit struct struct MoveMessage {     x: i32,     y: i32, } struct WriteMessage(String); // tuple struct struct ChangeColorMessage(i32, i32, i32); // tuple struct  But if we used the different structs, which each have their own type, we couldn’t as easily define a function to take any of these kinds of messages as we could with the Message enum defined in Listing 6-2, which is a single type.   There is one more similarity between enums and structs: just as we’re able to define methods on structs using impl, we’re also able to define methods on enums.  Here’s a method named call that we could define on our Message enum:  impl Message {     fn call(&amp;self) {         // method body would be defined here     } }  let m = Message::Write(String::from(\"hello\")); m.call();  The body of the method would use self to get the value that we called the method on. In this example, we’ve created a variable m that has the value Message::Write(String::from(\"hello\")), and that is what self will be in the body of the call method when m.call() runs.   The Option Enum and Its Advantages Over Null Values  This section explores a case study of Option, which is another enum defined by the standard library.  The Option type is used in many places because it encodes the very common scenario in which a value could be something or it could be nothing.  Expressing this concept in terms of the type system means the compiler can check whether you’ve handled all the cases you should be handling;  this functionality can prevent bugs that are extremely common in other programming languages.   Programming language design is often thought of in terms of which features you include, but the features you exclude are important too.  Rust doesn’t have the null feature that many other languages have.  Null is a value that means there is no value there. In languages with null, variables can always be in one of two states: null or not-null.   In his 2009 presentation “Null References: The Billion Dollar Mistake,” Tony Hoare, the inventor of null, has this to say:   I call it my billion-dollar mistake. At that time, I was designing the first comprehensive type system for references in an object-oriented language. My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn’t resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years.   The problem with null values is that if you try to use a null value as a not-null value, you’ll get an error of some kind. Because this null or not-null property is pervasive, it’s extremely easy to make this kind of error.   However, the concept that null is trying to express is still a useful one: a null is a value that is currently invalid or absent for some reason.   The problem isn’t really with the concept but with the particular implementation. As such, Rust does not have nulls, but it does have an enum that can encode the concept of a value being present or absent. This enum is Option&lt;T&gt;, and it is defined by the standard library as follows:  enum Option&lt;T&gt; {     Some(T),     None, }  The Option enum is so useful that it’s even included in the prelude; you don’t need to bring it into scope explicitly.  In addition, so are its variants: you can use `Some` and `None` directly without the `Option::` prefix. The `Option` enum is still just a regular enum, and `Some(T)` and `None` are still variants of type `Option`.   The &lt;T&gt; syntax is a feature of Rust we haven’t talked about yet. It’s a generic type parameter, and we’ll cover generics in more detail in Chapter 10.  For now, all you need to know is that &lt;T&gt; means the Some variant of the Option enum can hold one piece of data of any type.  Here are some examples of using Option values to hold number types and string types:  let some_number = Some(5); let some_string = Some(\"a string\");  let absent_number: Option&lt;i32&gt; = None;  If we use None rather than Some, we need to tell Rust what type of Option&lt;T&gt; we have, because the compiler can’t infer the type that the Some variant will hold by looking only at a None value.   When we have a Some value, we know that a value is present and the value is held within the Some. When we have a None value, in some sense, it means the same thing as null: we don’t have a valid value. So why is having Option&lt;T&gt; any better than having null?   In short, because Option&lt;T&gt; and T (where T can be any type) are different types, the compiler won’t let us use an Option&lt;T&gt; value as if it were definitely a valid value. For example, this code won’t compile because it’s trying to add an i8 to an Option&lt;i8&gt;:  let x: i8 = 5; let y: Option&lt;i8&gt; = Some(5);  let sum = x + y;  If we run this code, we get an error message like this:  error[E0277]: the trait bound `i8: std::ops::Add&lt;std::option::Option&lt;i8&gt;&gt;` is not satisfied  --&gt;   | 5 |     let sum = x + y;   |                 ^ no implementation for `i8 + std::option::Option&lt;i8&gt;`   |  Intense! In effect, this error message means that Rust doesn’t understand how to add an i8 and an Option&lt;i8&gt;, because they’re different types. When we have a value of a type like i8 in Rust, the compiler will ensure that we always have a valid value. We can proceed confidently without having to check for null before using that value. Only when we have an Option&lt;i8&gt; (or whatever type of value we’re working with) do we have to worry about possibly not having a value, and the compiler will make sure we handle that case before using the value.   In other words, you have to convert an Option&lt;T&gt; to a T before you can perform T operations with it. Generally, this helps catch one of the most common issues with null: assuming that something isn’t null when it actually is.   Not having to worry about incorrectly assuming a not-null value helps you to be more confident in your code. In order to have a value that can possibly be null, you must explicitly opt in by making the type of that value Option&lt;T&gt;. Then, when you use that value, you are required to explicitly handle the case when the value is null. Everywhere that a value has a type that isn’t an Option&lt;T&gt;, you can safely assume that the value isn’t null. This was a deliberate design decision for Rust to limit null’s pervasiveness and increase the safety of Rust code.   So, how do you get the T value out of a Some variant when you have a value of type Option&lt;T&gt; so you can use that value? The Option&lt;T&gt; enum has a large number of methods that are useful in a variety of situations; you can check them out in its documentation. Becoming familiar with the methods on Option will be extremely useful in your journey with Rust.   In general, in order to use an Option&lt;T&gt; value, you want to have code that will handle each variant. You want some code that will run only when you have a Some(T) value, and this code is allowed to use the inner T. You want some other code to run if you have a None value, and that code doesn’t have a T value available. The match expression is a control flow construct that does just this when used with enums: it will run different code depending on which variant of the enum it has, and that code can use the data inside the matching value.   The match Control Flow Operator  Rust has an extremely powerful control flow operator called match that allows you to compare a value against a series of patterns and then execute code based on which pattern matches. Patterns can be made up of literal values, variable names, wildcards, and many other things; Chapter 18 covers all the different kinds of patterns and what they do. The power of match comes from the expressiveness of the patterns and the fact that the compiler confirms that all possible cases are handled.   Think of a match expression as being like a coin-sorting machine: coins slide down a track with variously sized holes along it, and each coin falls through the first hole it encounters that it fits into. In the same way, values go through each pattern in a match, and at the first pattern the value “fits,” the value falls into the associated code block to be used during execution.   Because we just mentioned coins, let’s use them as an example using match! We can write a function that can take an unknown United States coin and, in a similar way as the counting machine, determine which coin it is and return its value in cents, as shown here in Listing 6-3.  enum Coin {     Penny,     Nickel,     Dime,     Quarter, }  fn value_in_cents(coin: Coin) -&gt; u8 {     // JHS) similar to 'case' in C/C++     match coin {         Coin::Penny =&gt; 1,         Coin::Nickel =&gt; 5,         Coin::Dime =&gt; 10,         Coin::Quarter =&gt; 25,     }  Listing 6-3: An enum and a match expression that has the variants of the enum as its patterns   Let’s break down the match in the value_in_cents function. First, we list the match keyword followed by an expression, which in this case is the value coin. This seems very similar to an expression used with if, but there’s a big difference: with if, the expression needs to return a Boolean value, but here, it can be any type. The type of coin in this example is the Coin enum that we defined on line 1.   Next are the match arms. An arm has two parts: a pattern and some code. The first arm here has a pattern that is the value Coin::Penny and then the =&gt; operator that separates the pattern and the code to run. The code in this case is just the value 1. Each arm is separated from the next with a comma(,).   When the match expression executes, it compares the resulting value against the pattern of each arm, in order. If a pattern matches the value, the code associated with that pattern is executed. If that pattern doesn’t match the value, execution continues to the next arm, much as in a coin-sorting machine. We can have as many arms as we need: in Listing 6-3, our match has four arms.   The code associated with each arm is an expression, and the resulting value of the expression in the matching arm is the value that gets returned for the entire match expression.   Curly brackets typically aren’t used if the match arm code is short, as it is in Listing 6-3 where each arm just returns a value. If you want to run multiple lines of code in a match arm, you can use curly brackets. For example, the following code would print “Lucky penny!” every time the method was called with a Coin::Penny but would still return the last value of the block, 1:   Patterns that Bind to Values  Another useful feature of match arms is that they can bind to the parts of the values that match the pattern. This is how we can extract values out of enum variants.   As an example, let’s change one of our enum variants to hold data inside it. From 1999 through 2008, the United States minted quarters with different designs for each of the 50 states on one side. No other coins got state designs, so only quarters have this extra value. We can add this information to our enum by changing the Quarter variant to include a UsState value stored inside it, which we’ve done here in Listing 6-4.  #[derive(Debug)] // so we can inspect the state in a minute enum UsState {     Alabama,     Alaska,     // --snip-- }  enum Coin {     Penny,     Nickel,     Dime,     Quarter(UsState), }  Listing 6-4: A Coin enum in which the Quarter variant also holds a UsState value  Let’s imagine that a friend of ours is trying to collect all 50 state quarters. While we sort our loose change by coin type, we’ll also call out the name of the state associated with each quarter so if it’s one our friend doesn’t have, they can add it to their collection.   In the match expression for this code, we add a variable called state to the pattern that matches values of the variant Coin::Quarter. When a Coin::Quarter matches, the state variable will bind to the value of that quarter’s state. Then we can use state in the code for that arm, like so:  fn value_in_cents(coin: Coin) -&gt; u8 {     match coin {         Coin::Penny =&gt; 1,         Coin::Nickel =&gt; 5,         Coin::Dime =&gt; 10,         Coin::Quarter(state) =&gt; {             println!(\"State quarter from {:?}!\", state);             25         },     } }  If we were to call value_in_cents(Coin::Quarter(UsState::Alaska)), coin would be Coin::Quarter(UsState::Alaska). When we compare that value with each of the match arms, none of them match until we reach Coin::Quarter(state). At that point, the binding for state will be the value UsState::Alaska. We can then use that binding in the println! expression, thus getting the inner state value out of the Coin enum variant for Quarter.   Matching with Option&lt;T&gt;  In the previous section, we wanted to get the inner T value out of the Some case when using Option&lt;T&gt;; we can also handle Option&lt;T&gt; using match as we did with the Coin enum! Instead of comparing coins, we’ll compare the variants of Option&lt;T&gt;, but the way that the match expression works remains the same.   Let’s say we want to write a function that takes an Option&lt;i32&gt; and, if there’s a value inside, adds 1 to that value. If there isn’t a value inside, the function should return the None value and not attempt to perform any operations.   This function is very easy to write, thanks to match, and will look like Listing 6-5.  fn plus_one(x: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; {     match x {         None =&gt; None,         Some(i) =&gt; Some(i + 1),     } }  let five = Some(5); let six = plus_one(five); let none = plus_one(None);  Listing 6-5: A function that uses a match expression on an Option   Let’s examine the first execution of plus_one in more detail. When we call plus_one(five), the variable x in the body of plus_one will have the value Some(5). We then compare that against each match arm.  None =&gt; None,  The Some(5) value doesn’t match the pattern None, so we continue to the next arm.  Some(i) =&gt; Some(i + 1),  Does Some(5) match Some(i)? Why yes it does! We have the same variant. The i binds to the value contained in Some, so i takes the value 5. The code in the match arm is then executed, so we add 1 to the value of i and create a new Some value with our total 6 inside.   Now let’s consider the second call of plus_one in Listing 6-5, where x is None. We enter the match and compare to the first arm.  None =&gt; None,  It matches! There’s no value to add to, so the program stops and returns the None value on the right side of =&gt;. Because the first arm matched, no other arms are compared.   Combining match and enums is useful in many situations. You’ll see this pattern a lot in Rust code: match against an enum, bind a variable to the data inside, and then execute code based on it. It’s a bit tricky at first, but once you get used to it, you’ll wish you had it in all languages. It’s consistently a user favorite.   Matches Are Exhaustive  There’s one other aspect of match we need to discuss. Consider this version of our plus_one function that has a bug and won’t compile:  fn plus_one(x: Option&lt;i32&gt;) -&gt; Option&lt;i32&gt; {     match x {         Some(i) =&gt; Some(i + 1),     } }  We didn’t handle the None case, so this code will cause a bug. Luckily, it’s a bug Rust knows how to catch. If we try to compile this code, we’ll get this error:  error[E0004]: non-exhaustive patterns: `None` not covered  --&gt;   | 6 |         match x {   |               ^ pattern `None` not covered  Rust knows that we didn’t cover every possible case and even knows which pattern we forgot! Matches in Rust are exhaustive: we must exhaust every last possibility in order for the code to be valid. Especially in the case of Option&lt;T&gt;, when Rust prevents us from forgetting to explicitly handle the None case, it protects us from assuming that we have a value when we might have null, thus making the billion-dollar mistake discussed earlier.   The _ Placeholder  Rust also has a pattern we can use when we don’t want to list all possible values.  For example, a u8 can have valid values of 0 through 255. If we only care about the values 1, 3, 5, and 7, we don’t want to have to list out 0, 2, 4, 6, 8, 9 all the way up to 255. Fortunately, we don’t have to: we can use the special pattern _ instead:  let some_u8_value = 0u8; match some_u8_value {     1 =&gt; println!(\"one\"),     3 =&gt; println!(\"three\"),     5 =&gt; println!(\"five\"),     7 =&gt; println!(\"seven\"),     _ =&gt; (), }  The _ pattern will match any value. By putting it after our other arms, the _ will match all the possible cases that aren’t specified before it. The () is just the unit value, so nothing will happen in the _ case. As a result, we can say that we want to do nothing for all the possible values that we don’t list before the _ placeholder.   However, the match expression can be a bit wordy in a situation in which we care about only one of the cases. For this situation, Rust provides if let.   Concise Control Flow with if let  The if let syntax lets you combine if and let into a less verbose way to handle values that match one pattern while ignoring the rest.  Consider the program in Listing 6-6 that matches on an Option&lt;u8&gt; value but only wants to execute code if the value is 3.  let some_u8_value = Some(0u8); match some_u8_value {     Some(3) =&gt; println!(\"three\"),     _ =&gt; (), }  Listing 6-6: A match that only cares about executing code when the value is Some(3)  We want to do something with the Some(3) match but do nothing with any other Some&lt;u8&gt; value or the None value.  To satisfy the match expression, we have to add _ =&gt; () after processing just one variant, which is a lot of boilerplate code to add.   Instead, we could write this in a shorter way using if let. The following code behaves the same as the match in Listing 6-6:  if let Some(3) = some_u8_value {     println!(\"three\"); }  The syntax if let takes a pattern and an expression separated by an equal(=) sign. It works the same way as a match, where the expression is given to the match and the pattern is its first arm.   Using if let means less typing, less indentation, and less boilerplate code. However, you lose the exhaustive checking that match enforces. Choosing between match and if let depends on what you’re doing in your particular situation and whether gaining conciseness is an appropriate trade-off for losing exhaustive checking.   In other words, you can think of if let as syntax sugar for a match that runs code when the value matches one pattern and then ignores all other values.   We can include an else with an if let. The block of code that goes with the else is the same as the block of code that would go with the _ case in the match expression that is equivalent to the if let and else. Recall the Coin enum definition in Listing 6-4, where the Quarter variant also held a UsState value. If we wanted to count all non-quarter coins we see while also announcing the state of the quarters, we could do that with a match expression like this:  let mut count = 0; match coin {     Coin::Quarter(state) =&gt; println!(\"State quarter from {:?}!\", state),     _ =&gt; count += 1, }  Or we could use an if let and else expression like this:  let mut count = 0; if let Coin::Quarter(state) = coin {     println!(\"State quarter from {:?}!\", state); } else {     count += 1; }  If you have a situation in which your program has logic that is too verbose to express using a match, remember that if let is in your Rust toolbox as well.   Summary  We’ve now covered how to use enums to create custom types that can be one of a set of enumerated values. We’ve shown how the standard library’s Option&lt;T&gt; type helps you use the type system to prevent errors. When enum values have data inside them, you can use match or if let to extract and use those values, depending on how many cases you need to handle.   Your Rust programs can now express concepts in your domain using structs and enums. Creating custom types to use in your API ensures type safety: the compiler will make certain your functions get only values of the type each function expects.   In order to provide a well-organized API to your users that is straightforward to use and only exposes exactly what your users will need, let’s now turn to Rust’s modules.  ","categories": ["RUST Language"],
        "tags": ["Enum","Enumerations","Pattern","Matching","Pattern Matching"],
        "url": "https://jjungs-lee.github.io//rust/6.Enums-and-Pattern-Matching",
        "teaser":null},{
        "title": "RUST : 7. Managing Growing Projects with Packages, Crates, and Modules",
        "excerpt":"As you write large programs, organizing your code will be important because keeping track of your entire program in your head will become impossible. By grouping related functionality and separating code with distinct features, you’ll clarify where to find code that implements a particular feature and where to go to change how a feature works.   Packages and Crates  The first parts of the module system we’ll cover are packages and crates. A crate is a binary or library. The crate root is a source file that the Rust compiler starts from and makes up the root module of your crate (we’ll explain modules in depth in the “Defining Modules to Control Scope and Privacy”) section. A package is one or more crates that provide a set of functionality. A package contains a Cargo.toml file that describes how to build those crates.   Several rules determine what a package can contain. A package must contain zero or one library crates, and no more. It can contain as many binary crates as you’d like, but it must contain at least one crate (either library or binary).   Let’s walk through what happens when we create a package. First, we enter the command cargo new:  $ cargo new my-project      Created binary (application) `my-project` package $ ls my-project Cargo.toml src $ ls my-project/src main.rs   When we entered the command, Cargo created a Cargo.toml file, giving us a package. Looking at the contents of Cargo.toml, there’s no mention of src/main.rs because Cargo follows a convention that src/main.rs is the crate root of a binary crate with the same name as the package. Likewise, Cargo knows that if the package directory contains src/lib.rs, the package contains a library crate with the same name as the package, and src/lib.rs is its crate root. Cargo passes the crate root files to rustc to build the library or binary.   Here, we have a package that only contains src/main.rs, meaning it only contains a binary crate named my-project. If a package contains src/main.rs and src/lib.rs, it has two crates: a library and a binary, both with the same name as the package. A package can have multiple binary crates by placing files in the src/bin directory: each file will be a separate binary crate.   A crate will group related functionality together in a scope so the functionality is easy to share between multiple projects. For example, the rand crate we used in Chapter 2 provides functionality that generates random numbers. We can use that functionality in our own projects by bringing the rand crate into our project’s scope. All the functionality provided by the rand crate is accessible through the crate’s name, rand.   Keeping a crate’s functionality in its own scope clarifies whether particular functionality is defined in our crate or the rand crate and prevents potential conflicts. For example, the rand crate provides a trait named Rng. We can also define a struct named Rng in our own crate. Because a crate’s functionality is namespaced in its own scope, when we add rand as a dependency, the compiler isn’t confused about what the name Rng refers to. In our crate, it refers to the struct Rng that we defined. We would access the Rng trait from the rand crate as rand::Rng.   Defining Modules to Control Scope and Privacy  In this section, we’ll talk about modules and other parts of the module system, namely paths that allow you to name items; the use keyword that brings a path into scope; and the pub keyword to make items public. We’ll also discuss the as keyword, external packages, and the glob operator. For now, let’s focus on modules!   Modules let us organize code within a crate into groups for readability and easy reuse. Modules also control the privacy of items, which is whether an item can be used by outside code (public) or is an internal implementation detail and not available for outside use (private).   As an example, let’s write a library crate that provides the functionality of a restaurant. We’ll define the signatures of functions but leave their bodies empty to concentrate on the organization of the code, rather than actually implement a restaurant in code.   In the restaurant industry, some parts of a restaurant are referred to as front of house and others as back of house. Front of house is where customers are; this is where hosts seat customers, servers take orders and payment, and bartenders make drinks. Back of house is where the chefs and cooks work in the kitchen, dishwashers clean up, and managers do administrative work.   To structure our crate in the same way that a real restaurant works, we can organize the functions into nested modules. Create a new library named restaurant by running cargo new --lib restaurant; then put the code in Listing 7-1 into src/lib.rs to define some modules and function signatures.  // Filename: src/lib.rs mod front_of_house {     mod hosting {         fn add_to_waitlist() {}         fn seat_at_table() {}     }      mod serving {         fn take_order() {}         fn serve_order() {}         fn take_payment() {}     } }  Listing 7-1: A front_of_house module containing other modules that then contain functions   We define a module by starting with the mod keyword and then specify the name of the module (in this case, front_of_house) and place curly brackets around the body of the module. Inside modules, we can have other modules, as in this case with the modules hosting and serving. Modules can also hold definitions for other items, such as structs, enums, constants, traits, or—as in Listing 7-1—functions.   By using modules, we can group related definitions together and name why they’re related. Programmers using this code would have an easier time finding the definitions they wanted to use because they could navigate the code based on the groups rather than having to read through all the definitions. Programmers adding new functionality to this code would know where to place the code to keep the program organized.   Earlier, we mentioned that src/main.rs and src/lib.rs are called crate roots. The reason for their name is that the contents of either of these two files form a module named crate at the root of the crate’s module structure, known as the module tree.   Listing 7-2 shows the module tree for the structure in Listing 7-1.  crate  └── front_of_house      ├── hosting      │   ├── add_to_waitlist      │   └── seat_at_table      └── serving          ├── take_order          ├── serve_order          └── take_payment  Listing 7-2: The module tree for the code in Listing 7-1  This tree shows how some of the modules nest inside one another (for example, hosting nests inside front_of_house). The tree also shows that some modules are siblings to each other, meaning they’re defined in the same module (hosting and serving are defined within front_of_house). To continue the family metaphor, if module A is contained inside module B, we say that module A is the child of module B and that module B is the parent of module A. Notice that the entire module tree is rooted under the implicit module named crate.   The module tree might remind you of the filesystem’s directory tree on your computer; this is a very apt comparison! Just like directories in a filesystem, you use modules to organize your code. And just like files in a directory, we need a way to find our modules.   Paths for Referring to an Item in the Module Tree  To show Rust where to find an item in a module tree, we use a path in the same way we use a path when navigating a filesystem. If we want to call a function, we need to know its path.   A path can take two forms:     An absolute path starts from a crate root by using a crate name or a literal crate.   A relative path starts from the current module and uses self, super, or an identifier in the current module.   Both absolute and relative paths are followed by one or more identifiers separated by double colons (::).   Let’s return to the example in Listing 7-1. How do we call the add_to_waitlist function? This is the same as asking, what’s the path of the add_to_waitlist function? In Listing 7-3, we simplified our code a bit by removing some of the modules and functions. We’ll show two ways to call the add_to_waitlist function from a new function eat_at_restaurant defined in the crate root. The eat_at_restaurant function is part of our library crate’s public API, so we mark it with the pub keyword. In the ”Exposing Paths with the pub Keyword” section, we’ll go into more detail about pub. Note that this example won’t compile just yet; we’ll explain why in a bit.   mod front_of_house {     mod hosting {         fn add_to_waitlist() {}     } }  pub fn eat_at_restaurant() {     // Absolute path     crate::front_of_house::hosting::add_to_waitlist();      // Relative path     front_of_house::hosting::add_to_waitlist(); }  Listing 7-3: Calling the add_to_waitlist function using absolute and relative paths   The first time we call the add_to_waitlist function in eat_at_restaurant, we use an absolute path. The add_to_waitlist function is defined in the same crate as eat_at_restaurant, which means we can use the crate keyword to start an absolute path.   After crate, we include each of the successive modules until we make our way to add_to_waitlist. You can imagine a filesystem with the same structure, and we’d specify the path /front_of_house/hosting/add_to_waitlist to run the add_to_waitlist program; using the crate name to start from the crate root is like using / to start from the filesystem root in your shell.   The second time we call add_to_waitlist in eat_at_restaurant, we use a relative path. The path starts with front_of_house, the name of the module defined at the same level of the module tree as eat_at_restaurant. Here the filesystem equivalent would be using the path front_of_house/hosting/add_to_waitlist. Starting with a name means that the path is relative.   Choosing whether to use a relative or absolute path is a decision you’ll make based on your project.  The decision should depend on whether you’re more likely to move item definition code separately from or together with the code that uses the item.  For example, if we move the front_of_house module and the eat_at_restaurant function into a module named customer_experience, we’d need to update the absolute path to add_to_waitlist, but the relative path would still be valid. However, if we moved the eat_at_restaurant function separately into a module named dining, the absolute path to the add_to_waitlist call would stay the same, but the relative path would need to be updated. Our preference is to specify absolute paths because it’s more likely to move code definitions and item calls independently of each other.   Let’s try to compile Listing 7-3 and find out why it won’t compile yet! The error we get is shown in Listing 7-4.  $ cargo build    Compiling restaurant v0.1.0 (file:///projects/restaurant) error[E0603]: module `hosting` is private  --&gt; src/lib.rs:9:28   | 9 |     crate::front_of_house::hosting::add_to_waitlist();   |                            ^^^^^^^  error[E0603]: module `hosting` is private   --&gt; src/lib.rs:12:21    | 12 |     front_of_house::hosting::add_to_waitlist();    |                     ^^^^^^^  Listing 7-4: Compiler errors from building the code in Listing 7-3   The error messages say that module hosting is private. In other words, we have the correct paths for the hosting module and the add_to_waitlist function, but Rust won’t let us use them because it doesn’t have access to the private sections.   Modules aren’t useful only for organizing your code. They also define Rust’s privacy boundary: the line that encapsulates the implementation details external code isn’t allowed to know about, call, or rely on. So, if you want to make an item like a function or struct private, you put it in a module.   The way privacy works in Rust is that all items (functions, methods, structs, enums, modules, and constants) are private by default. Items in a parent module can’t use the private items inside child modules, but items in child modules can use the items in their ancestor modules. The reason is that child modules wrap and hide their implementation details, but the child modules can see the context in which they’re defined. To continue with the restaurant metaphor, think of the privacy rules as being like the back office of a restaurant: what goes on in there is private to restaurant customers, but office managers can see and do everything in the restaurant in which they operate.   Rust chose to have the module system function this way so that hiding inner implementation details is the default. That way, you know which parts of the inner code you can change without breaking outer code. But you can expose inner parts of child modules code to outer ancestor modules by using the pub keyword to make an item public.   Exposing Paths with the pub Keyword  Let’s return to the error in Listing 7-4 that told us the hosting module is private.  We want the eat_at_restaurant function in the parent module to have access to the add_to_waitlist function in the child module, so we mark the hosting module with the pub keyword, as shown in Listing 7-5.   mod front_of_house {     pub mod hosting {         fn add_to_waitlist() {}     } }  pub fn eat_at_restaurant() {     // Absolute path     crate::front_of_house::hosting::add_to_waitlist();      // Relative path     front_of_house::hosting::add_to_waitlist(); }  Listing 7-5: Declaring the hosting module as pub to use it from eat_at_restaurant  Unfortunately, the code in Listing 7-5 still results in an error, as shown in Listing 7-6.  $ cargo build    Compiling restaurant v0.1.0 (file:///projects/restaurant) error[E0603]: function `add_to_waitlist` is private  --&gt; src/lib.rs:9:37   | 9 |     crate::front_of_house::hosting::add_to_waitlist();   |                                     ^^^^^^^^^^^^^^^  error[E0603]: function `add_to_waitlist` is private   --&gt; src/lib.rs:12:30    | 12 |     front_of_house::hosting::add_to_waitlist();    |                              ^^^^^^^^^^^^^^^  What happened? Adding the pub keyword in front of mod hosting makes the module public.  With this change, if we can access front_of_house, we can access hosting. But the contents of hosting are still private;  making the module public doesn’t make its contents public. The pub keyword on a module only lets code in its ancestor modules refer to it.   The errors in Listing 7-6 say that the add_to_waitlist function is private. The privacy rules apply to structs, enums, functions, and methods as well as modules.   Let’s also make the add_to_waitlist function public by adding the pub keyword before its definition, as in Listing 7-7.  mod front_of_house {     pub mod hosting {         pub fn add_to_waitlist() {}     } }  pub fn eat_at_restaurant() {     // Absolute path     crate::front_of_house::hosting::add_to_waitlist();      // Relative path     front_of_house::hosting::add_to_waitlist(); }  Listing 7-7: Adding the pub keyword to mod hosting and fn add_to_waitlist lets us call the function from eat_at_restaurant  Now the code will compile! Let’s look at the absolute and the relative path and double-check why adding the pub keyword lets us use these paths in add_to_waitlist with respect to the privacy rules.   In the absolute path, we start with crate, the root of our crate’s module tree. Then the front_of_house module is defined in the crate root.  The front_of_house module isn’t public, but because the eat_at_restaurant function is defined in the same module as front_of_house (that is, eat_at_restaurant and front_of_house are siblings –&gt; think about directory), we can refer to front_of_house from eat_at_restaurant. Next is the hosting module marked with pub. We can access the parent module of hosting, so we can access hosting. Finally, the add_to_waitlist function is marked with pub and we can access its parent module, so this function call works!   In the relative path, the logic is the same as the absolute path except for the first step: rather than starting from the crate root, the path starts from front_of_house. The front_of_house module is defined within the same module as eat_at_restaurant, so the relative path starting from the module in which eat_at_restaurant is defined works. Then, because hosting and add_to_waitlist are marked with pub, the rest of the path works, and this function call is valid!   Starting Relative Paths with super  We can also construct relative paths that begin in the parent module by using super at the start of the path.  This is like starting a filesystem path with the .. syntax. Why would we want to do this?   Consider the code in Listing 7-8 that models the situation in which a chef fixes an incorrect order and personally brings it out to the customer.  The function fix_incorrect_order calls the function serve_order by specifying the path to serve_order starting with super:  fn serve_order() {}  mod back_of_house {     fn fix_incorrect_order() {         cook_order();         super::serve_order();     }      fn cook_order() {} }  Listing 7-8: Calling a function using a relative path starting with super  The fix_incorrect_order function is in the back_of_house module, so we can use super to go to the parent module of back_of_house, which in this case is crate, the root. From there, we look for serve_order and find it. Success!  We think the back_of_house module and the serve_order function are likely to stay in the same relationship to each other and get moved together should we decide to reorganize the crate’s module tree. Therefore, we used super so we’ll have fewer places to update code in the future if this code gets moved to a different module.   Making Structs and Enums Public  We can also use pub to designate structs and enums as public, but there are a few extra details.  If we use pub before a struct definition, we make the struct public, but the struct’s fields will still be private.  We can make each field public or not on a case-by-case basis.  In Listing 7-9, we’ve defined a public back_of_house::Breakfast struct with a public toast field but a private seasonal_fruit field.  This models the case in a restaurant where the customer can pick the type of bread that comes with a meal, but the chef decides which fruit accompanies the meal based on what’s in season and in stock. The available fruit changes quickly, so customers can’t choose the fruit or even see which fruit they’ll get.  mod back_of_house {     pub struct Breakfast {         pub toast: String,         seasonal_fruit: String,     }      impl Breakfast {         pub fn summer(toast: &amp;str) -&gt; Breakfast {             Breakfast {                 toast: String::from(toast),                 seasonal_fruit: String::from(\"peaches\"),             }         }     } }  pub fn eat_at_restaurant() {     // Order a breakfast in the summer with Rye toast     let mut meal = back_of_house::Breakfast::summer(\"Rye\");     // Change our mind about what bread we'd like     meal.toast = String::from(\"Wheat\");     println!(\"I'd like {} toast please\", meal.toast);      // The next line won't compile if we uncomment it; we're not allowed     // to see or modify the seasonal fruit that comes with the meal     // meal.seasonal_fruit = String::from(\"blueberries\"); }  Listing 7-9: A struct with some public fields and some private fields  Because the toast field in the back_of_house::Breakfast struct is public, in eat_at_restaurant we can write and read to the toast field using dot notation.  Notice that we can’t use the seasonal_fruit field in eat_at_restaurant because seasonal_fruit is private.  Try uncommenting the line modifying the seasonal_fruit field value to see what error you get!   Also, note that because back_of_house::Breakfast has a private field, the struct needs to provide a public associated function that constructs an instance of Breakfast (we’ve named it summer here). If Breakfast didn’t have such a function, we couldn’t create an instance of Breakfast in eat_at_restaurant because we couldn’t set the value of the private seasonal_fruit field in eat_at_restaurant.   In contrast, if we make an enum public, all of its variants are then public. We only need the pub before the enum keyword, as shown in Listing 7-10.  mod back_of_house {     pub enum Appetizer {         Soup,         Salad,     } }  pub fn eat_at_restaurant() {     let order1 = back_of_house::Appetizer::Soup;     let order2 = back_of_house::Appetizer::Salad; }  Listing 7-10: Designating an enum as public makes all its variants public  Because we made the Appetizer enum public, we can use the Soup and Salad variants in eat_at_restaurant.  Enums aren’t very useful unless their variants are public; it would be annoying to have to annotate all enum variants with pub in every case, so the default for enum variants is to be public.  Structs are often useful without their fields being public, so struct fields follow the general rule of everything being private by default unless annotated with pub.   There’s one more situation involving pub that we haven’t covered, and that is our last module system feature: the use keyword.  We’ll cover use by itself first, and then we’ll show how to combine pub and use.   Bringing Paths into Scope with the use Keyword  It might seem like the paths we’ve written to call functions so far are inconveniently long and repetitive.  For example, in Listing 7-7, whether we chose the absolute or relative path to the add_to_waitlist function, every time we wanted to call add_to_waitlist we had to specify front_of_house and hosting too.  Fortunately, there’s a way to simplify this process. We can bring a path into a scope once and then call the items in that path as if they’re local items with the use keyword.   In Listing 7-11, we bring the crate::front_of_house::hosting module into the scope of the eat_at_restaurant function so we only have to specify hosting::add_to_waitlist to call the add_to_waitlist function in eat_at_restaurant.   mod front_of_house {     pub mod hosting {         pub fn add_to_waitlist() {}     } }  use crate::front_of_house::hosting;  pub fn eat_at_restaurant() {     hosting::add_to_waitlist();     hosting::add_to_waitlist();     hosting::add_to_waitlist(); }  Listing 7-11: Bringing a module into scope with use   Adding use and a path in a scope is similar to creating a symbolic link in the filesystem. By adding use crate::front_of_house::hosting in the crate root, hosting is now a valid name in that scope, just as though the hosting module had been defined in the crate root. Paths brought into scope with use also check privacy, like any other paths.   You can also bring an item into scope with use and a relative path. Listing 7-12 shows how to specify a relative path to get the same behavior as in Listing 7-11.  mod front_of_house {     pub mod hosting {         pub fn add_to_waitlist() {}     } }  use front_of_house::hosting;  pub fn eat_at_restaurant() {     hosting::add_to_waitlist();     hosting::add_to_waitlist();     hosting::add_to_waitlist(); }  Listing 7-12: Bringing a module into scope with use and a relative path   Creating Idiomatic use Paths  In Listing 7-11, you might have wondered why we specified use crate::front_of_house::hosting and then called hosting::add_to_waitlist in eat_at_restaurant rather than specifying the use path all the way out to the add_to_waitlist function to achieve the same result, as in Listing 7-13.  mod front_of_house {     pub mod hosting {         pub fn add_to_waitlist() {}     } }  use crate::front_of_house::hosting::add_to_waitlist;  pub fn eat_at_restaurant() {     add_to_waitlist();     add_to_waitlist();     add_to_waitlist(); }  Listing 7-13: Bringing the add_to_waitlist function into scope with use, which is unidiomatic  Although both Listing 7-11 and 7-13 accomplish the same task, Listing 7-11 is the idiomatic way to bring a function into scope with use.  Bringing the function’s parent module into scope with use so we have to specify the parent module when calling the function makes it clear that the function isn’t locally defined while still minimizing repetition of the full path. The code in Listing 7-13 is unclear as to where add_to_waitlist is defined.   On the other hand, when bringing in structs, enums, and other items with use, it’s idiomatic to specify the full path.  Listing 7-14 shows the idiomatic way to bring the standard library’s HashMap struct into the scope of a binary crate.  use std::collections::HashMap;  fn main() {     let mut map = HashMap::new();     map.insert(1, 2); }  Listing 7-14: Bringing HashMap into scope in an idiomatic way   There’s no strong reason behind this idiom: it’s just the convention that has emerged, and folks have gotten used to reading and writing Rust code this way.   The exception to this idiom is if we’re bringing two items with the same name into scope with use statements, because Rust doesn’t allow that.  Listing 7-15 shows how to bring two Result types into scope that have the same name but different parent modules and how to refer to them.  use std::fmt; use std::io;  fn function1() -&gt; fmt::Result {     // --snip-- }  fn function2() -&gt; io::Result&lt;()&gt; {     // --snip-- }  Listing 7-15: Bringing two types with the same name into the same scope requires using their parent modules.  As you can see, using the parent modules distinguishes the two Result types.  If instead we specified use std::fmt::Result and use std::io::Result, we’d have two Result types in the same scope and Rust wouldn’t know which one we meant when we used Result.   Providing New Names with the as Keyword  When we bring a name into scope with the use keyword, the name available in the new scope is private.  To enable the code that calls our code to refer to that name as if it had been defined in that code’s scope, we can combine pub and use. This technique is called re-exporting because we’re bringing an item into scope but also making that item available for others to bring into their scope.   Listing 7-17 shows the code in Listing 7-11 with use in the root module changed to pub use.  mod front_of_house {     pub mod hosting {         pub fn add_to_waitlist() {}     } }  pub use crate::front_of_house::hosting;  pub fn eat_at_restaurant() {     hosting::add_to_waitlist();     hosting::add_to_waitlist();     hosting::add_to_waitlist(); }  Listing 7-17: Making a name available for any code to use from a new scope with pub use  By using pub use, external code can now call the add_to_waitlist function using hosting::add_to_waitlist.  If we hadn’t specified pub use, the eat_at_restaurant function could call hosting::add_to_waitlist in its scope, but external code couldn’t take advantage of this new path.   Re-exporting is useful when the internal structure of your code is different from how programmers calling your code would think about the domain. For example, in this restaurant metaphor, the people running the restaurant think about “front of house” and “back of house.”  But customers visiting a restaurant probably won’t think about the parts of the restaurant in those terms.  With pub use, we can write our code with one structure but expose a different structure. Doing so makes our library well organized for programmers working on the library and programmers calling the library.   Using External Packages  In Chapter 2, we programmed a guessing game project that used an external package called rand to get random numbers. To use rand in our project, we added this line to Cargo.toml:  //Filename: Cargo.toml [dependencies] rand = \"0.5.5\"  Adding rand as a dependency in Cargo.toml tells Cargo to download the rand package and any dependencies from crates.io and make rand available to our project.   Then, to bring rand definitions into the scope of our package, we added a use line starting with the name of the package, rand, and listed the items we wanted to bring into scope. Recall that in the “Generating a Random Number” section in Chapter 2, we brought the Rng trait into scope and called the rand::thread_rng function:  use rand::Rng; fn main() {     let secret_number = rand::thread_rng().gen_range(1, 101); }  Members of the Rust community have made many packages available at crates.io, and pulling any of them into your package involves these same steps: listing them in your package’s Cargo.toml file and using use to bring items into scope.   Note that the standard library (std) is also a crate that’s external to our package.  Because the standard library is shipped with the Rust language, we don’t need to change Cargo.toml to include std.  But we do need to refer to it with use to bring items from there into our package’s scope. For example, with HashMap we would use this line:  use std::collections::HashMap;  This is an absolute path starting with std, the name of the standard library crate.   Using Nested Paths to Clean Up Large use Lists  If we’re using multiple items defined in the same package or same module, listing each item on its own line can take up a lot of vertical space in our files. For example, these two use statements we had in the Guessing Game in Listing 2-4 bring items from std into scope:  use std::io; use std::cmp::Ordering; // ---snip---  Instead, we can use nested paths to bring the same items into scope in one line.  We do this by specifying the common part of the path, followed by two colons, and then curly brackets around a list of the parts of the paths that differ, as shown in Listing 7-18.  use std::{cmp::Ordering, io}; // ---snip---  Listing 7-18: Specifying a nested path to bring multiple items with the same prefix into scope   In bigger programs, bringing many items into scope from the same package or module using nested paths can reduce the number of separate use statements needed by a lot!   We can use a nested path at any level in a path, which is useful when combining two use statements that share a subpath.  For example, Listing 7-19 shows two use statements: one that brings std::io into scope and one that brings std::io::Write into scope.  use std::io; use std::io::Write;  Listing 7-19: Two use statements where one is a subpath of the other  The common part of these two paths is std::io, and that’s the complete first path.  To merge these two paths into one use statement, we can use self in the nested path, as shown in Listing 7-20.  use std::io::{self, Write};  Listing 7-20: Combining the paths in Listing 7-19 into one use statement  This line brings std::io and std::io::Write into scope.   The Glob Operator  If we want to bring all public items defined in a path into scope, we can specify that path followed by *, the glob operator:  use std::collections::*;  This use statement brings all public items defined in std::collections into the current scope. Be careful when using the glob operator! Glob can make it harder to tell what names are in scope and where a name used in your program was defined.   The glob operator is often used when testing to bring everything under test into the tests module; we’ll talk about that in the “How to Write Tests” section in Chapter 11. The glob operator is also sometimes used as part of the prelude pattern: see the standard library documentation for more information on that pattern.   Separating Modules into Different Files  So far, all the examples in this chapter defined multiple modules in one file. When modules get large, you might want to move their definitions to a separate file to make the code easier to navigate.   For example, let’s start from the code in Listing 7-17 and move the front_of_house module to its own file src/front_of_house.rs by changing the crate root file so it contains the code shown in Listing 7-21. In this case, the crate root file is src/lib.rs, but this procedure also works with binary crates whose crate root file is src/main.rs.   // Filename: src/lib.rs mod front_of_house;  pub use crate::front_of_house::hosting;  pub fn eat_at_restaurant() {     hosting::add_to_waitlist();     hosting::add_to_waitlist();     hosting::add_to_waitlist(); }  Listing 7-21: Declaring the front_of_house module whose body will be in src/front_of_house.rs   And src/front_of_house.rs gets the definitions from the body of the front_of_house module, as shown in Listing 7-22.  // Filename: src/front_of_house.rs pub mod hosting {     pub fn add_to_waitlist() {} }  Listing 7-22: Definitions inside the front_of_house module in src/front_of_house.rs  Using a semicolon after mod front_of_house rather than using a block tells Rust to load the contents of the module from another file with the same name as the module. To continue with our example and extract the hosting module to its own file as well, we change src/front_of_house.rs to contain only the declaration of the hosting module:  // Filename: src/front_of_house.rs pub mod hosting;  Then we create a src/front_of_house directory and a file src/front_of_house/hosting.rs to contain the definitions made in the hosting module:  // Filename: src/front_of_house/hosting.rs pub fn add_to_waitlist() {}  The module tree remains the same, and the function calls in eat_at_restaurant will work without any modification, even though the definitions live in different files. This technique lets you move modules to new files as they grow in size.   Note that the pub use crate::front_of_house::hosting statement in src/lib.rs also hasn’t changed, nor does use have any impact on what files are compiled as part of the crate. The mod keyword declares modules, and Rust looks in a file with the same name as the module for the code that goes into that module.   Summary  Rust lets you split a package into multiple crates and a crate into modules so you can refer to items defined in one module from another module. You can do this by specifying absolute or relative paths. These paths can be brought into scope with a use statement so you can use a shorter path for multiple uses of the item in that scope. Module code is private by default, but you can make definitions public by adding the pub keyword.  ","categories": ["RUST Language"],
        "tags": ["Mod","Module","Private","Public","Package","Crates","Paths"],
        "url": "https://jjungs-lee.github.io//rust/7.Managing-Growing-Projects-with-Packages,-Crates,-and-Modules",
        "teaser":null},{
        "title": "RUST : 8. Common Collections",
        "excerpt":"Rust’s standard library includes a number of very useful data structures called collections. Most other data types represent one specific value, but collections can contain multiple values. Unlike the built-in array and tuple types, the data these collections point to is stored on the heap, which means the amount of data does not need to be known at compile time and can grow or shrink as the program runs. Each kind of collection has different capabilities and costs, and choosing an appropriate one for your current situation is a skill you’ll develop over time. In this chapter, we’ll discuss three collections that are used very often in Rust programs:      A vector allows you to store a variable number of values next to each other.   A string is a collection of characters. We’ve mentioned the String type previously, but in this chapter we’ll talk about it in depth.   A hash map allows you to associate a value with a particular key. It’s a particular implementation of the more general data structure called a map.   To learn about the other kinds of collections provided by the standard library, see the documentation.   Storing Lists of Values with Vectors  The first collection type we’ll look at is Vec&lt;T&gt;, also known as a vector. Vectors allow you to store more than one value in a single data structure that puts all the values next to each other in memory. Vectors can only store values of the same type. They are useful when you have a list of items, such as the lines of text in a file or the prices of items in a shopping cart.   Creating a New Vector  To create a new, empty vector, we can call the Vec::new function, as shown in Listing 8-1.  let v: Vec&lt;i32&gt; = Vec::new();  Listing 8-1: Creating a new, empty vector to hold values of type i32  Note that we added a type annotation here. Because we aren’t inserting any values into this vector, Rust doesn’t know what kind of elements we intend to store.  Vectors are implemented using generics; we’ll cover how to use generics with your own types in Chapter 10. For now, know that the Vec&lt;T&gt; type provided by the standard library can hold any type, and when a specific vector holds a specific type, the type is specified within angle brackets(&lt;&gt;).  In Listing 8-1, we’ve told Rust that the Vec&lt;T&gt; in v will hold elements of the i32 type.   In more realistic code, Rust can often infer the type of value you want to store once you insert values, so you rarely need to do this type annotation.  It’s more common to create a Vec&lt;T&gt; that has initial values, and Rust provides the vec! macro for convenience. The macro will create a new vector that holds the values you give it. Listing 8-2 creates a new Vec that holds the values 1, 2, and 3.   let v = vec![1, 2, 3];  Listing 8-2: Creating a new vector containing values  Because we’ve given initial i32 values, Rust can infer that the type of v is Vec, and the type annotation isn’t necessary.   Updating a Vector  To create a vector and then add elements to it, we can use the push method, as shown in Listing 8-3.  let mut v = Vec::new();  v.push(5); v.push(6); v.push(7); v.push(8);  Listing 8-3: Using the push method to add values to a vector  As with any variable, if we want to be able to change its value, we need to make it mutable using the mut keyword, as discussed in Chapter 3.  The numbers we place inside are all of type i32, and Rust infers this from the data, so we don’t need the Vec&lt;i32&gt; annotation.   Dropping a Vector Drops Its Elements  Like any other struct, a vector is freed when it goes out of scope, as annotated in Listing 8-4.  {     let v = vec![1, 2, 3, 4];      // do stuff with v  } // &lt;- v goes out of scope and is freed here  Listing 8-4: Showing where the vector and its elements are dropped  When the vector gets dropped, all of its contents are also dropped, meaning those integers it holds will be cleaned up.  This may seem like a straightforward point but can get a bit more complicated when you start to introduce references to the elements of the vector.   Reading Elements of Vectors  There are two ways to reference a value stored in a vector.  In the examples, we’ve annotated the types of the values that are returned from these functions for extra clarity.   Listing 8-5 shows both methods of accessing a value in a vector, either with indexing syntax or the get method.  let v = vec![1, 2, 3, 4, 5];  let third: &amp;i32 = &amp;v[2]; println!(\"The third element is {}\", third);  match v.get(2) {     Some(third) =&gt; println!(\"The third element is {}\", third),     None =&gt; println!(\"There is no third element.\"), }  Listing 8-5: Using indexing syntax or the get method to access an item in a vector  Note two details here.  First, we use the index value of 2 to get the third element: vectors are indexed by number, starting at zero.  Second, the two ways to get the third element are by using &amp; and [], which gives us a reference, or by using the get method with the index passed as an argument, which gives us an Option&lt;&amp;T&gt;.   Rust has two ways to reference an element so you can choose how the program behaves when you try to use an index value that the vector doesn’t have an element for.  As an example, let’s see what a program will do if it has a vector that holds five elements and then tries to access an element at index 100, as shown in Listing 8-6.  let v = vec![1, 2, 3, 4, 5];  let does_not_exist = &amp;v[100]; let does_not_exist = v.get(100);  Listing 8-6: Attempting to access the element at index 100 in a vector containing five elements  When we run this code, the first [] method will cause the program to panic because it references a nonexistent element.  This method is best used when you want your program to crash if there’s an attempt to access an element past the end of the vector.   When the get method is passed an index that is outside the vector, it returns None without panicking. You would use this method if accessing an element beyond the range of the vector happens occasionally under normal circumstances.  Your code will then have logic to handle having either Some(&amp;element) or None, as discussed in Chapter 6.   When the program has a valid reference, the borrow checker enforces the ownership and borrowing rules (covered in Chapter 4) to ensure this reference and any other references to the contents of the vector remain valid.  Recall the rule that states you can’t have mutable and immutable references in the same scope.  That rule applies in Listing 8-7, where we hold an immutable reference to the first element in a vector and try to add an element to the end, which won’t work.  let mut v = vec![1, 2, 3, 4, 5];  let first = &amp;v[0];  v.push(6);  Listing 8-7: Attempting to add an element to a vector while holding a reference to an item  Compiling this code will result in this error:  error[E0502]: cannot borrow `v` as mutable because it is also borrowed as immutable  --&gt; src/main.rs:6:5   | 4 |     let first = &amp;v[0];   |                  - immutable borrow occurs here 5 | 6 |     v.push(6);   |     ^^^^^^^^^ mutable borrow occurs here 7 | 8 |     println!(\"The first element is: {}\", first);   |                                          ----- immutable borrow later used here  The code in Listing 8-7 might look like it should work: why should a reference to the first element care about what changes at the end of the vector?  This error is due to the way vectors work: adding a new element onto the end of the vector might require allocating new memory and copying the old elements to the new space, if there isn’t enough room to put all the elements next to each other where the vector currently is.  In that case, the reference to the first element would be pointing to deallocated memory.  The borrowing rules prevent programs from ending up in that situation.   Note: For more on the implementation details of the Vec type, see “The Rustonomicon” at https://doc.rust-lang.org/stable/nomicon/vec.html.   Iterating over the Values in a Vector  If we want to access each element in a vector in turn, we can iterate through all of the elements rather than use indices to access one at a time.  Listing 8-8 shows how to use a for loop to get immutable references to each element in a vector of i32 values and print them.  let v = vec![100, 32, 57]; for i in &amp;v {     println!(\"{}\", i); }  Listing 8-8: Printing each element in a vector by iterating over the elements using a for loop  We can also iterate over mutable references to each element in a mutable vector in order to make changes to all the elements.  The for loop in Listing 8-9 will add 50 to each element.  let mut v = vec![100, 32, 57]; for i in &amp;mut v {     *i += 50; }  Listing 8-9: Iterating over mutable references to elements in a vector   To change the value that the mutable reference refers to, we have to use the dereference operator (*) to get to the value in i before we can use the += operator.  We’ll talk more about the dereference operator in the “Following the Pointer to the Value with the Dereference Operator” section of Chapter 15.   Using an Enum to Store Multiple Types  At the beginning of this chapter, we said that vectors can only store values that are the same type.  This can be inconvenient; there are definitely use cases for needing to store a list of items of different types.  Fortunately, the variants of an enum are defined under the same enum type, so when we need to store elements of a different type in a vector, we can define and use an enum!  enum SpreadsheetCell {     Int(i32),     Float(f64),     Text(String), }  let row = vec![     SpreadsheetCell::Int(3),     SpreadsheetCell::Text(String::from(\"blue\")),     SpreadsheetCell::Float(10.12), ];  Listing 8-10: Defining an enum to store values of different types in one vector  Rust needs to know what types will be in the vector at compile time so it knows exactly how much memory on the heap will be needed to store each element.  A secondary advantage is that we can be explicit about what types are allowed in this vector.  If Rust allowed a vector to hold any type, there would be a chance that one or more of the types would cause errors with the operations performed on the elements of the vector. Using an enum plus a match expression means that Rust will ensure at compile time that every possible case is handled, as discussed in Chapter 6.   When you’re writing a program, if you don’t know the exhaustive set of types the program will get at runtime to store in a vector, the enum technique won’t work. Instead, you can use a trait object, which we’ll cover in Chapter 17.   Now that we’ve discussed some of the most common ways to use vectors, be sure to review the API documentation for all the many useful methods defined on Vec by the standard library.   Storing UTF-8 Encoded Text with Strings  We talked about strings in Chapter 4, but we’ll look at them in more depth now. New Rustaceans commonly get stuck on strings for a combination of three reasons:  Rust’s propensity for exposing possible errors, strings being a more complicated data structure than many programmers give them credit for, and UTF-8. These factors combine in a way that can seem difficult when you’re coming from other programming languages.   It’s useful to discuss strings in the context of collections because strings are implemented as a collection of bytes, plus some methods to provide useful functionality when those bytes are interpreted as text.  In this section, we’ll talk about the operations on String that every collection type has, such as creating, updating, and reading.  We’ll also discuss the ways in which String is different from the other collections, namely how indexing into a String is complicated by the differences between how people and computers interpret String data.   What Is a String?  We’ll first define what we mean by the term string. Rust has only one string type in the core language, which is the string slice str that is usually seen in its borrowed form &amp;str. In Chapter 4, we talked about string slices, which are references to some UTF-8 encoded string data stored elsewhere. String literals, for example, are stored in the program’s binary and are therefore string slices.   The String type, which is provided by Rust’s standard library rather than coded into the core language, is a growable, mutable, owned, UTF-8 encoded string type.  When Rustaceans refer to “strings” in Rust, they usually mean the String and the string slice &amp;str types, not just one of those types.  Although this section is largely about String, both types are used heavily in Rust’s standard library, and both String and string slices are UTF-8 encoded.   Rust’s standard library also includes a number of other string types, such as OsString, OsStr, CString, and CStr.  Library crates can provide even more options for storing string data. See how those names all end in String or Str?  They refer to owned and borrowed variants, just like the String and str types you’ve seen previously.  These string types can store text in different encodings or be represented in memory in a different way, for example.  We won’t discuss these other string types in this chapter; see their API documentation for more about how to use them and when each is appropriate.   Creating a New String  Many of the same operations available with Vec are available with String as well, starting with the new function to create a string, shown in Listing 8-11.  let mut s = String::new();  Listing 8-11: Creating a new, empty String  This line creates a new empty string called s, which we can then load data into.  Often, we’ll have some initial data that we want to start the string with.  For that, we use the to_string method, which is available on any type that implements the Display trait, as string literals do.  Listing 8-12 shows two examples.  let data = \"initial contents\";  let s = data.to_string();  // the method also works on a literal directly: let s = \"initial contents\".to_string();  Listing 8-12: Using the to_string method to create a String from a string litera  This code creates a string containing initial contents.   We can also use the function String::from to create a String from a string literal.  The code in Listing 8-13 is equivalent to the code from Listing 8-12 that uses to_string.  let s = String::from(\"initial contents\");  Listing 8-13: Using the String::from function to create a String from a string literal  Because strings are used for so many things, we can use many different generic APIs for strings, providing us with a lot of options.  Some of them can seem redundant, but they all have their place!  In this case, String::from and to_string do the same thing, so which you choose is a matter of style.   Remember that strings are UTF-8 encoded, so we can include any properly encoded data in them, as shown in Listing 8-14.  // All of these are valid String values. let hello = String::from(\"السلام عليكم\"); let hello = String::from(\"Dobrý den\"); let hello = String::from(\"Hello\"); let hello = String::from(\"שָׁלוֹם\"); let hello = String::from(\"नमस्ते\"); let hello = String::from(\"こんにちは\"); let hello = String::from(\"안녕하세요\"); let hello = String::from(\"你好\"); let hello = String::from(\"Olá\"); let hello = String::from(\"Здравствуйте\"); let hello = String::from(\"Hola\");  Listing 8-14: Storing greetings in different languages in strings   Updating a String  A String can grow in size and its contents can change, just like the contents of a Vec&lt;T&gt;, if you push more data into it.  In addition, you can conveniently use the + operator or the format! macro to concatenate String values.   Appending to a String with push_str and push  We can grow a String by using the push_str method to append a string slice, as shown in Listing 8-15.  let mut s = String::from(\"foo\"); s.push_str(\"bar\");  Listing 8-15: Appending a string slice to a String using the push_str method  After these two lines, s will contain foobar. The push_str method takes a string slice because we don’t necessarily want to take ownership of the parameter.  For example, the code in Listing 8-16 shows that it would be unfortunate if we weren’t able to use s2 after appending its contents to s1.  let mut s1 = String::from(\"foo\"); let s2 = \"bar\"; s1.push_str(s2); println!(\"s2 is {}\", s2);  Listing 8-16: Using a string slice after appending its contents to a String  If the push_str method took ownership of s2, we wouldn’t be able to print its value on the last line.  However, this code works as we’d expect!   The push method takes a single character as a parameter and adds it to the String. Listing 8-17 shows code that adds the letter l to a String using the push method.  // As a result of this code, s will contain `lol`. let mut s = String::from(\"lo\"); s.push('l');  Listing 8-17: Adding one character to a String value using push   Concatenation with the + Operator or the format! Macro  Often, you’ll want to combine two existing strings. One way is to use the + operator, as shown in Listing 8-18.  let s1 = String::from(\"Hello, \"); let s2 = String::from(\"world!\"); let s3 = s1 + &amp;s2; // note s1 has been moved here and can no longer be used  Listing 8-18: Using the + operator to combine two String values into a new String value   The string s3 will contain Hello, world! as a result of this code.  The reason s1 is no longer valid after the addition and the reason we used a reference to s2 has to do with the signature of the method that gets called when we use the + operator.  The + operator uses the add method, whose signature looks something like this:  fn add(self, s: &amp;str) -&gt; String {  This isn’t the exact signature that’s in the standard library: in the standard library, add is defined using generics.  Here, we’re looking at the signature of add with concrete types substituted for the generic ones, which is what happens when we call this method with String values.  We’ll discuss generics in Chapter 10. This signature gives us the clues we need to understand the tricky bits of the + operator.   First, s2 has an &amp;, meaning that we’re adding a reference of the second string to the first string because of the s parameter in the add function: we can only add a &amp;str to a String; we can’t add two String values together.  But wait—the type of &amp;s2 is &amp;String, not &amp;str, as specified in the second parameter to add. So why does Listing 8-18 compile?   The reason we’re able to use &amp;s2 in the call to add is that the compiler can coerce the &amp;String argument into a &amp;str.  When we call the add method, Rust uses a deref coercion, which here turns &amp;s2 into &amp;s2[..].  We’ll discuss deref coercion in more depth in Chapter 15.  Because add does not take ownership of the s parameter, s2 will still be a valid String after this operation.   Second, we can see in the signature that add takes ownership of self, because self does not have an &amp;.  This means s1 in Listing 8-18 will be moved into the add call and no longer be valid after that.  So although let s3 = s1 + &amp;s2; looks like it will copy both strings and create a new one, this statement actually takes ownership of s1, appends a copy of the contents of s2, and then returns ownership of the result.  In other words, it looks like it’s making a lot of copies but isn’t; the implementation is more efficient than copying.   If we need to concatenate multiple strings, the behavior of the + operator gets unwieldy:  let s1 = String::from(\"tic\"); let s2 = String::from(\"tac\"); let s3 = String::from(\"toe\");  let s = s1 + \"-\" + &amp;s2 + \"-\" + &amp;s3;  At this point, s will be tic-tac-toe. With all of the + and characters, it’s difficult to see what’s going on.  For more complicated string combining, we can use the format! macro:  let s1 = String::from(\"tic\"); let s2 = String::from(\"tac\"); let s3 = String::from(\"toe\");  let s = format!(\"{}-{}-{}\", s1, s2, s3);  This code also sets s to tic-tac-toe. The format! macro works in the same way as println!, but instead of printing the output to the screen, it returns a String with the contents.  The version of the code using format! is much easier to read and doesn’t take ownership of any of its parameters.   Indexing into Strings   This section differ other programming languages   In many other programming languages, accessing individual characters in a string by referencing them by index is a valid and common operation.  However, if you try to access parts of a String using indexing syntax in Rust, you’ll get an error. Consider the invalid code in Listing 8-19.  let s1 = String::from(\"hello\"); let h = s1[0];  Listing 8-19: Attempting to use indexing syntax with a String   This code will result in the following error:  error[E0277]: the trait bound `std::string::String: std::ops::Index&lt;{integer}&gt;` is not satisfied  --&gt;   | 3 |     let h = s1[0];   |             ^^^^^ the type `std::string::String` cannot be indexed by `{integer}`   |   = help: the trait `std::ops::Index&lt;{integer}&gt;` is not implemented for `std::string::String`  The error and the note tell the story: Rust strings don’t support indexing. But why not?  To answer that question, we need to discuss how Rust stores strings in memory.   Internal Representation  A String is a wrapper over a Vec&lt;u8&gt;. Let’s look at some of our properly encoded UTF-8 example strings from Listing 8-14. First, this one:  let len = String::from(\"Hola\").len();  In this case, len will be 4, which means the vector storing the string “Hola” is 4 bytes long.  Each of these letters takes 1 byte when encoded in UTF-8. But what about the following line? (Note that this string begins with the capital Cyrillic letter Ze, not the Arabic number 3.)  let len = String::from(\"Здравствуйте\").len();  Asked how long the string is, you might say 12.  However, Rust’s answer is 24: that’s the number of bytes it takes to encode “Здравствуйте” in UTF-8, because each Unicode scalar value in that string takes 2 bytes of storage. Therefore, an index into the string’s bytes will not always correlate to a valid Unicode scalar value. To demonstrate, consider this invalid Rust code:   let hello = \"Здравствуйте\"; let answer = &amp;hello[0];  What should the value of answer be? Should it be З, the first letter? When encoded in UTF-8, the first byte of З is 208 and the second is 151, so answer should in fact be 208, but 208 is not a valid character on its own.  Returning 208 is likely not what a user would want if they asked for the first letter of this string; however, that’s the only data that Rust has at byte index 0.  Users generally don’t want the byte value returned, even if the string contains only Latin letters: if &amp;”hello”[0] were valid code that returned the byte value, it would return 104, not h. To avoid returning an unexpected value and causing bugs that might not be discovered immediately, Rust doesn’t compile this code at all and prevents misunderstandings early in the development process.   Bytes and Scalar Values and Grapheme Clusters! Oh My!  Another point about UTF-8 is that there are actually three relevant ways to look at strings from Rust’s perspective: as bytes, scalar values, and grapheme clusters (the closest thing to what we would call letters).   If we look at the Hindi word “नमस्ते” written in the Devanagari script, it is stored as a vector of u8 values that looks like this:  [224, 164, 168, 224, 164, 174, 224, 164, 184, 224, 165, 141, 224, 164, 164, 224, 165, 135]  That’s 18 bytes and is how computers ultimately store this data.  If we look at them as Unicode scalar values, which are what Rust’s char type is, those bytes look like this:  ['न', 'म', 'स', '्', 'त', 'े']  There are six char values here, but the fourth and sixth are not letters: they’re diacritics that don’t make sense on their own.  Finally, if we look at them as grapheme clusters, we’d get what a person would call the four letters that make up the Hindi word:  [\"न\", \"म\", \"स्\", \"ते\"]  Rust provides different ways of interpreting the raw string data that computers store so that each program can choose the interpretation it needs, no matter what human language the data is in.   A final reason Rust doesn’t allow us to index into a String to get a character is that indexing operations are expected to always take constant time (O(1)). But it isn’t possible to guarantee that performance with a String, because Rust would have to walk through the contents from the beginning to the index to determine how many valid characters there were.   Slicing Strings  Indexing into a string is often a bad idea because it’s not clear what the return type of the string-indexing operation should be: a byte value, a character, a grapheme cluster, or a string slice.  Therefore, Rust asks you to be more specific if you really need to use indices to create string slices.  To be more specific in your indexing and indicate that you want a string slice, rather than indexing using [] with a single number, you can use [] with a range to create a string slice containing particular bytes:  let hello = \"Здравствуйте\";  let s = &amp;hello[0..4];  Here, s will be a &amp;str that contains the first 4 bytes of the string.  Earlier, we mentioned that each of these characters was 2 bytes, which means s will be “Зд”.   What would happen if we used &amp;hello[0..1]? The answer: Rust would panic at runtime in the same way as if an invalid index were accessed in a vector:  thread 'main' panicked at 'byte index 1 is not a char boundary;  it is inside 'З' (bytes 0..2) of `Здравствуйте`', src/libcore/str/mod.rs:2188:4 You should use ranges to create string slices with caution,  because doing so can crash your program.  You should use ranges to create string slices with caution, because doing so can crash your program.   Methods for Iterating Over Strings  Fortunately, you can access elements in a string in other ways.   If you need to perform operations on individual Unicode scalar values, the best way to do so is to use the chars method.  Calling chars on “नमस्ते” separates out and returns six values of type char, and you can iterate over the result to access each element:  for c in \"नमस्ते\".chars() {     println!(\"{}\", c); } // This code will print the following: न म स ् त े  The bytes method returns each raw byte, which might be appropriate for your domain:  for b in \"नमस्ते\".bytes() {     println!(\"{}\", b); }  This code will print the 18 bytes that make up this String:  224 164 // --snip-- 165 135  But be sure to remember that valid Unicode scalar values may be made up of more than 1 byte.   Getting grapheme clusters from strings is complex, so this functionality is not provided by the standard library. Crates are available on crates.io if this is the functionality you need.   Strings Are Not So Simple  To summarize, strings are complicated. Different programming languages make different choices about how to present this complexity to the programmer. Rust has chosen to make the correct handling of String data the default behavior for all Rust programs, which means programmers have to put more thought into handling UTF-8 data upfront. This trade-off exposes more of the complexity of strings than is apparent in other programming languages, but it prevents you from having to handle errors involving non-ASCII characters later in your development life cycle.   Storing Keys with Associated Values in Hash Maps  The last of our common collections is the hash map. The type HashMap&lt;K, V&gt; stores a mapping of keys of type K to values of type V.  It does this via a hashing function, which determines how it places these keys and values into memory. Many programming languages support this kind of data structure, but they often use a different name, such as hash, map, object, hash table, dictionary, or associative array, just to name a few.   Hash maps are useful when you want to look up data not by using an index, as you can with vectors, but by using a key that can be of any type. For example, in a game, you could keep track of each team’s score in a hash map in which each key is a team’s name and the values are each team’s score. Given a team name, you can retrieve its score.   Creating a New Hash Map  You can create an empty hash map with new and add elements with insert. In Listing 8-20, we’re keeping track of the scores of two teams whose names are Blue and Yellow. The Blue team starts with 10 points, and the Yellow team starts with 50.  use std::collections::HashMap;  let mut scores = HashMap::new();  scores.insert(String::from(\"Blue\"), 10); scores.insert(String::from(\"Yellow\"), 50);  Listing 8-20: Creating a new hash map and inserting some keys and values  Note that we need to first use the HashMap from the collections portion of the standard library.  Of our three common collections, this one is the least often used, so it’s not included in the features brought into scope automatically in the prelude.  Hash maps also have less support from the standard library; there’s no built-in macro to construct them, for example.   Just like vectors, hash maps store their data on the heap. This HashMap has keys of type String and values of type i32. Like vectors, hash maps are homogeneous: all of the keys must have the same type, and all of the values must have the same type.   Another way of constructing a hash map is by using the collect method on a vector of tuples, where each tuple consists of a key and its value.  The collect method gathers data into a number of collection types, including HashMap.  For example, if we had the team names and initial scores in two separate vectors, we could use the zip method to create a vector of tuples where “Blue” is paired with 10, and so forth. Then we could use the collect method to turn that vector of tuples into a hash map, as shown in Listing 8-21.  use std::collections::HashMap;  let teams  = vec![String::from(\"Blue\"), String::from(\"Yellow\")]; let initial_scores = vec![10, 50];  let scores: HashMap&lt;_, _&gt; = teams.iter().zip(initial_scores.iter()).collect();  Listing 8-21: Creating a hash map from a list of teams and a list of scores   The type annotation HashMap&lt;_, _&gt; is needed here because it’s possible to collect into many different data structures and Rust doesn’t know which you want unless you specify. For the parameters for the key and value types, however, we use underscores, and Rust can infer the types that the hash map contains based on the types of the data in the vectors.   Hash Maps and Ownership  For types that implement the Copy trait, like i32, the values are copied into the hash map.  For owned values like String, the values will be moved and the hash map will be the owner of those values, as demonstrated in Listing 8-22.   use std::collections::HashMap;  let field_name = String::from(\"Favorite color\"); let field_value = String::from(\"Blue\");  let mut map = HashMap::new(); map.insert(field_name, field_value); // field_name and field_value are invalid at this point, try using them and // see what compiler error you get!  Listing 8-22: Showing that keys and values are owned by the hash map once they’re inserted   We aren’t able to use the variables field_name and field_value after they’ve been moved into the hash map with the call to insert.   If we insert references to values into the hash map, the values won’t be moved into the hash map. The values that the references point to must be valid for at least as long as the hash map is valid. We’ll talk more about these issues in the “Validating References with Lifetimes” section in Chapter 10.   Accessing Values in a Hash Map  We can get a value out of the hash map by providing its key to the get method, as shown in Listing 8-23.   use std::collections::HashMap;  let mut scores = HashMap::new();  scores.insert(String::from(\"Blue\"), 10); scores.insert(String::from(\"Yellow\"), 50);  let team_name = String::from(\"Blue\"); let score = scores.get(&amp;team_name);  Listing 8-23: Accessing the score for the Blue team stored in the hash map   Here, score will have the value that’s associated with the Blue team, and the result will be Some(&amp;10).  The result is wrapped in Some because get returns an Option&lt;&amp;V&gt;; if there’s no value for that key in the hash map, get will return None.  The program will need to handle the Option in one of the ways that we covered in Chapter 6.   We can iterate over each key/value pair in a hash map in a similar manner as we do with vectors, using a for loop:  use std::collections::HashMap;  let mut scores = HashMap::new();  scores.insert(String::from(\"Blue\"), 10); scores.insert(String::from(\"Yellow\"), 50);  for (key, value) in &amp;scores {     println!(\"{}: {}\", key, value); } // This code will print each pair in an arbitrary order: Yellow: 50 Blue: 10   Updating a Hash Map  Although the number of keys and values is growable, each key can only have one value associated with it at a time.  When you want to change the data in a hash map, you have to decide how to handle the case when a key already has a value assigned.  You could replace the old value with the new value, completely disregarding the old value.  You could keep the old value and ignore the new value, only adding the new value if the key doesn’t already have a value.  Or you could combine the old value and the new value. Let’s look at how to do each of these!   Overwriting a Value  If we insert a key and a value into a hash map and then insert that same key with a different value, the value associated with that key will be replaced.  Even though the code in Listing 8-24 calls insert twice, the hash map will only contain one key/value pair because we’re inserting the value for the Blue team’s key both times.   use std::collections::HashMap;  let mut scores = HashMap::new();  scores.insert(String::from(\"Blue\"), 10); scores.insert(String::from(\"Blue\"), 25);  println!(\"{:?}\", scores);  Listing 8-24: Replacing a value stored with a particular key  This code will print {\"Blue\": 25}. The original value of 10 has been overwritten.   Only Inserting a Value If the Key Has No Value  It’s common to check whether a particular key has a value and, if it doesn’t, insert a value for it.  Hash maps have a special API for this called entry that takes the key you want to check as a parameter.  The return value of the entry method is an enum called Entry that represents a value that might or might not exist.  Let’s say we want to check whether the key for the Yellow team has a value associated with it.  If it doesn’t, we want to insert the value 50, and the same for the Blue team. Using the entry API, the code looks like Listing 8-25.   use std::collections::HashMap;  let mut scores = HashMap::new(); scores.insert(String::from(\"Blue\"), 10);  scores.entry(String::from(\"Yellow\")).or_insert(50); scores.entry(String::from(\"Blue\")).or_insert(50);  println!(\"{:?}\", scores);  Listing 8-25: Using the entry method to only insert if the key does not already have a value   The or_insert method on Entry is defined to return a mutable reference to the value for the corresponding Entry key if that key exists, and if not, inserts the parameter as the new value for this key and returns a mutable reference to the new value.  This technique is much cleaner than writing the logic ourselves and, in addition, plays more nicely with the borrow checker.   Running the code in Listing 8-25 will print {\"Yellow\": 50, \"Blue\": 10}. The first call to entry will insert the key for the Yellow team with the value 50 because the Yellow team doesn’t have a value already. The second call to entry will not change the hash map because the Blue team already has the value 10.   Updating a Value Based on the Old Value  Another common use case for hash maps is to look up a key’s value and then update it based on the old value.  For instance, Listing 8-26 shows code that counts how many times each word appears in some text.  We use a hash map with the words as keys and increment the value to keep track of how many times we’ve seen that word.  If it’s the first time we’ve seen a word, we’ll first insert the value 0.  use std::collections::HashMap;  let text = \"hello world wonderful world\";  let mut map = HashMap::new();  for word in text.split_whitespace() {     let count = map.entry(word).or_insert(0);     *count += 1; }  println!(\"{:?}\", map);  Listing 8-26: Counting occurrences of words using a hash map that stores words and counts  This code will print {\"world\": 2, \"hello\": 1, \"wonderful\": 1}. The or_insert method actually returns a mutable reference (&amp;mut V) to the value for this key.  Here we store that mutable reference in the count variable, so in order to assign to that value, we must first dereference count using the asterisk (*).  The mutable reference goes out of scope at the end of the for loop, so all of these changes are safe and allowed by the borrowing rules.   Hashing Functions  By default, HashMap uses a “cryptographically strong”1 hashing function that can provide resistance to Denial of Service (DoS) attacks.  This is not the fastest hashing algorithm available, but the trade-off for better security that comes with the drop in performance is worth it.  If you profile your code and find that the default hash function is too slow for your purposes, you can switch to another function by specifying a different hasher.  A hasher is a type that implements the BuildHasher trait. We’ll talk about traits and how to implement them in Chapter 10.  You don’t necessarily have to implement your own hasher from scratch; crates.io has libraries shared by other Rust users that provide hashers implementing many common hashing algorithms.   Summary  Vectors, strings, and hash maps will provide a large amount of functionality necessary in programs when you need to store, access, and modify data. Here are some exercises you should now be equipped to solve:      Given a list of integers, use a vector and return the mean (the average value), median (when sorted, the value in the middle position), and mode (the value that occurs most often; a hash map will be helpful here) of the list.   Convert strings to pig latin. The first consonant of each word is moved to the end of the word and “ay” is added, so “first” becomes “irst-fay.” Words that start with a vowel have “hay” added to the end instead (“apple” becomes “apple-hay”). Keep in mind the details about UTF-8 encoding!   Using a hash map and vectors, create a text interface to allow a user to add employee names to a department in a company. For example, “Add Sally to Engineering” or “Add Amir to Sales.” Then let the user retrieve a list of all people in a department or all people in the company by department, sorted alphabetically.   The standard library API documentation describes methods that vectors, strings, and hash maps have that will be helpful for these exercises!                   https://www.131002.net/siphash/siphash.pdf &#8617;           ","categories": ["RUST Language"],
        "tags": ["Library","Collection","Vector","String","Hash map"],
        "url": "https://jjungs-lee.github.io//rust/8.Common-Collections",
        "teaser":null},{
        "title": "RUST : 9. Error Handling",
        "excerpt":"Rust’s commitment to reliability extends to error handling. Errors are a fact of life in software, so Rust has a number of features for handling situations in which something goes wrong. In many cases, Rust requires you to acknowledge the possibility of an error and take some action before your code will compile. This requirement makes your program more robust by ensuring that you’ll discover errors and handle them appropriately before you’ve deployed your code to production!   Rust groups errors into two major categories: recoverable and unrecoverable errors. For a recoverable error, such as a file not found error, it’s reasonable to report the problem to the user and retry the operation. Unrecoverable errors are always symptoms of bugs, like trying to access a location beyond the end of an array.   Most languages don’t distinguish between these two kinds of errors and handle both in the same way, using mechanisms such as exceptions. Rust doesn’t have exceptions.  Instead, it has the type Result&lt;T, E&gt; for recoverable errors and the panic! macro that stops execution when the program encounters an unrecoverable error.  This chapter covers calling panic! first and then talks about returning Result&lt;T, E&gt; values. Additionally, we’ll explore considerations when deciding whether to try to recover from an error or to stop execution.   Unrecoverable Errors with panic!  Sometimes, bad things happen in your code, and there’s nothing you can do about it. In these cases, Rust has the panic! macro.  When the panic! macro executes, your program will print a failure message, unwind and clean up the stack, and then quit.  This most commonly occurs when a bug of some kind has been detected and it’s not clear to the programmer how to handle the error.      Unwinding the Stack or Aborting in Response to a Panic    By default, when a panic occurs, the program starts unwinding, which means Rust walks back up the stack and cleans up the data from each function it encounters.  But this walking back and cleanup is a lot of work. The alternative is to immediately abort, which ends the program without cleaning up.  Memory that the program was using will then need to be cleaned up by the operating system.  If in your project you need to make the resulting binary as small as possible, you can switch from unwinding to aborting upon a panic by adding panic = 'abort' to the appropriate [profile] sections in your Cargo.toml file.  For example, if you want to abort on panic in release mode, add this:    [profile.release]   panic = 'abort'      Let’s try calling panic! in a simple program:  fn main() {     panic!(\"crash and burn\"); }  When you run the program, you’ll see something like this:  $ cargo run    Compiling panic v0.1.0 (file:///projects/panic)     Finished dev [unoptimized + debuginfo] target(s) in 0.25s      Running `target/debug/panic` thread 'main' panicked at 'crash and burn', src/main.rs:2:5 note: Run with `RUST_BACKTRACE=1` for a backtrace.  The call to panic! causes the error message contained in the last two lines. The first line shows our panic message and the place in our source code where the panic occurred: src/main.rs:2:5 indicates that it’s the second line, fifth character of our src/main.rs file.   In this case, the line indicated is part of our code, and if we go to that line, we see the panic! macro call. In other cases, the panic! call might be in code that our code calls, and the filename and line number reported by the error message will be someone else’s code where the panic! macro is called, not the line of our code that eventually led to the panic! call. We can use the backtrace of the functions the panic! call came from to figure out the part of our code that is causing the problem. We’ll discuss what a backtrace is in more detail next.   Using a panic! Backtrace   Let’s look at another example to see what it’s like when a panic! call comes from a library because of a bug in our code instead of from our code calling the macro directly. Listing 9-1 has some code that attempts to access an element by index in a vector.   fn main() {     let v = vec![1, 2, 3];      v[99]; }  Listing 9-1: Attempting to access an element beyond the end of a vector, which will cause a call to panic!   Here, we’re attempting to access the 100th element of our vector (which is at index 99 because indexing starts at zero), but it has only 3 elements.  In this situation, Rust will panic. Using [] is supposed to return an element, but if you pass an invalid index, there’s no element that Rust could return here that would be correct.   Other languages, like C, will attempt to give you exactly what you asked for in this situation, even though it isn’t what you want: you’ll get whatever is at the location in memory that would correspond to that element in the vector, even though the memory doesn’t belong to the vector.  This is called a buffer overread and can lead to security vulnerabilities if an attacker is able to manipulate the index in such a way as to read data they shouldn’t be allowed to that is stored after the array.   To protect your program from this sort of vulnerability, if you try to read an element at an index that doesn’t exist, Rust will stop execution and refuse to continue.  Let’s try it and see:  $ cargo run    Compiling panic v0.1.0 (file:///projects/panic)     Finished dev [unoptimized + debuginfo] target(s) in 0.27s      Running `target/debug/panic` thread 'main' panicked at 'index out of bounds: the len is 3 but the index is 99', libcore/slice/mod.rs:2448:10 note: Run with `RUST_BACKTRACE=1` for a backtrace.  This error points at a file we didn’t write, libcore/slice/mod.rs.  That’s the implementation of slice in the Rust source code.  The code that gets run when we use [] on our vector v is in libcore/slice/mod.rs, and that is where the panic! is actually happening.   The next note line tells us that we can set the RUST_BACKTRACE environment variable to get a backtrace of exactly what happened to cause the error.  A backtrace is a list of all the functions that have been called to get to this point.  Backtraces in Rust work as they do in other languages: the key to reading the backtrace is to start from the top and read until you see files you wrote.  That’s the spot where the problem originated.  The lines above the lines mentioning your files are code that your code called; the lines below are code that called your code.  These lines might include core Rust code, standard library code, or crates that you’re using.  Let’s try getting a backtrace by setting the RUST_BACKTRACE environment variable to any value except 0.  Listing 9-2 shows output similar to what you’ll see.  $ RUST_BACKTRACE=1 cargo run     Finished dev [unoptimized + debuginfo] target(s) in 0.00s      Running `target/debug/panic` thread 'main' panicked at 'index out of bounds: the len is 3 but the index is 99', libcore/slice/mod.rs:2448:10 stack backtrace:    0: std::sys::unix::backtrace::tracing::imp::unwind_backtrace              at libstd/sys/unix/backtrace/tracing/gcc_s.rs:49    1: std::sys_common::backtrace::print              at libstd/sys_common/backtrace.rs:71              at libstd/sys_common/backtrace.rs:59    2: std::panicking::default_hook::              at libstd/panicking.rs:211    3: std::panicking::default_hook              at libstd/panicking.rs:227    4: &lt;std::panicking::begin_panic::PanicPayload&lt;A&gt; as core::panic::BoxMeUp&gt;::get              at libstd/panicking.rs:476    5: std::panicking::continue_panic_fmt              at libstd/panicking.rs:390    6: std::panicking::try::do_call              at libstd/panicking.rs:325    7: core::ptr::drop_in_place              at libcore/panicking.rs:77    8: core::ptr::drop_in_place              at libcore/panicking.rs:59    9: &lt;usize as core::slice::SliceIndex&lt;[T]&gt;&gt;::index              at libcore/slice/mod.rs:2448   10: core::slice::&lt;impl core::ops::index::Index&lt;I&gt; for [T]&gt;::index              at libcore/slice/mod.rs:2316   11: &lt;alloc::vec::Vec&lt;T&gt; as core::ops::index::Index&lt;I&gt;&gt;::index              at liballoc/vec.rs:1653   12: panic::main              at src/main.rs:4   13: std::rt::lang_start::              at libstd/rt.rs:74   14: std::panicking::try::do_call              at libstd/rt.rs:59              at libstd/panicking.rs:310   15: macho_symbol_search              at libpanic_unwind/lib.rs:102   16: std::alloc::default_alloc_error_hook              at libstd/panicking.rs:289              at libstd/panic.rs:392              at libstd/rt.rs:58   17: std::rt::lang_start              at libstd/rt.rs:74   18: panic::main  Listing 9-2: The backtrace generated by a call to panic! displayed when the environment variable RUST_BACKTRACE is set   That’s a lot of output! The exact output you see might be different depending on your operating system and Rust version.  In order to get backtraces with this information, debug symbols must be enabled.  Debug symbols are enabled by default when using cargo build or cargo run without the --release flag, as we have here.   In the output in Listing 9-2, line 12 of the backtrace points to the line in our project that’s causing the problem: line 4 of src/main.rs.  If we don’t want our program to panic, the location pointed to by the first line mentioning a file we wrote is where we should start investigating.  In Listing 9-1, where we deliberately wrote code that would panic in order to demonstrate how to use backtraces, the way to fix the panic is to not request an element at index 99 from a vector that only contains 3 items.  When your code panics in the future, you’ll need to figure out what action the code is taking with what values to cause the panic and what the code should do instead.   We’ll come back to panic! and when we should and should not use panic! to handle error conditions in the “To panic! or Not to panic!” section later in this chapter. Next, we’ll look at how to recover from an error using Result.   Recoverable Errors with Result  Most errors aren’t serious enough to require the program to stop entirely.  Sometimes, when a function fails, it’s for a reason that you can easily interpret and respond to.  For example, if you try to open a file and that operation fails because the file doesn’t exist, you might want to create the file instead of terminating the process.   Recall from “Handling Potential Failure with the Result Type” in Chapter 2 that the Result enum is defined as having two variants, Ok and Err, as follows  enum Result&lt;T, E&gt; {     Ok(T),     Err(E), }  The T and E are generic type parameters: we’ll discuss generics in more detail in Chapter 10.  What you need to know right now is that T represents the type of the value that will be returned in a success case within the Ok variant, and E represents the type of the error that will be returned in a failure case within the Err variant.  Because Result has these generic type parameters, we can use the Result type and the functions that the standard library has defined on it in many different situations where the successful value and error value we want to return may differ.   Let’s call a function that returns a Result value because the function could fail. In Listing 9-3 we try to open a file.  use std::fs::File;  fn main() {     let f = File::open(\"hello.txt\"); }  Listing 9-3: Opening a file  How do we know File::open returns a Result? We could look at the standard library API documentation, or we could ask the compiler!  If we give f a type annotation that we know is not the return type of the function and then try to compile the code, the compiler will tell us that the types don’t match. The error message will then tell us what the type of f is.  Let’s try it! We know that the return type of File::open isn’t of type u32, so let’s change the let f statement to this:   let f: u32 = File::open(\"hello.txt\");  Attempting to compile now gives us the following output:  error[E0308]: mismatched types  --&gt; src/main.rs:4:18   | 4 |     let f: u32 = File::open(\"hello.txt\");   |                  ^^^^^^^^^^^^^^^^^^^^^^^ expected u32, found enum `std::result::Result`   |   = note: expected type `u32`              found type `std::result::Result&lt;std::fs::File, std::io::Error&gt;`  This tells us the return type of the File::open function is a Result&lt;T, E&gt;.  The generic parameter T has been filled in here with the type of the success value, std::fs::File, which is a file handle.  The type of E used in the error value is std::io::Error.   This return type means the call to File::open might succeed and return a file handle that we can read from or write to.  The function call also might fail: for example, the file might not exist, or we might not have permission to access the file.  The File::open function needs to have a way to tell us whether it succeeded or failed and at the same time give us either the file handle or error information. This information is exactly what the Result enum conveys.   In the case where File::open succeeds, the value in the variable f will be an instance of Ok that contains a file handle. In the case where it fails, the value in f will be an instance of Err that contains more information about the kind of error that happened.   We need to add to the code in Listing 9-3 to take different actions depending on the value File::open returns.  Listing 9-4 shows one way to handle the Result using a basic tool, the match expression that we discussed in Chapter 6.  use std::fs::File;  fn main() {     let f = File::open(\"hello.txt\");      let f = match f {         Ok(file) =&gt; file,         Err(error) =&gt; {             panic!(\"Problem opening the file: {:?}\", error)         },     }; }  Listing 9-4: Using a match expression to handle the Result variants that might be returned   Note that, like the Option enum, the Result enum and its variants have been brought into scope by the prelude,  so we don’t need to specify Result:: before the Ok and Err variants in the match arms.   Here we tell Rust that when the result is Ok, return the inner file value out of the Ok variant,  and we then assign that file handle value to the variable f. After the match, we can use the file handle for reading or writing.   The other arm of the match handles the case where we get an Err value from File::open.  In this example, we’ve chosen to call the panic! macro.  If there’s no file named hello.txt in our current directory and we run this code, we’ll see the following output from the panic! macro:   thread 'main' panicked at 'Problem opening the file: Error { repr: Os { code: 2, message: \"No such file or directory\" } }', src/main.rs:9:12  As usual, this output tells us exactly what has gone wrong.   Matching on Different Errors  The code in Listing 9-4 will panic! no matter why File::open failed.  What we want to do instead is take different actions for different failure reasons:  if File::open failed because the file doesn’t exist, we want to create the file and return the handle to the new file.  If File::open failed for any other reason—for example, because we didn’t have permission to open the file—we still want the code to panic! in the same way as it did in Listing 9-4.  Look at Listing 9-5, which adds an inner match expression.   use std::fs::File; use std::io::ErrorKind;  fn main() {     let f = File::open(\"hello.txt\");      let f = match f {         Ok(file) =&gt; file,         Err(error) =&gt; match error.kind() {             ErrorKind::NotFound =&gt; match File::create(\"hello.txt\") {                 Ok(fc) =&gt; fc,                 Err(e) =&gt; panic!(\"Problem creating the file: {:?}\", e),             },             other_error =&gt; panic!(\"Problem opening the file: {:?}\", other_error),         },     }; }  Listing 9-5: Handling different kinds of errors in different ways   The type of the value that File::open returns inside the Err variant is io::Error, which is a struct provided by the standard library.  This struct has a method kind that we can call to get an io::ErrorKind value.  The enum io::ErrorKind is provided by the standard library and has variants representing the different kinds of errors that might result from an io operation. The variant we want to use is ErrorKind::NotFound, which indicates the file we’re trying to open doesn’t exist yet. So we match on f, but we also have an inner match on error.kind().   The condition we want to check in the inner match is whether the value returned by error.kind() is the NotFound variant of the ErrorKind enum. If it is, we try to create the file with File::create.  However, because File::create could also fail, we need a second arm in the inner match expression.  When the file can’t be created, a different error message is printed.  The second arm of the outer match stays the same, so the program panics on any error besides the missing file error.   That’s a lot of match! The match expression is very useful but also very much a primitive.  In Chapter 13, you’ll learn about closures; the Result&lt;T, E&gt; type has many methods that accept a closure and are implemented using match expressions.  Using those methods will make your code more concise. A more seasoned Rustacean might write this code instead of Listing 9-5:   use std::fs::File; use std::io::ErrorKind;  fn main() {     let f = File::open(\"hello.txt\").unwrap_or_else(|error| {         if error.kind() == ErrorKind::NotFound {             File::create(\"hello.txt\").unwrap_or_else(|error| {                 panic!(\"Problem creating the file: {:?}\", error);             })         } else {             panic!(\"Problem opening the file: {:?}\", error);         }     }); }   Although this code has the same behavior as Listing 9-5, it doesn’t contain any match expressions and is cleaner to read.  Come back to this example after you’ve read Chapter 13, and look up the unwrap_or_else method in the standard library documentation.  Many more of these methods can clean up huge nested match expressions when you’re dealing with errors.   Shortcuts for Panic on Error: unwrap and expect  Using match works well enough, but it can be a bit verbose and doesn’t always communicate intent well.  The Result&lt;T, E&gt; type has many helper methods defined on it to do various tasks.  One of those methods, called unwrap, is a shortcut method that is implemented just like the match expression we wrote in Listing 9-4.  If the Result value is the Ok variant, unwrap will return the value inside the Ok.  If the Result is the Err variant, unwrap will call the panic! macro for us. Here is an example of unwrap in action:  use std::fs::File;  fn main() {     let f = File::open(\"hello.txt\").unwrap(); }  If we run this code without a hello.txt file, we’ll see an error message from the panic! call that the unwrap method makes:  thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error { repr: Os { code: 2, message: \"No such file or directory\" } }', src/libcore/result.rs:906:4   Another method, expect, which is similar to unwrap, lets us also choose the panic! error message.  Using expect instead of unwrap and providing good error messages can convey your intent and make tracking down the source of a panic easier.  The syntax of expect looks like this:  use std::fs::File;  fn main() {     let f = File::open(\"hello.txt\").expect(\"Failed to open hello.txt\"); }  We use expect in the same way as unwrap: to return the file handle or call the panic! macro.  The error message used by expect in its call to panic! will be the parameter that we pass to expect, rather than the default panic! message that unwrap uses.  Here’s what it looks like:   thread 'main' panicked at 'Failed to open hello.txt: Error { repr: Os { code: 2, message: \"No such file or directory\" } }', src/libcore/result.rs:906:4  Because this error message starts with the text we specified, Failed to open hello.txt, it will be easier to find where in the code this error message is coming from.  If we use unwrap in multiple places, it can take more time to figure out exactly which unwrap is causing the panic because all unwrap calls that panic print the same message.   Propagating Errors  When you’re writing a function whose implementation calls something that might fail, instead of handling the error within this function, you can return the error to the calling code so that it can decide what to do.  This is known as propagating the error and gives more control to the calling code, where there might be more information or logic that dictates how the error should be handled than what you have available in the context of your code.   For example, Listing 9-6 shows a function that reads a username from a file.  If the file doesn’t exist or can’t be read, this function will return those errors to the code that called this function.  use std::io; use std::io::Read; use std::fs::File;  fn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {     let f = File::open(\"hello.txt\");      let mut f = match f {         Ok(file) =&gt; file,         Err(e) =&gt; return Err(e),     };      let mut s = String::new();      match f.read_to_string(&amp;mut s) {         Ok(_) =&gt; Ok(s),         Err(e) =&gt; Err(e),     } }  Listing 9-6: A function that returns errors to the calling code using match   This function can be written in a much shorter way, but we’re going to start by doing a lot of it manually in order to explore error handling; at the end, we’ll show the shorter way.  Let’s look at the return type of the function first: Result&lt;String, io::Error&gt;.  This means the function is returning a value of the type Result&lt;T, E&gt; where the generic parameter T has been filled in with the concrete type String and the generic type E has been filled in with the concrete type io::Error.  If this function succeeds without any problems, the code that calls this function will receive an Ok value that holds a String—the username that this function read from the file.  If this function encounters any problems, the code that calls this function will receive an Err value that holds an instance of io::Error that contains more information about what the problems were.  We chose io::Error as the return type of this function because that happens to be the type of the error value returned from both of the operations we’re calling in this function’s body that might fail: the File::open function and the read_to_string method.   The body of the function starts by calling the File::open function. Then we handle the Result value returned with a match similar to the match in Listing 9-4, only instead of calling panic! in the Err case, we return early from this function and pass the error value from File::open back to the calling code as this function’s error value. If File::open succeeds, we store the file handle in the variable f and continue.   Then we create a new String in variable s and call the read_to_string method on the file handle in f to read the contents of the file into s.  The read_to_string method also returns a Result because it might fail, even though File::open succeeded.  So we need another match to handle that Result: if read_to_string succeeds, then our function has succeeded, and we return the username from the file that’s now in s wrapped in an Ok.  If read_to_string fails, we return the error value in the same way that we returned the error value in the match that handled the return value of File::open.  However, we don’t need to explicitly say return, because this is the last expression in the function.   The code that calls this code will then handle getting either an Ok value that contains a username or an Err value that contains an io::Error.  We don’t know what the calling code will do with those values.  If the calling code gets an Err value, it could call panic! and crash the program, use a default username, or look up the username from somewhere other than a file, for example.  We don’t have enough information on what the calling code is actually trying to do, so we propagate all the success or error information upward for it to handle appropriately.   This pattern of propagating errors is so common in Rust that Rust provides the question mark operator ? to make this easier.   A Shortcut for Propagating Errors: the ? Operator  Listing 9-7 shows an implementation of read_username_from_file that has the same functionality as it had in Listing 9-6, but this implementation uses the ? operator.  use std::io; use std::io::Read; use std::fs::File;  fn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {     let mut f = File::open(\"hello.txt\")?;     let mut s = String::new();     f.read_to_string(&amp;mut s)?;     Ok(s) }  Listing 9-7: A function that returns errors to the calling code using the ? operator   I don’t know about this section… So confusing  The ? placed after a Result value is defined to work in almost the same way as the match expressions we defined to handle the Result values in Listing 9-6.  If the value of the Result is an Ok, the value inside the Ok will get returned from this expression, and the program will continue.  If the value is an Err, the Err will be returned from the whole function as if we had used the return keyword so the error value gets propagated to the calling code.   There is a difference between what the match expression from Listing 9-6 and the ? operator do: error values that have the ? operator called on them go through the from function, defined in the From trait in the standard library, which is used to convert errors from one type into another.  When the ? operator calls the from function, the error type received is converted into the error type defined in the return type of the current function.  This is useful when a function returns one error type to represent all the ways a function might fail, even if parts might fail for many different reasons.  As long as each error type implements the from function to define how to convert itself to the returned error type, the ? operator takes care of the conversion automatically.   In the context of Listing 9-7, the ? at the end of the File::open call will return the value inside an Ok to the variable f.  If an error occurs, the ? operator will return early out of the whole function and give any Err value to the calling code.  The same thing applies to the ? at the end of the read_to_string call.   The ? operator eliminates a lot of boilerplate and makes this function’s implementation simpler.  We could even shorten this code further by chaining method calls immediately after the ?, as shown in Listing 9-8.  use std::io; use std::io::Read; use std::fs::File;  fn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {     let mut s = String::new();      File::open(\"hello.txt\")?.read_to_string(&amp;mut s)?;      Ok(s) }  Listing 9-8: Chaining method calls after the ? operator   We’ve moved the creation of the new String in s to the beginning of the function; that part hasn’t changed.  Instead of creating a variable f, we’ve chained the call to read_to_string directly onto the result of File::open(\"hello.txt\")?.  We still have a ? at the end of the read_to_string call, and we still return an Ok value containing the username in s when both File::open and read_to_string succeed rather than returning errors.  The functionality is again the same as in Listing 9-6 and Listing 9-7; this is just a different, more ergonomic way to write it.   Speaking of different ways to write this function, Listing 9-9 shows that there’s a way to make this even shorter.  use std::io; use std::fs;  fn read_username_from_file() -&gt; Result&lt;String, io::Error&gt; {     fs::read_to_string(\"hello.txt\") }  Listing 9-9: Using fs::read_to_string instead of opening and then reading the file   Reading a file into a string is a fairly common operation, so Rust provides the convenient fs::read_to_string function that opens the file, creates a new String, reads the contents of the file, puts the contents into that String, and returns it. Of course, using fs::read_to_string doesn’t give us the opportunity to explain all the error handling, so we did it the longer way first.   The ? Operator Can Be Used in Functions That Return Result  The ? operator can be used in functions that have a return type of Result, because it is defined to work in the same way as the match expression we defined in Listing 9-6. The part of the match that requires a return type of Result is return Err(e), so the return type of the function can be a Result to be compatible with this return.   Let’s look at what happens if we use the ? operator in the main function, which you’ll recall has a return type of ():  use std::fs::File;  fn main() {     let f = File::open(\"hello.txt\")?; }  When we compile this code, we get the following error message:   error[E0277]: the `?` operator can only be used in a function that returns `Result` or `Option` (or another type that implements `std::ops::Try`)  --&gt; src/main.rs:4:13   | 4 |     let f = File::open(\"hello.txt\")?;   |             ^^^^^^^^^^^^^^^^^^^^^^^^ cannot use the `?` operator in a   function that returns `()`   |   = help: the trait `std::ops::Try` is not implemented for `()`   = note: required by `std::ops::Try::from_error`  This error points out that we’re only allowed to use the ? operator in a function that returns Result or Option or another type that implements std::ops::Try.  When you’re writing code in a function that doesn’t return one of these types, and you want to use ? when you call other functions that return Result&lt;T, E&gt;, you have two choices to fix this problem.  One technique is to change the return type of your function to be Result&lt;T, E&gt; if you have no restrictions preventing that.  The other technique is to use a match or one of the Result&lt;T, E&gt; methods to handle the Result&lt;T, E&gt; in whatever way is appropriate.   The main function is special, and there are restrictions on what its return type must be. One valid return type for main is (), and conveniently, another valid return type is Result&lt;T, E&gt;, as shown here:  use std::error::Error; use std::fs::File;  fn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     let f = File::open(\"hello.txt\")?;      Ok(()) }  The Box&lt;dyn Error&gt; type is called a trait object, which we’ll talk about in the “Using Trait Objects that Allow for Values of Different Types” section in Chapter 17.  For now, you can read Box&lt;dyn Error&gt; to mean “any kind of error.” Using ? in a main function with this return type is allowed.   Now that we’ve discussed the details of calling panic! or returning Result, let’s return to the topic of how to decide which is appropriate to use in which cases.   To panic! or Not to panic!  So how do you decide when you should call panic! and when you should return Result? When code panics, there’s no way to recover.  You could call panic! for any error situation, whether there’s a possible way to recover or not, but then you’re making the decision on behalf of the code calling your code that a situation is unrecoverable.  When you choose to return a Result value, you give the calling code options rather than making the decision for it.  The calling code could choose to attempt to recover in a way that’s appropriate for its situation, or it could decide that an Err value in this case is unrecoverable, so it can call panic! and turn your recoverable error into an unrecoverable one.  Therefore, returning Result is a good default choice when you’re defining a function that might fail.   In rare situations, it’s more appropriate to write code that panics instead of returning a Result.  Let’s explore why it’s appropriate to panic in examples, prototype code, and tests.  Then we’ll discuss situations in which the compiler can’t tell that failure is impossible, but you as a human can.  The chapter will conclude with some general guidelines on how to decide whether to panic in library code.   Examples, Prototype Code, and Tests  When you’re writing an example to illustrate some concept, having robust error-handling code in the example as well can make the example less clear.  In examples, it’s understood that a call to a method like unwrap that could panic is meant as a placeholder for the way you’d want your application to handle errors, which can differ based on what the rest of your code is doing.   Similarly, the unwrap and expect methods are very handy when prototyping, before you’re ready to decide how to handle errors.  They leave clear markers in your code for when you’re ready to make your program more robust.   If a method call fails in a test, you’d want the whole test to fail, even if that method isn’t the functionality under test.  Because panic! is how a test is marked as a failure, calling unwrap or expect is exactly what should happen.   Cases in Which You Have More Information Than the Compiler  It would also be appropriate to call unwrap when you have some other logic that ensures the Result will have an Ok value, but the logic isn’t something the compiler understands.  You’ll still have a Result value that you need to handle: whatever operation you’re calling still has the possibility of failing in general, even though it’s logically impossible in your particular situation.  If you can ensure by manually inspecting the code that you’ll never have an Err variant, it’s perfectly acceptable to call unwrap. Here’s an example:   use std::net::IpAddr;  let home: IpAddr = \"127.0.0.1\".parse().unwrap();  We’re creating an IpAddr instance by parsing a hardcoded string.  We can see that 127.0.0.1 is a valid IP address, so it’s acceptable to use unwrap here.  However, having a hardcoded, valid string doesn’t change the return type of the parse method: we still get a Result value, and the compiler will still make us handle the Result as if the Err variant is a possibility because the compiler isn’t smart enough to see that this string is always a valid IP address.  If the IP address string came from a user rather than being hardcoded into the program and therefore did have a possibility of failure, we’d definitely want to handle the Result in a more robust way instead.   Guidelines for Error Handling  It’s advisable to have your code panic when it’s possible that your code could end up in a bad state.  In this context, a bad state is when some assumption, guarantee, contract, or invariant has been broken, such as when invalid values, contradictory values, or missing values are passed to your code—plus one or more of the following:      The bad state is not something that’s expected to happen occasionally.   Your code after this point needs to rely on not being in this bad state.   There’s not a good way to encode this information in the types you use.   If someone calls your code and passes in values that don’t make sense, the best choice might be to call panic! and alert the person using your library to the bug in their code so they can fix it during development. Similarly, panic!is often appropriate if you’re calling external code that is out of your control and it returns an invalid state that you have no way of fixing.   However, when failure is expected, it’s more appropriate to return a Result than to make a panic! call.  Examples include a parser being given malformed data or an HTTP request returning a status that indicates you have hit a rate limit.  In these cases, returning a Result indicates that failure is an expected possibility that the calling code must decide how to handle.   When your code performs operations on values, your code should verify the values are valid first and panic if the values aren’t valid.  This is mostly for safety reasons: attempting to operate on invalid data can expose your code to vulnerabilities.  This is the main reason the standard library will call panic! if you attempt an out-of-bounds memory access: trying to access memory that doesn’t belong to the current data structure is a common security problem.  Functions often have contracts: their behavior is only guaranteed if the inputs meet particular requirements.  Panicking when the contract is violated makes sense because a contract violation always indicates a caller-side bug and it’s not a kind of error you want the calling code to have to explicitly handle.  In fact, there’s no reasonable way for calling code to recover; the calling programmers need to fix the code.  Contracts for a function, especially when a violation will cause a panic, should be explained in the API documentation for the function.   However, having lots of error checks in all of your functions would be verbose and annoying.  Fortunately, you can use Rust’s type system (and thus the type checking the compiler does) to do many of the checks for you.  If your function has a particular type as a parameter, you can proceed with your code’s logic knowing that the compiler has already ensured you have a valid value.  For example, if you have a type rather than an Option, your program expects to have something rather than nothing.  Your code then doesn’t have to handle two cases for the Some and None variants: it will only have one case for definitely having a value.  Code trying to pass nothing to your function won’t even compile, so your function doesn’t have to check for that case at runtime.  Another example is using an unsigned integer type such as u32, which ensures the parameter is never negative.   Creating Custom Types for Validation  Let’s take the idea of using Rust’s type system to ensure we have a valid value one step further and look at creating a custom type for validation.  Recall the guessing game in Chapter 2 in which our code asked the user to guess a number between 1 and 100.  We never validated that the user’s guess was between those numbers before checking it against our secret number; we only validated that the guess was positive.  In this case, the consequences were not very dire: our output of “Too high” or “Too low” would still be correct.  But it would be a useful enhancement to guide the user toward valid guesses and have different behavior when a user guesses a number that’s out of range versus when a user types, for example, letters instead.   One way to do this would be to parse the guess as an i32 instead of only a u32 to allow potentially negative numbers, and then add a check for the number being in range, like so:  loop {     // --snip--      let guess: i32 = match guess.trim().parse() {         Ok(num) =&gt; num,         Err(_) =&gt; continue,     };      if guess &lt; 1 || guess &gt; 100 {         println!(\"The secret number will be between 1 and 100.\");         continue;     }      match guess.cmp(&amp;secret_number) {     // --snip-- }  The if expression checks whether our value is out of range, tells the user about the problem, and calls continue to start the next iteration of the loop and ask for another guess. After the if expression, we can proceed with the comparisons between guess and the secret number knowing that guess is between 1 and 100.   However, this is not an ideal solution: if it was absolutely critical that the program only operated on values between 1 and 100, and it had many functions with this requirement, having a check like this in every function would be tedious (and might impact performance).   Instead, we can make a new type and put the validations in a function to create an instance of the type rather than repeating the validations everywhere. That way, it’s safe for functions to use the new type in their signatures and confidently use the values they receive. Listing 9-10 shows one way to define a Guess type that will only create an instance of 1 if the new function receives a value between 1 and 100.   pub struct Guess {     value: i32, }  impl Guess {     pub fn new(value: i32) -&gt; Guess {         if value &lt; 1 || value &gt; 100 {             panic!(\"Guess value must be between 1 and 100, got {}.\", value);         }          Guess {             value         }     }      pub fn value(&amp;self) -&gt; i32 {         self.value     } }  Listing 9-10: A Guess type that will only continue with values between 1 and 100   First, we define a struct named Guess that has a field named value that holds an i32. This is where the number will be stored.   Then we implement an associated function named new on Guess that creates instances of Guess values. T he new function is defined to have one parameter named value of type i32 and to return a Guess.  The code in the body of the new function tests value to make sure it’s between 1 and 100.  If value doesn’t pass this test, we make a panic! call, which will alert the programmer who is writing the calling code that they have a bug they need to fix, because creating a Guess with a value outside this range would violate the contract that Guess::new is relying on.  The conditions in which Guess::new might panic should be discussed in its public-facing API documentation;  we’ll cover documentation conventions indicating the possibility of a panic! in the API documentation that you create in Chapter 14.  If value does pass the test, we create a new Guess with its value field set to the value parameter and return the Guess.   Next, we implement a method named value that borrows self, doesn’t have any other parameters, and returns an i32.  This kind of method is sometimes called a getter, because its purpose is to get some data from its fields and return it.  This public method is necessary because the value field of the Guess struct is private.  It’s important that the value field be private so code using the Guess struct is not allowed to set value directly: code outside the module must use the Guess::new function to create an instance of Guess, thereby ensuring there’s no way for a Guess to have a value that hasn’t been checked by the conditions in the Guess::new function.   A function that has a parameter or returns only numbers between 1 and 100 could then declare in its signature that it takes or returns a Guess rather than an i32 and wouldn’t need to do any additional checks in its body.   Summary  Rust’s error handling features are designed to help you write more robust code.  The panic! macro signals that your program is in a state it can’t handle and lets you tell the process to stop instead of trying to proceed with invalid or incorrect values.  The Result enum uses Rust’s type system to indicate that operations might fail in a way that your code could recover from.  You can use Result to tell code that calls your code that it needs to handle potential success or failure as well.  Using panic! and Result in the appropriate situations will make your code more reliable in the face of inevitable problems.  ","categories": ["RUST Language"],
        "tags": ["Error","Handler","Panic"],
        "url": "https://jjungs-lee.github.io//rust/9.Error-Handling",
        "teaser":null},{
        "title": "RUST : 10. Generic Types, Traits, and Lifetimes",
        "excerpt":"Generic Types, Traits, and Lifetimes  Every programming language has tools for effectively handling the duplication of concepts.  In Rust, one such tool is generics. Generics are abstract stand-ins for concrete types or other properties.  When we’re writing code, we can express the behavior of generics or how they relate to other generics without knowing what will be in their place when compiling and running the code.   Similar to the way a function takes parameters with unknown values to run the same code on multiple concrete values,  functions can take parameters of some generic type instead of a concrete type, like i32 or String.  In fact, we’ve already used generics in Chapter 6 with Option&lt;T&gt;, Chapter 8 with Vec&lt;T&gt; and HashMap&lt;K, V&gt;, and Chapter 9 with Result&lt;T, E&gt;.  In this chapter, you’ll explore how to define your own types, functions, and methods with generics!   First, we’ll review how to extract a function to reduce code duplication. Next, we’ll use the same technique to make a generic function from two functions that differ only in the types of their parameters. We’ll also explain how to use generic types in struct and enum definitions.   Then you’ll learn how to use traits to define behavior in a generic way.  You can combine traits with generic types to constrain a generic type to only those types that have a particular behavior, as opposed to just any type.   Finally, we’ll discuss lifetimes, a variety of generics that give the compiler information about how references relate to each other.  Lifetimes allow us to borrow values in many situations while still enabling the compiler to check that the references are valid.   Removing Duplication by Extracting a Function  Before diving into generics syntax, let’s first look at how to remove duplication that doesn’t involve generic types by extracting a function.  Then we’ll apply this technique to extract a generic function! In the same way that you recognize duplicated code to extract into a function, you’ll start to recognize duplicated code that can use generics.   Consider a short program that finds the largest number in a list, as shown in Listing 10-1.  fn main() {     let number_list = vec![34, 50, 25, 100, 65];      let mut largest = number_list[0];      for number in number_list {         if number &gt; largest {             largest = number;         }     }      println!(\"The largest number is {}\", largest); }  Listing 10-1: Code to find the largest number in a list of numbers  This code stores a list of integers in the variable number_list and places the first number in the list in a variable named largest.  Then it iterates through all the numbers in the list, and if the current number is greater than the number stored in largest, it replaces the number in that variable.  However, if the current number is less than or equal to the largest number seen so far, the variable doesn’t change, and the code moves on to the next number in the list.  After considering all the numbers in the list, largest should hold the largest number, which in this case is 100.   To find the largest number in two different lists of numbers, we can duplicate the code in Listing 10-1 and use the same logic at two different places in the program, as shown in Listing 10-2.  fn main() {     let number_list = vec![34, 50, 25, 100, 65];      let mut largest = number_list[0];      for number in number_list {         if number &gt; largest {             largest = number;         }     }      println!(\"The largest number is {}\", largest);      let number_list = vec![102, 34, 6000, 89, 54, 2, 43, 8];      let mut largest = number_list[0];      for number in number_list {         if number &gt; largest {             largest = number;         }     }      println!(\"The largest number is {}\", largest);  Listing 10-2: Code to find the largest number in two lists of numbers  Although this code works, duplicating code is tedious and error prone.  We also have to update the code in multiple places when we want to change it.   To eliminate this duplication, we can create an abstraction by defining a function that operates on any list of integers given to it in a parameter.  This solution makes our code clearer and lets us express the concept of finding the largest number in a list abstractly.   In Listing 10-3, we extracted the code that finds the largest number into a function named largest.  Unlike the code in Listing 10-1, which can find the largest number in only one particular list, this program can find the largest number in two different lists.  fn largest(list: &amp;[i32]) -&gt; i32 {     let mut largest = list[0];      for &amp;item in list {         if item &gt; largest {             largest = item;         }     }      largest }  fn main() {     let number_list = vec![34, 50, 25, 100, 65];      let result = largest(&amp;number_list);     println!(\"The largest number is {}\", result);      let number_list = vec![102, 34, 6000, 89, 54, 2, 43, 8];      let result = largest(&amp;number_list);     println!(\"The largest number is {}\", result); }  Listing 10-3: Abstracted code to find the largest number in two lists  The largest function has a parameter called list,  which represents any concrete slice of i32 values that we might pass into the function. As a result, when we call the function, the code runs on the specific values that we pass in.   In sum, here are the steps we took to change the code from Listing 10-2 to Listing 10-3:      Identify duplicate code.   Extract the duplicate code into the body of the function and specify the inputs and return values of that code in the function signature.   Update the two instances of duplicated code to call the function instead.   Next, we’ll use these same steps with generics to reduce code duplication in different ways.  In the same way that the function body can operate on an abstract list instead of specific values,  generics allow code to operate on abstract types.   Generic Data Types  We can use generics to create definitions for items like function signatures or structs,  which we can then use with many different concrete data types.  Let’s first look at how to define functions, structs, enums, and methods using generics.  Then we’ll discuss how generics affect code performance.   In Function Definitions  When defining a function that uses generics, we place the generics in the signature of the function where we would usually specify the data types of the parameters and return value.  Doing so makes our code more flexible and provides more functionality to callers of our function while preventing code duplication.   Continuing with our largest function, Listing 10-4 shows two functions that both find the largest value in a slice.  fn largest_i32(list: &amp;[i32]) -&gt; i32 {     let mut largest = list[0];      for &amp;item in list.iter() {         if item &gt; largest {             largest = item;         }     }      largest }  fn largest_char(list: &amp;[char]) -&gt; char {     let mut largest = list[0];      for &amp;item in list.iter() {         if item &gt; largest {             largest = item;         }     }      largest }  fn main() {     let number_list = vec![34, 50, 25, 100, 65];      let result = largest_i32(&amp;number_list);     println!(\"The largest number is {}\", result);      let char_list = vec!['y', 'm', 'a', 'q'];      let result = largest_char(&amp;char_list);     println!(\"The largest char is {}\", result); }  Listing 10-4: Two functions that differ only in their names and the types in their signatures   The largest_i32 function is the one we extracted in Listing 10-3 that finds the largest i32 in a slice.  The largest_char function finds the largest char in a slice.  The function bodies have the same code, so let’s eliminate the duplication by introducing a generic type parameter in a single function.   To parameterize the types in the new function we’ll define, we need to name the type parameter, just as we do for the value parameters to a function. You can use any identifier as a type parameter name.  But we’ll use T because, by convention, parameter names in Rust are short, often just a letter, and Rust’s type-naming convention is CamelCase. Short for “type,” T is the default choice of most Rust programmers.   When we use a parameter in the body of the function, we have to declare the parameter name in the signature so the compiler knows what that name means.  Similarly, when we use a type parameter name in a function signature, we have to declare the type parameter name before we use it.  To define the generic largest function, place type name declarations inside angle brackets, &lt;&gt;, between the name of the function and the parameter list, like this:  fn largest&lt;T&gt;(list: &amp;[T]) -&gt; T {   We read this definition as: the function largest is generic over some type T.  This function has one parameter named list, which is a slice of values of type T.  The largest function will return a value of the same type T.   Listing 10-5 shows the combined largest function definition using the generic data type in its signature.  The listing also shows how we can call the function with either a slice of i32 values or char values.  Note that this code won’t compile yet, but we’ll fix it later in this chapter.  fn largest&lt;T&gt;(list: &amp;[T]) -&gt; T {     let mut largest = list[0];      for &amp;item in list.iter() {         if item &gt; largest {             largest = item;         }     }      largest }  fn main() {     let number_list = vec![34, 50, 25, 100, 65];      let result = largest(&amp;number_list);     println!(\"The largest number is {}\", result);      let char_list = vec!['y', 'm', 'a', 'q'];      let result = largest(&amp;char_list);     println!(\"The largest char is {}\", result); }  Listing 10-5: A definition of the largest function that uses generic type parameters but doesn’t compile yet   If we compile this code right now, we’ll get this error:   error[E0369]: binary operation `&gt;` cannot be applied to type `T`  --&gt; src/main.rs:5:12   | 5 |         if item &gt; largest {   |            ^^^^^^^^^^^^^^   |   = note: an implementation of `std::cmp::PartialOrd`            might be missing for `T`  The note mentions std::cmp::PartialOrd, which is a trait.  We’ll talk about traits in the next section.  For now, this error states that the body of largest won’t work for all possible types that T could be.  Because we want to compare values of type T in the body, we can only use types whose values can be ordered.  To enable comparisons, the standard library has the std::cmp::PartialOrd trait that you can implement on types (see Appendix C for more on this trait).  You’ll learn how to specify that a generic type has a particular trait in the “Traits as Parameters” section, but let’s first explore other ways of using generic type parameters.   In Struct Definitions  We can also define structs to use a generic type parameter in one or more fields using the &lt;&gt; syntax.  Listing 10-6 shows how to define a Point&lt;T&gt; struct to hold x and y coordinate values of any type.  struct Point&lt;T&gt; {     x: T,     y: T, }  fn main() {     let integer = Point { x: 5, y: 10 };     let float = Point { x: 1.0, y: 4.0 }; }  ####  Listing 10-6: A Point struct that holds x and y values of type T   The syntax for using generics in struct definitions is similar to that used in function definitions. First, we declare the name of the type parameter inside angle brackets just after the name of the struct. Then we can use the generic type in the struct definition where we would otherwise specify concrete data types.   Note that because we’ve used only one generic type to define Point, this definition says that the Point struct is generic over some type T, and the fields x and y are both that same type, whatever that type may be. If we create an instance of a Point that has values of different types, as in Listing 10-7, our code won’t compile.  struct Point&lt;T&gt; {     x: T,     y: T, }  fn main() {     let wont_work = Point { x: 5, y: 4.0 }; }  Listing 10-7: The fields x and y must be the same type because both have the same generic data type T.  In this example, when we assign the integer value 5 to x, we let the compiler know that the generic type T will be an integer for this instance of Point&lt;T&gt;. Then when we specify 4.0 for y, which we’ve defined to have the same type as x, we’ll get a type mismatch error like this:  error[E0308]: mismatched types  --&gt; src/main.rs:7:38   | 7 |     let wont_work = Point { x: 5, y: 4.0 };   |                                      ^^^ expected integer, found floating-point number   |   = note: expected type `{integer}`              found type `{float}`  To define a Point struct where x and y are both generics but could have different types, we can use multiple generic type parameters.  For example, in Listing 10-8, we can change the definition of Point to be generic over types T and U where x is of type T and y is of type U.  struct Point&lt;T, U&gt; {     x: T,     y: U, }  fn main() {     let both_integer = Point { x: 5, y: 10 };     let both_float = Point { x: 1.0, y: 4.0 };     let integer_and_float = Point { x: 5, y: 4.0 }; }  Listing 10-8: A Point&lt;T, U&gt; generic over two types so that x and y can be values of different types  Now all the instances of Point shown are allowed! You can use as many generic type parameters in a definition as you want, but using more than a few makes your code hard to read. When you need lots of generic types in your code, it could indicate that your code needs restructuring into smaller pieces.   In Enum Definitions  As we did with structs, we can define enums to hold generic data types in their variants.  Let’s take another look at the Option&lt;T&gt; enum that the standard library provides, which we used in Chapter 6:  enum Option&lt;T&gt; {     Some(T),     None, }  This definition should now make more sense to you.  As you can see, Option&lt;T&gt; is an enum that is generic over type T and has two variants: Some, which holds one value of type T, and a None variant that doesn’t hold any value.  By using the Option&lt;T&gt; enum, we can express the abstract concept of having an optional value, and because Option&lt;T&gt; is generic, we can use this abstraction no matter what the type of the optional value is.   Enums can use multiple generic types as well.  The definition of the Result enum that we used in Chapter 9 is one example:  enum Result&lt;T, E&gt; {     Ok(T),     Err(E), }  The Result enum is generic over two types, T and E, and has two variants: Ok, which holds a value of type T, and Err, which holds a value of type E.  This definition makes it convenient to use the Result enum anywhere we have an operation that might succeed (return a value of some type T) or fail (return an error of some type E).  In fact, this is what we used to open a file in Listing 9-3, where T was filled in with the type std::fs::File when the file was opened successfully and E was filled in with the type std::io::Error when there were problems opening the file.   When you recognize situations in your code with multiple struct or enum definitions that differ only in the types of the values they hold, you can avoid duplication by using generic types instead.   In Method Definitions  We can implement methods on structs and enums (as we did in Chapter 5) and use generic types in their definitions, too.  Listing 10-9 shows the Point&lt;T&gt; struct we defined in Listing 10-6 with a method named x implemented on it.  struct Point&lt;T&gt; {     x: T,     y: T, }  impl&lt;T&gt; Point&lt;T&gt; {     fn x(&amp;self) -&gt; &amp;T {         &amp;self.x     } }  fn main() {     let p = Point { x: 5, y: 10 };      println!(\"p.x = {}\", p.x()); }  Listing 10-9: Implementing a method named x on the Point struct that will return a reference to the x field of type T   Here, we’ve defined a method named x on Point&lt;T&gt; that returns a reference to the data in the field x.   Note that we have to declare T just after impl so we can use it to specify that we’re implementing methods on the type Point&lt;T&gt;.  By declaring T as a generic type after impl, Rust can identify that the type in the angle brackets in Point is a generic type rather than a concrete type.   We could, for example, implement methods only on Point&lt;f32&gt; instances rather than on Point&lt;T&gt; instances with any generic type.  In Listing 10-10 we use the concrete type f32, meaning we don’t declare any types after impl.   impl Point&lt;f32&gt; {     fn distance_from_origin(&amp;self) -&gt; f32 {         (self.x.powi(2) + self.y.powi(2)).sqrt()     } }  Listing 10-10: An impl block that only applies to a struct with a particular concrete type for the generic type parameter T  This code means the type Point&lt;f32&gt; will have a method named distance_from_origin and other instances of Point&lt;T&gt; where T is not of type f32 will not have this method defined.  The method measures how far our point is from the point at coordinates (0.0, 0.0) and uses mathematical operations that are available only for floating point types.   Generic type parameters in a struct definition aren’t always the same as those you use in that struct’s method signatures.  For example, Listing 10-11 defines the method mixup on the Point&lt;T, U&gt; struct from Listing 10-8.  The method takes another Point as a parameter, which might have different types from the self Point we’re calling mixup on.  The method creates a new Point instance with the x value from the self Point (of type T) and the y value from the passed-in Point (of type W).  struct Point&lt;T, U&gt; {     x: T,     y: U, }  impl&lt;T, U&gt; Point&lt;T, U&gt; {     fn mixup&lt;V, W&gt;(self, other: Point&lt;V, W&gt;) -&gt; Point&lt;T, W&gt; {         Point {             x: self.x,             y: other.y,         }     } }  fn main() {     let p1 = Point { x: 5, y: 10.4 };     let p2 = Point { x: \"Hello\", y: 'c'};      let p3 = p1.mixup(p2);      println!(\"p3.x = {}, p3.y = {}\", p3.x, p3.y); }  Listing 10-11: A method that uses different generic types from its struct’s definition  In main, we’ve defined a Point that has an i32 for x (with value 5) and an f64 for y (with value 10.4).  The p2 variable is a Point struct that has a string slice for x (with value \"Hello\") and a char for y (with value c).  Calling mixup on p1 with the argument p2 gives us p3, which will have an i32 for x, because x came from p1.  The p3 variable will have a char for y, because y came from p2. The println! macro call will print p3.x = 5, p3.y = c.   The purpose of this example is to demonstrate a situation in which some generic parameters are declared with impl and some are declared with the method definition.  Here, the generic parameters T and U are declared after impl, because they go with the struct definition.  The generic parameters V and W are declared after fn mixup, because they’re only relevant to the method.   Performance of Code Using Generics  You might be wondering whether there is a runtime cost when you’re using generic type parameters.  The good news is that Rust implements generics in such a way that your code doesn’t run any slower using generic types than it would with concrete types.   Rust accomplishes this by performing monomorphization of the code that is using generics at compile time.  Monomorphization is the process of turning generic code into specific code by filling in the concrete types that are used when compiled.   In this process, the compiler does the opposite of the steps we used to create the generic function in Listing 10-5:  the compiler looks at all the places where generic code is called and generates code for the concrete types the generic code is called with.   Let’s look at how this works with an example that uses the standard library’s Option&lt;T&gt; enum:  let integer = Some(5); let float = Some(5.0);  When Rust compiles this code, it performs monomorphization.  During that process, the compiler reads the values that have been used in Option&lt;T&gt; instances and identifies two kinds of Option&lt;T&gt;: one is i32 and the other is f64.  As such, it expands the generic definition of Option&lt;T&gt; into Option_i32 and Option_f64, thereby replacing the generic definition with the specific ones.   The monomorphized version of the code looks like the following. The generic Option&lt;T&gt; is replaced with the specific definitions created by the compiler:  enum Option_i32 {     Some(i32),     None, }  enum Option_f64 {     Some(f64),     None, }  fn main() {     let integer = Option_i32::Some(5);     let float = Option_f64::Some(5.0); }  Because Rust compiles generic code into code that specifies the type in each instance, we pay no runtime cost for using generics.  When the code runs, it performs just as it would if we had duplicated each definition by hand.  The process of monomorphization makes Rust’s generics extremely efficient at runtime.   Traits: Defining Shared Behavior  A trait tells the Rust compiler about functionality a particular type has and can share with other types.  We can use traits to define shared behavior in an abstract way.  We can use trait bounds to specify that a generic can be any type that has certain behavior.   Note: Traits are similar to a feature often called interfaces in other languages, although with some differences.   Defining a Trait  A type’s behavior consists of the methods we can call on that type.  Different types share the same behavior if we can call the same methods on all of those types.  Trait definitions are a way to group method signatures together to define a set of behaviors necessary to accomplish some purpose.   For example, let’s say we have multiple structs that hold various kinds and amounts of text: a NewsArticle struct that holds a news story filed in a particular location and a Tweet that can have at most 280 characters along with metadata that indicates whether it was a new tweet, a retweet, or a reply to another tweet.   We want to make a media aggregator library that can display summaries of data that might be stored in a NewsArticle or Tweet instance.  To do this, we need a summary from each type, and we need to request that summary by calling a summarize method on an instance.  Listing 10-12 shows the definition of a Summary trait that expresses this behavior.   pub trait Summary {     fn summarize(&amp;self) -&gt; String; }  Listing 10-12: A Summary trait that consists of the behavior provided by a summarize method  Here, we declare a trait using the trait keyword and then the trait’s name, which is Summary in this case.  Inside the curly brackets, we declare the method signatures that describe the behaviors of the types that implement this trait, which in this case is fn summarize(&amp;self) -&gt; String.   After the method signature, instead of providing an implementation within curly brackets, we use a semicolon.  Each type implementing this trait must provide its own custom behavior for the body of the method.  The compiler will enforce that any type that has the Summary trait will have the method summarize defined with this signature exactly.   A trait can have multiple methods in its body: the method signatures are listed one per line and each line ends in a semicolon.   Implementing a Trait on a Type  Now that we’ve defined the desired behavior using the Summary trait, we can implement it on the types in our media aggregator.  Listing 10-13 shows an implementation of the Summary trait on the NewsArticle struct that uses the headline, the author, and the location to create the return value of summarize.  For the Tweet struct, we define summarize as the username followed by the entire text of the tweet, assuming that tweet content is already limited to 280 characters.  pub struct NewsArticle {     pub headline: String,     pub location: String,     pub author: String,     pub content: String, }  impl Summary for NewsArticle {     fn summarize(&amp;self) -&gt; String {         format!(\"{}, by {} ({})\", self.headline, self.author, self.location)     } }  pub struct Tweet {     pub username: String,     pub content: String,     pub reply: bool,     pub retweet: bool, }  impl Summary for Tweet {     fn summarize(&amp;self) -&gt; String {         format!(\"{}: {}\", self.username, self.content)     } }  Listing 10-13: Implementing the Summary trait on the NewsArticle and Tweet types   Implementing a trait on a type is similar to implementing regular methods.  The difference is that after impl, we put the trait name that we want to implement, then use the for keyword, and then specify the name of the type we want to implement the trait for.  Within the impl block, we put the method signatures that the trait definition has defined.  Instead of adding a semicolon after each signature, we use curly brackets and fill in the method body with the specific behavior that we want the methods of the trait to have for the particular type.   After implementing the trait, we can call the methods on instances of NewsArticle and Tweet in the same way we call regular methods, like this:  let tweet = Tweet {     username: String::from(\"horse_ebooks\"),     content: String::from(\"of course, as you probably already know, people\"),     reply: false,     retweet: false, };  println!(\"1 new tweet: {}\", tweet.summarize());  This code prints 1 new tweet: horse_ebooks: of course, as you probably already know, people.   Note that because we defined the Summary trait and the NewsArticle and Tweet types in the same lib.rs in Listing 10-13, they’re all in the same scope.  Let’s say this lib.rs is for a crate we’ve called aggregator and someone else wants to use our crate’s functionality to implement the Summary trait on a struct defined within their library’s scope.  They would need to bring the trait into their scope first.  They would do so by specifying use aggregator::Summary;, which then would enable them to implement Summary for their type.  extern crate aggregator;  use aggregator::Summary;  struct WeatherForecast {     high_temp: f64,     low_temp: f64,     chance_of_precipitation: f64, }  impl Summary for WeatherForecast {     fn summarize(&amp;self) -&gt; String {         format!(\"The high will be {}, and the low will be {}. The chance of         precipitation is {}%.\", self.high_temp, self.low_temp,         self.chance_of_precipitation)     } }  The Summary trait would also need to be a public trait for another crate to implement it, which it is because we put the pub keyword before trait in Listing 10-12.   One restriction to note with trait implementations is that we can implement a trait on a type only if either the trait or the type is local to our crate.  For example, we can implement standard library traits like Display on a custom type like Tweet as part of our aggregator crate functionality, because the type Tweet is local to our aggregator crate.  We can also implement Summary on Vec&lt;T&gt; in our aggregator crate, because the trait Summary is local to our aggregator crate.   But we can’t implement external traits on external types.  For example, we can’t implement the Display trait on Vec&lt;T&gt; within our aggregator crate, because Display and Vec&lt;T&gt; are defined in the standard library and aren’t local to our aggregator crate.  This restriction is part of a property of programs called coherence, and more specifically the orphan rule, so named because the parent type is not present.  This rule ensures that other people’s code can’t break your code and vice versa.  Without the rule, two crates could implement the same trait for the same type, and Rust wouldn’t know which implementation to use.   Default Implementations  Sometimes it’s useful to have default behavior for some or all of the methods in a trait instead of requiring implementations for all methods on every type.  Then, as we implement the trait on a particular type, we can keep or override each method’s default behavior.   Listing 10-14 shows how to specify a default string for the summarize method of the Summary trait instead of only defining the method signature, as we did in Listing 10-12.  pub trait Summary {     fn summarize(&amp;self) -&gt; String {         String::from(\"(Read more...)\")     } }  Listing 10-14: Definition of a Summary trait with a default implementation of the summarize method  To use a default implementation to summarize instances of NewsArticle instead of defining a custom implementation, we specify an empty impl block with impl Summary for NewsArticle {}.   Even though we’re no longer defining the summarize method on NewsArticle directly,  we’ve provided a default implementation and specified that NewsArticle implements the Summary trait.  As a result, we can still call the summarize method on an instance of NewsArticle, like this:  let article = NewsArticle {     headline: String::from(\"Penguins win the Stanley Cup Championship!\"),     location: String::from(\"Pittsburgh, PA, USA\"),     author: String::from(\"Iceburgh\"),     content: String::from(\"The Pittsburgh Penguins once again are the best     hockey team in the NHL.\"), };  println!(\"New article available! {}\", article.summarize());  This code prints New article available! (Read more...).   Creating a default implementation for summarize doesn’t require us to change anything about the implementation of Summary on Tweet in Listing 10-13.  The reason is that the syntax for overriding a default implementation is the same as the syntax for implementing a trait method that doesn’t have a default implementation.   Default implementations can call other methods in the same trait, even if those other methods don’t have a default implementation.  In this way, a trait can provide a lot of useful functionality and only require implementors to specify a small part of it.  For example, we could define the Summary trait to have a summarize_author method whose implementation is required, and then define a summarize method that has a default implementation that calls the summarize_author method:  pub trait Summary {     fn summarize_author(&amp;self) -&gt; String;      fn summarize(&amp;self) -&gt; String {         format!(\"(Read more from {}...)\", self.summarize_author())     } }  To use this version of Summary, we only need to define summarize_author when we implement the trait on a type:  impl Summary for Tweet {     fn summarize_author(&amp;self) -&gt; String {         format!(\"@{}\", self.username)     } }  After we define summarize_author, we can call summarize on instances of the Tweet struct, and the default implementation of summarize will call the definition of summarize_author that we’ve provided.  Because we’ve implemented summarize_author, the Summary trait has given us the behavior of the summarize` method without requiring us to write any more code.  let tweet = Tweet {     username: String::from(\"horse_ebooks\"),     content: String::from(\"of course, as you probably already know, people\"),     reply: false,     retweet: false, };  println!(\"1 new tweet: {}\", tweet.summarize());  This code prints 1 new tweet: (Read more from @horse_ebooks...). Note that it isn’t possible to call the default implementation from an overriding implementation of that same method.   Traits as Parameters  Now that you know how to define and implement traits, we can explore how to use traits to define functions that accept many different types.   For example, in Listing 10-13, we implemented the Summary trait on the NewsArticle and Tweet types.  We can define a notify function that calls the summarize method on its item parameter, which is of some type that implements the Summary trait.  To do this, we can use the impl Trait syntax, like this:   pub fn notify(item: impl Summary) {     println!(\"Breaking news! {}\", item.summarize()); }  Instead of a concrete type for the item parameter, we specify the impl keyword and the trait name.  This parameter accepts any type that implements the specified trait.  In the body of notify, we can call any methods on item that come from the Summary trait, such as summarize.  We can call notify and pass in any instance of NewsArticle or Tweet.  Code that calls the function with any other type, such as a String or an i32, won’t compile because those types don’t implement Summary.   Trait Bound Syntax  The impl Trait syntax works for straightforward cases but is actually syntax sugar for a longer form, which is called a trait bound; it looks like this:  pub fn notify&lt;T: Summary&gt;(item: T) {     println!(\"Breaking news! {}\", item.summarize()); }  This longer form is equivalent to the example in the previous section but is more verbose.  We place trait bounds with the declaration of the generic type parameter after a colon and inside angle brackets.   The impl Trait syntax is convenient and makes for more concise code in simple cases.  The trait bound syntax can express more complexity in other cases.  For example, we can have two parameters that implement Summary. Using the impl Trait syntax looks like this:  pub fn notify(item1: impl Summary, item2: impl Summary) {  If we wanted this function to allow item1 and item2 to have different types, using impl Trait would be appropriate (as long as both types implement Summary).  If we wanted to force both parameters to have the same type, that’s only possible to express using a trait bound, like this:  pub fn notify&lt;T: Summary&gt;(item1: T, item2: T) {  The generic type T specified as the type of the item1 and item2 parameters constrains the function such that the concrete type of the value passed as an argument for item1 and item2 must be the same.   Specifying Multiple Trait Bounds with the + Syntax  We can also specify more than one trait bound.  Say we wanted notify to use display formatting on item as well as the summarize method:  we specify in the notify definition that item must implement both Display and Summary.  We can do so using the + syntax:  pub fn notify(item: impl Summary + Display) {  The + syntax is also valid with trait bounds on generic types:  pub fn notify&lt;T: Summary + Display&gt;(item: T) {  With the two trait bounds specified, the body of notify can call summarize and use {} to format item.   Clearer Trait Bounds with where Clauses  Using too many trait bounds has its downsides.  Each generic has its own trait bounds, so functions with multiple generic type parameters can contain lots of trait bound information between the function’s name and its parameter list, making the function signature hard to read.  For this reason, Rust has alternate syntax for specifying trait bounds inside a where clause after the function signature.  So instead of writing this:  fn some_function&lt;T: Display + Clone, U: Clone + Debug&gt;(t: T, u: U) -&gt; i32 {  we can use a where clause, like this:  fn some_function&lt;T, U&gt;(t: T, u: U) -&gt; i32     where T: Display + Clone,           U: Clone + Debug {  This function’s signature is less cluttered: the function name, parameter list, and return type are close together, similar to a function without lots of trait bounds.   Returning Types that Implement Traits  We can also use the impl Trait syntax in the return position to return a value of some type that implements a trait, as shown here:   fn returns_summarizable() -&gt; impl Summary {     Tweet {         username: String::from(\"horse_ebooks\"),         content: String::from(\"of course, as you probably already know, people\"),         reply: false,         retweet: false,     } }  By using impl Summary for the return type, we specify that the returns_summarizable function returns some type that implements the Summary trait without naming the concrete type.  In this case, returns_summarizable returns a Tweet, but the code calling this function doesn’t know that.   The ability to return a type that is only specified by the trait it implements is especially useful in the context of closures and iterators, which we cover in Chapter 13.  Closures and iterators create types that only the compiler knows or types that are very long to specify.  The impl Trait syntax lets you concisely specify that a function returns some type that implements the Iterator trait without needing to write out a very long type.   However, you can only use impl Trait if you’re returning a single type.  For example, this code that returns either a NewsArticle or a Tweet with the return type specified as impl Summary wouldn’t work:  fn returns_summarizable(switch: bool) -&gt; impl Summary {     if switch {         NewsArticle {             headline: String::from(\"Penguins win the Stanley Cup Championship!\"),             location: String::from(\"Pittsburgh, PA, USA\"),             author: String::from(\"Iceburgh\"),             content: String::from(\"The Pittsburgh Penguins once again are the best             hockey team in the NHL.\"),         }     } else {         Tweet {             username: String::from(\"horse_ebooks\"),             content: String::from(\"of course, as you probably already know, people\"),             reply: false,             retweet: false,         }     } }  Returning either a NewsArticle or a Tweet isn’t allowed due to restrictions around how the impl Trait syntax is implemented in the compiler.  We’ll cover how to write a function with this behavior in the “Using Trait Objects That Allow for Values of Different Types” section of Chapter 17.   Fixing the largest Function with Trait Bounds  Now that you know how to specify the behavior you want to use using the generic type parameter’s bounds, let’s return to Listing 10-5 to fix the definition of the largest function that uses a generic type parameter!  Last time we tried to run that code, we received this error:  error[E0369]: binary operation `&gt;` cannot be applied to type `T`  --&gt; src/main.rs:5:12   | 5 |         if item &gt; largest {   |            ^^^^^^^^^^^^^^   |   = note: an implementation of `std::cmp::PartialOrd` might be missing for `T`  In the body of largest we wanted to compare two values of type T using the greater than (&gt;) operator.  Because that operator is defined as a default method on the standard library trait std::cmp::PartialOrd, we need to specify PartialOrd in the trait bounds for T so the largest function can work on slices of any type that we can compare.  We don’t need to bring PartialOrd into scope because it’s in the prelude.  Change the signature of largest to look like this:  fn largest&lt;T: PartialOrd&gt;(list: &amp;[T]) -&gt; T {  This time when we compile the code, we get a different set of errors:  error[E0508]: cannot move out of type `[T]`, a non-copy slice  --&gt; src/main.rs:2:23   | 2 |     let mut largest = list[0];   |                       ^^^^^^^   |                       |   |                       cannot move out of here   |                       help: consider using a reference instead: `&amp;list[0]`  error[E0507]: cannot move out of borrowed content  --&gt; src/main.rs:4:9   | 4 |     for &amp;item in list.iter() {   |         ^----   |         ||   |         |hint: to prevent move, use `ref item` or `ref mut item`   |         cannot move out of borrowed content  The key line in this error is cannot move out of type [T], a non-copy slice.  With our non-generic versions of the largest function, we were only trying to find the largest i32 or char.  As discussed in the “Stack-Only Data: Copy” section in Chapter 4, types like i32 and char that have a known size can be stored on the stack, so they implement the Copy trait.  But when we made the largest function generic, it became possible for the list parameter to have types in it that don’t implement the Copy trait.  Consequently, we wouldn’t be able to move the value out of list[0] and into the largest variable, resulting in this error.   To call this code with only those types that implement the Copy trait, we can add Copy to the trait bounds of T!  Listing 10-15 shows the complete code of a generic largest function that will compile as long as the types of the values in the slice that we pass into the function implement the PartialOrd and Copy traits, like i32 and char do.  fn largest&lt;T: PartialOrd + Copy&gt;(list: &amp;[T]) -&gt; T {     let mut largest = list[0];      for &amp;item in list.iter() {         if item &gt; largest {             largest = item;         }     }      largest }  fn main() {     let number_list = vec![34, 50, 25, 100, 65];      let result = largest(&amp;number_list);     println!(\"The largest number is {}\", result);      let char_list = vec!['y', 'm', 'a', 'q'];      let result = largest(&amp;char_list);     println!(\"The largest char is {}\", result); }  Listing 10-15: A working definition of the largest function that works on any generic type that implements the PartialOrd and Copy traits   If we don’t want to restrict the largest function to the types that implement the Copy trait, we could specify that T has the trait bound Clone instead of Copy.  Then we could clone each value in the slice when we want the largest function to have ownership.  Using the clone function means we’re potentially making more heap allocations in the case of types that own heap data like String, and heap allocations can be slow if we’re working with large amounts of data.   Another way we could implement largest is for the function to return a reference to a T value in the slice.  If we change the return type to &amp;T instead of T, thereby changing the body of the function to return a reference, we wouldn’t need the Clone or Copy trait bounds and we could avoid heap allocations.  Try implementing these alternate solutions on your own!   Using Trait Bounds to Conditionally Implement Methods  By using a trait bound with an impl block that uses generic type parameters, we can implement methods conditionally for types that implement the specified traits.  For example, the type Pair&lt;T&gt; in Listing 10-16 always implements the new function.  But Pair&lt;T&gt; only implements the cmp_display method if its inner type T implements the PartialOrd trait that enables comparison and the Display trait that enables printing.   #![allow(unused_variables)] fn main() {   use std::fmt::Display;    struct Pair&lt;T&gt; {       x: T,       y: T,   }    impl&lt;T&gt; Pair&lt;T&gt; {       fn new(x: T, y: T) -&gt; Self {           Self {               x,               y,           }       }   }    impl&lt;T: Display + PartialOrd&gt; Pair&lt;T&gt; {       fn cmp_display(&amp;self) {           if self.x &gt;= self.y {               println!(\"The largest member is x = {}\", self.x);           } else {               println!(\"The largest member is y = {}\", self.y);           }       }   } }  Listing 10-16: Conditionally implement methods on a generic type depending on trait bounds  We can also conditionally implement a trait for any type that implements another trait.  Implementations of a trait on any type that satisfies the trait bounds are called blanket implementations and are extensively used in the Rust standard library.  For example, the standard library implements the ToString trait on any type that implements the Display trait.  The impl block in the standard library looks similar to this code:  impl&lt;T: Display&gt; ToString for T {     // --snip-- }  Because the standard library has this blanket implementation, we can call the to_string method defined by the ToString trait on any type that implements the Display trait.  For example, we can turn integers into their corresponding String values like this because integers implement Display:  let s = 3.to_string();  Blanket implementations appear in the documentation for the trait in the “Implementors” section.   Traits and trait bounds let us write code that uses generic type parameters to reduce duplication but also specify to the compiler that we want the generic type to have particular behavior.  The compiler can then use the trait bound information to check that all the concrete types used with our code provide the correct behavior.  In dynamically typed languages, we would get an error at runtime if we called a method on a type which didn’t implement the type which defines the method.  But Rust moves these errors to compile time so we’re forced to fix the problems before our code is even able to run.  Additionally, we don’t have to write code that checks for behavior at runtime because we’ve already checked at compile time.  Doing so improves performance without having to give up the flexibility of generics.   Another kind of generic that we’ve already been using is called lifetimes.  Rather than ensuring that a type has the behavior we want, lifetimes ensure that references are valid as long as we need them to be.  Let’s look at how lifetimes do that.   Validating References with Lifetimes  One detail we didn’t discuss in the “References and Borrowing” section in Chapter 4 is that every reference in Rust has a lifetime, which is the scope for which that reference is valid.  Most of the time, lifetimes are implicit and inferred, just like most of the time, types are inferred.  We must annotate types when multiple types are possible.  In a similar way, we must annotate lifetimes when the lifetimes of references could be related in a few different ways.  Rust requires us to annotate the relationships using generic lifetime parameters to ensure the actual references used at runtime will definitely be valid.   The concept of lifetimes is somewhat different from tools in other programming languages, arguably making lifetimes Rust’s most distinctive feature.  Although we won’t cover lifetimes in their entirety in this chapter, we’ll discuss common ways you might encounter lifetime syntax so you can become familiar with the concepts.   Preventing Dangling References with Lifetimes  The main aim of lifetimes is to prevent dangling references, which cause a program to reference data other than the data it’s intended to reference.  Consider the program in Listing 10-17, which has an outer scope and an inner scope.  {     let r;      {         let x = 5;         r = &amp;x;     }      println!(\"r: {}\", r); }  Listing 10-17: An attempt to use a reference whose value has gone out of scope   Note: The examples in Listings 10-17, 10-18, and 10-24 declare variables without giving them an initial value, so the variable name exists in the outer scope. At first glance, this might appear to be in conflict with Rust’s having no null values. However, if we try to use a variable before giving it a value, we’ll get a compile-time error, which shows that Rust indeed does not allow null values.   The outer scope declares a variable named r with no initial value, and the inner scope declares a variable named x with the initial value of 5.  Inside the inner scope, we attempt to set the value of r as a reference to x.  Then the inner scope ends, and we attempt to print the value in r.  This code won’t compile because the value r is referring to has gone out of scope before we try to use it.  Here is the error message:  error[E0597]: `x` does not live long enough   --&gt; src/main.rs:7:5    | 6  |         r = &amp;x;    |              - borrow occurs here 7  |     }    |     ^ `x` dropped here while still borrowed ... 10 | }    | - borrowed value needs to live until here  The variable x doesn’t “live long enough.”  The reason is that x will be out of scope when the inner scope ends on line 7.  But r is still valid for the outer scope; because its scope is larger, we say that it “lives longer.”  If Rust allowed this code to work, r would be referencing memory that was deallocated when x went out of scope, and anything we tried to do with r wouldn’t work correctly.  So how does Rust determine that this code is invalid? It uses a borrow checker.   The Borrow Checker  The Rust compiler has a borrow checker that compares scopes to determine whether all borrows are valid.  Listing 10-18 shows the same code as Listing 10-17 but with annotations showing the lifetimes of the variables.  {     let r;                // ---------+-- 'a                           //          |     {                     //          |         let x = 5;        // -+-- 'b  |         r = &amp;x;           //  |       |     }                     // -+       |                           //          |     println!(\"r: {}\", r); //          | }                         // ---------+  Listing 10-18: Annotations of the lifetimes of r and x, named ‘a and ‘b, respectively   Here, we’ve annotated the lifetime of r with 'a and the lifetime of x with 'b.  As you can see, the inner 'b block is much smaller than the outer 'a lifetime block.  At compile time, Rust compares the size of the two lifetimes and sees that r has a lifetime of 'a but that it refers to memory with a lifetime of 'b.  The program is rejected because ‘b is shorter than ‘a: the subject of the reference doesn’t live as long as the reference.   Listing 10-19 fixes the code so it doesn’t have a dangling reference and compiles without any errors.  {     let x = 5;            // ----------+-- 'b                           //           |     let r = &amp;x;           // --+-- 'a  |                           //   |       |     println!(\"r: {}\", r); //   |       |                           // --+       | }                         // ----------+  Listing 10-19: A valid reference because the data has a longer lifetime than the reference   Here, x has the lifetime 'b, which in this case is larger than 'a. This means r can reference x because Rust knows that the reference in r will always be valid while x is valid.   Now that you know where the lifetimes of references are and how Rust analyzes lifetimes to ensure references will always be valid, let’s explore generic lifetimes of parameters and return values in the context of functions.   Generic Lifetimes in Functions  Let’s write a function that returns the longer of two string slices.  This function will take two string slices and return a string slice.  After we’ve implemented the longest function, the code in Listing 10-20 should print The longest string is abcd.   fn main() {     let string1 = String::from(\"abcd\");     let string2 = \"xyz\";      let result = longest(string1.as_str(), string2);     println!(\"The longest string is {}\", result); }  Listing 10-20: A main function that calls the longest function to find the longer of two string slices   Note that we want the function to take string slices, which are references, because we don’t want the longest function to take ownership of its parameters.  We want to allow the function to accept slices of a String (the type stored in the variable string1) as well as string literals (which is what variable string2 contains).   Refer to the “String Slices as Parameters” section in Chapter 4 for more discussion about why the parameters we use in Listing 10-20 are the ones we want.   If we try to implement the longest function as shown in Listing 10-21, it won’t compile.   fn longest(x: &amp;str, y: &amp;str) -&gt; &amp;str {     if x.len() &gt; y.len() {         x     } else {         y     } }  Listing 10-21: An implementation of the longest function that returns the longer of two string slices but does not yet compile   Instead, we get the following error that talks about lifetimes:   error[E0106]: missing lifetime specifier  --&gt; src/main.rs:1:33   | 1 | fn longest(x: &amp;str, y: &amp;str) -&gt; &amp;str {   |                                 ^ expected lifetime parameter   |   = help: this function's return type contains a borrowed value, but the signature does not say whether it is borrowed from `x` or `y`   The help text reveals that the return type needs a generic lifetime parameter on it because Rust can’t tell whether the reference being returned refers to x or y.  Actually, we don’t know either, because the if block in the body of this function returns a reference to x and the else block returns a reference to y!   When we’re defining this function, we don’t know the concrete values that will be passed into this function, so we don’t know whether the if case or the else case will execute.  We also don’t know the concrete lifetimes of the references that will be passed in, so we can’t look at the scopes as we did in Listings 10-18 and 10-19 to determine whether the reference we return will always be valid.  The borrow checker can’t determine this either, because it doesn’t know how the lifetimes of x and y relate to the lifetime of the return value.  To fix this error, we’ll add generic lifetime parameters that define the relationship between the references so the borrow checker can perform its analysis.   Lifetime Annotation Syntax  Lifetime annotations don’t change how long any of the references live.  Just as functions can accept any type when the signature specifies a generic type parameter, functions can accept references with any lifetime by specifying a generic lifetime parameter.  Lifetime annotations describe the relationships of the lifetimes of multiple references to each other without affecting the lifetimes.   Lifetime annotations have a slightly unusual syntax: the names of lifetime parameters must start with an apostrophe (') and are usually all lowercase and very short, like generic types.  Most people use the name 'a.  We place lifetime parameter annotations after the &amp; of a reference, using a space to separate the annotation from the reference’s type.   Here are some examples: a reference to an i32 without a lifetime parameter, a reference to an i32 that has a lifetime parameter named 'a, and a mutable reference to an i32 that also has the lifetime 'a.  &amp;i32        // a reference &amp;'a i32     // a reference with an explicit lifetime &amp;'a mut i32 // a mutable reference with an explicit lifetime  One lifetime annotation by itself doesn’t have much meaning, because the annotations are meant to tell Rust how generic lifetime parameters of multiple references relate to each other.  For example, let’s say we have a function with the parameter first that is a reference to an i32 with lifetime 'a.  The function also has another parameter named second that is another reference to an i32 that also has the lifetime 'a.  The lifetime annotations indicate that the references first and second must both live as long as that generic lifetime.   Lifetime Annotations in Function Signatures  Now let’s examine lifetime annotations in the context of the longest function.  As with generic type parameters, we need to declare generic lifetime parameters inside angle brackets between the function name and the parameter list.  The constraint we want to express in this signature is that all the references in the parameters and the return value must have the same lifetime.  We’ll name the lifetime 'a and then add it to each reference, as shown in Listing 10-22.   fn longest&lt;'a&gt;(x: &amp;'a str, y: &amp;'a str) -&gt; &amp;'a str {     if x.len() &gt; y.len() {         x     } else {         y     } }  Listing 10-22: The longest function definition specifying that all the references in the signature must have the same lifetime ‘a   This code should compile and produce the result we want when we use it with the main function in Listing 10-20.   The function signature now tells Rust that for some lifetime 'a, the function takes two parameters, both of which are string slices that live at least as long as lifetime 'a.  The function signature also tells Rust that the string slice returned from the function will live at least as long as lifetime 'a.  In practice, it means that the lifetime of the reference returned by the longest function is the same as the smaller of the lifetimes of the references passed in.  These constraints are what we want Rust to enforce.  Remember, when we specify the lifetime parameters in this function signature, we’re not changing the lifetimes of any values passed in or returned.  Rather, we’re specifying that the borrow checker should reject any values that don’t adhere to these constraints.  Note that the longest function doesn’t need to know exactly how long x and y will live, only that some scope can be substituted for 'a that will satisfy this signature.   When annotating lifetimes in functions, the annotations go in the function signature, not in the function body.  Rust can analyze the code within the function without any help.  However, when a function has references to or from code outside that function, it becomes almost impossible for Rust to figure out the lifetimes of the parameters or return values on its own.  The lifetimes might be different each time the function is called. This is why we need to annotate the lifetimes manually.   When we pass concrete references to longest, the concrete lifetime that is substituted for 'a is the part of the scope of x that overlaps with the scope of y.  In other words, the generic lifetime 'a will get the concrete lifetime that is equal to the smaller of the lifetimes of x and y.  Because we’ve annotated the returned reference with the same lifetime parameter 'a, the returned reference will also be valid for the length of the smaller of the lifetimes of x and y.   Let’s look at how the lifetime annotations restrict the longest function by passing in references that have different concrete lifetimes. Listing 10-23 is a straightforward example.  fn main() {     let string1 = String::from(\"long string is long\");      {         let string2 = String::from(\"xyz\");         let result = longest(string1.as_str(), string2.as_str());         println!(\"The longest string is {}\", result);     } }  Listing 10-23: Using the longest function with references to String values that have different concrete lifetimes   In this example, string1 is valid until the end of the outer scope, string2 is valid until the end of the inner scope, and result references something that is valid until the end of the inner scope.  Run this code, and you’ll see that the borrow checker approves of this code; it will compile and print The longest string is long string is long.   Next, let’s try an example that shows that the lifetime of the reference in result must be the smaller lifetime of the two arguments.  We’ll move the declaration of the result variable outside the inner scope but leave the assignment of the value to the result variable inside the scope with string2.  Then we’ll move the println! that uses result outside the inner scope, after the inner scope has ended.  The code in Listing 10-24 will not compile.   fn main() {     let string1 = String::from(\"long string is long\");     let result;     {         let string2 = String::from(\"xyz\");         result = longest(string1.as_str(), string2.as_str());     }     println!(\"The longest string is {}\", result); }  Listing 10-24: Attempting to use result after string2 has gone out of scope   When we try to compile this code, we’ll get this error:   error[E0597]: `string2` does not live long enough   --&gt; src/main.rs:15:5    | 14 |         result = longest(string1.as_str(), string2.as_str());    |                                            ------- borrow occurs here 15 |     }    |     ^ `string2` dropped here while still borrowed 16 |     println!(\"The longest string is {}\", result); 17 | }    | - borrowed value needs to live until here  The error shows that for result to be valid for the println! statement, string2 would need to be valid until the end of the outer scope.  Rust knows this because we annotated the lifetimes of the function parameters and return values using the same lifetime parameter 'a.   As humans, we can look at this code and see that string1 is longer than string2 and therefore result will contain a reference to string1.  Because string1 has not gone out of scope yet, a reference to string1 will still be valid for the println! statement.  However, the compiler can’t see that the reference is valid in this case.  We’ve told Rust that the lifetime of the reference returned by the longest function is the same as the smaller of the lifetimes of the references passed in.  Therefore, the borrow checker disallows the code in Listing 10-24 as possibly having an invalid reference.   Try designing more experiments that vary the values and lifetimes of the references passed in to the longest function and how the returned reference is used.  Make hypotheses about whether or not your experiments will pass the borrow checker before you compile; then check to see if you’re right!   Thinking in Terms of Lifetimes  The way in which you need to specify lifetime parameters depends on what your function is doing.  For example, if we changed the implementation of the longest function to always return the first parameter rather than the longest string slice, we wouldn’t need to specify a lifetime on the y parameter.  The following code will compile:  fn longest&lt;'a&gt;(x: &amp;'a str, y: &amp;str) -&gt; &amp;'a str {     x }  In this example, we’ve specified a lifetime parameter 'a for the parameter x and the return type, but not for the parameter y, because the lifetime of y does not have any relationship with the lifetime of x or the return value.   When returning a reference from a function, the lifetime parameter for the return type needs to match the lifetime parameter for one of the parameters.  If the reference returned does not refer to one of the parameters, it must refer to a value created within this function, which would be a dangling reference because the value will go out of scope at the end of the function.  Consider this attempted implementation of the longest function that won’t compile:  fn longest&lt;'a&gt;(x: &amp;str, y: &amp;str) -&gt; &amp;'a str {     let result = String::from(\"really long string\");     result.as_str() }  Here, even though we’ve specified a lifetime parameter 'a for the return type, this implementation will fail to compile because the return value lifetime is not related to the lifetime of the parameters at all.  Here is the error message we get:   error[E0597]: `result` does not live long enough  --&gt; src/main.rs:3:5   | 3 |     result.as_str()   |     ^^^^^^ does not live long enough 4 | }   | - borrowed value only lives until here   | note: borrowed value must be valid for the lifetime 'a as defined on the function body at 1:1...  --&gt; src/main.rs:1:1   | 1 | / fn longest&lt;'a&gt;(x: &amp;str, y: &amp;str) -&gt; &amp;'a str { 2 | |     let result = String::from(\"really long string\"); 3 | |     result.as_str() 4 | | }   | |_^  The problem is that result goes out of scope and gets cleaned up at the end of the longest function.  We’re also trying to return a reference to result from the function.  There is no way we can specify lifetime parameters that would change the dangling reference, and Rust won’t let us create a dangling reference.  In this case, the best fix would be to return an owned data type rather than a reference so the calling function is then responsible for cleaning up the value.   Ultimately, lifetime syntax is about connecting the lifetimes of various parameters and return values of functions.  Once they’re connected, Rust has enough information to allow memory-safe operations and disallow operations that would create dangling pointers or otherwise violate memory safety.   Lifetime Annotations in Struct Definitions  So far, we’ve only defined structs to hold owned types.  It’s possible for structs to hold references, but in that case we would need to add a lifetime annotation on every reference in the struct’s definition.  Listing 10-25 has a struct named ImportantExcerpt that holds a string slice.  struct ImportantExcerpt&lt;'a&gt; {     part: &amp;'a str, }  fn main() {     let novel = String::from(\"Call me Ishmael. Some years ago...\");     let first_sentence = novel.split('.')         .next()         .expect(\"Could not find a '.'\");     let i = ImportantExcerpt { part: first_sentence }; }  Listing 10-25: A struct that holds a reference, so its definition needs a lifetime annotation   This struct has one field, part, that holds a string slice, which is a reference.  As with generic data types, we declare the name of the generic lifetime parameter inside angle brackets after the name of the struct so we can use the lifetime parameter in the body of the struct definition.  This annotation means an instance of ImportantExcerpt can’t outlive the reference it holds in its part field.   The main function here creates an instance of the ImportantExcerpt struct that holds a reference to the first sentence of the String owned by the variable novel.  The data in novel exists before the ImportantExcerpt instance is created.  In addition, novel doesn’t go out of scope until after the ImportantExcerpt goes out of scope, so the reference in the ImportantExcerpt instance is valid.   Lifetime Elision  You’ve learned that every reference has a lifetime and that you need to specify lifetime parameters for functions or structs that use references.  However, in Chapter 4 we had a function in Listing 4-9, which is shown again in Listing 10-26, that compiled without lifetime annotations.  fn first_word(s: &amp;str) -&gt; &amp;str {     let bytes = s.as_bytes();      for (i, &amp;item) in bytes.iter().enumerate() {         if item == b' ' {             return &amp;s[0..i];         }     }      &amp;s[..] }  Listing 10-26: A function we defined in Listing 4-9 that compiled without lifetime annotations, even though the parameter and return type are references   The reason this function compiles without lifetime annotations is historical: in early versions (pre-1.0) of Rust, this code wouldn’t have compiled because every reference needed an explicit lifetime.  At that time, the function signature would have been written like this:   fn first_word&lt;'a&gt;(s: &amp;'a str) -&gt; &amp;'a str {  After writing a lot of Rust code, the Rust team found that Rust programmers were entering the same lifetime annotations over and over in particular situations.  These situations were predictable and followed a few deterministic patterns.  The developers programmed these patterns into the compiler’s code so the borrow checker could infer the lifetimes in these situations and wouldn’t need explicit annotations.   This piece of Rust history is relevant because it’s possible that more deterministic patterns will emerge and be added to the compiler. In the future, even fewer lifetime annotations might be required.   The patterns programmed into Rust’s analysis of references are called the lifetime elision rules.  These aren’t rules for programmers to follow; they’re a set of particular cases that the compiler will consider, and if your code fits these cases, you don’t need to write the lifetimes explicitly.   The elision rules don’t provide full inference.  If Rust deterministically applies the rules but there is still ambiguity as to what lifetimes the references have, the compiler won’t guess what the lifetime of the remaining references should be.  In this case, instead of guessing, the compiler will give you an error that you can resolve by adding the lifetime annotations that specify how the references relate to each other.   Lifetimes on function or method parameters are called input lifetimes, and lifetimes on return values are called output lifetimes.   The compiler uses three rules to figure out what lifetimes references have when there aren’t explicit annotations.  The first rule applies to input lifetimes, and the second and third rules apply to output lifetimes.  If the compiler gets to the end of the three rules and there are still references for which it can’t figure out lifetimes, the compiler will stop with an error. These rules apply to fn definitions as well as impl blocks.   The first rule is that each parameter that is a reference gets its own lifetime parameter.  In other words, a function with one parameter gets one lifetime parameter: fn foo&lt;'a&gt;(x: &amp;'a i32); a function with two parameters gets two separate lifetime parameters: fn foo&lt;'a, 'b&gt;(x: &amp;'a i32, y: &amp;'b i32); and so on.   The second rule is if there is exactly one input lifetime parameter, that lifetime is assigned to all output lifetime parameters: fn foo&lt;'a&gt;(x: &amp;'a i32) -&gt; &amp;'a i32.   The third rule is if there are multiple input lifetime parameters, but one of them is &amp;self or &amp;mut self because this is a method, the lifetime of self is assigned to all output lifetime parameters.  This third rule makes methods much nicer to read and write because fewer symbols are necessary.   Let’s pretend we’re the compiler.  We’ll apply these rules to figure out what the lifetimes of the references in the signature of the first_word function in Listing 10-26 are.  The signature starts without any lifetimes associated with the references:   fn first_word(s: &amp;str) -&gt; &amp;str {   Then the compiler applies the first rule, which specifies that each parameter gets its own lifetime.  We’ll call it 'a as usual, so now the signature is this:  fn first_word&lt;'a&gt;(s: &amp;'a str) -&gt; &amp;str {   The second rule applies because there is exactly one input lifetime.  The second rule specifies that the lifetime of the one input parameter gets assigned to the output lifetime, so the signature is now this:  fn first_word&lt;'a&gt;(s: &amp;'a str) -&gt; &amp;'a str {   Now all the references in this function signature have lifetimes, and the compiler can continue its analysis without needing the programmer to annotate the lifetimes in this function signature.   Let’s look at another example, this time using the longest function that had no lifetime parameters when we started working with it in Listing 10-21:  fn longest(x: &amp;str, y: &amp;str) -&gt; &amp;str {  Let’s apply the first rule: each parameter gets its own lifetime.  This time we have two parameters instead of one, so we have two lifetimes:  fn longest&lt;'a, 'b&gt;(x: &amp;'a str, y: &amp;'b str) -&gt; &amp;str {  You can see that the second rule doesn’t apply because there is more than one input lifetime.  The third rule doesn’t apply either, because longest is a function rather than a method, so none of the parameters are self.  After working through all three rules, we still haven’t figured out what the return type’s lifetime is.  This is why we got an error trying to compile the code in Listing 10-21: the compiler worked through the lifetime elision rules but still couldn’t figure out all the lifetimes of the references in the signature.   Because the third rule really only applies in method signatures, we’ll look at lifetimes in that context next to see why the third rule means we don’t have to annotate lifetimes in method signatures very often.   Lifetime Annotations in Method Definitions  When we implement methods on a struct with lifetimes, we use the same syntax as that of generic type parameters shown in Listing 10-11.  Where we declare and use the lifetime parameters depends on whether they’re related to the struct fields or the method parameters and return values.   Lifetime names for struct fields always need to be declared after the impl keyword and then used after the struct’s name, because those lifetimes are part of the struct’s type.   In method signatures inside the impl block, references might be tied to the lifetime of references in the struct’s fields, or they might be independent.  In addition, the lifetime elision rules often make it so that lifetime annotations aren’t necessary in method signatures.  Let’s look at some examples using the struct named ImportantExcerpt that we defined in Listing 10-25.   First, we’ll use a method named level whose only parameter is a reference to self and whose return value is an i32, which is not a reference to anything:  impl&lt;'a&gt; ImportantExcerpt&lt;'a&gt; {     fn level(&amp;self) -&gt; i32 {         3     } }  The lifetime parameter declaration after impl and its use after the type name are required, but we’re not required to annotate the lifetime of the reference to self because of the first elision rule.   Here is an example where the third lifetime elision rule applies:   impl&lt;'a&gt; ImportantExcerpt&lt;'a&gt; {     fn announce_and_return_part(&amp;self, announcement: &amp;str) -&gt; &amp;str {         println!(\"Attention please: {}\", announcement);         self.part     } }  There are two input lifetimes, so Rust applies the first lifetime elision rule and gives both &amp;self and announcement their own lifetimes.  Then, because one of the parameters is &amp;self, the return type gets the lifetime of &amp;self, and all lifetimes have been accounted for.   The Static Lifetime  One special lifetime we need to discuss is 'static, which means that this reference can live for the entire duration of the program.  All string literals have the 'static lifetime, which we can annotate as follows:  let s: &amp;'static str = \"I have a static lifetime.\";  The text of this string is stored directly in the program’s binary, which is always available.  Therefore, the lifetime of all string literals is 'static.   You might see suggestions to use the 'static lifetime in error messages.  But before specifying 'static as the lifetime for a reference, think about whether the reference you have actually lives the entire lifetime of your program or not.  You might consider whether you want it to live that long, even if it could.  Most of the time, the problem results from attempting to create a dangling reference or a mismatch of the available lifetimes.  In such cases, the solution is fixing those problems, not specifying the 'static lifetime.   Generic Type Parameters, Trait Bounds, and Lifetimes Together  Let’s briefly look at the syntax of specifying generic type parameters, trait bounds, and lifetimes all in one function!  use std::fmt::Display;  fn longest_with_an_announcement&lt;'a, T&gt;(x: &amp;'a str, y: &amp;'a str, ann: T) -&gt; &amp;'a str     where T: Display {     println!(\"Announcement! {}\", ann);     if x.len() &gt; y.len() {         x     } else {         y     } }  This is the longest function from Listing 10-22 that returns the longer of two string slices.  But now it has an extra parameter named ann of the generic type T, which can be filled in by any type that implements the Display trait as specified by the where clause.  This extra parameter will be printed before the function compares the lengths of the string slices, which is why the Display trait bound is necessary.  Because lifetimes are a type of generic, the declarations of the lifetime parameter 'a and the generic type parameter T go in the same list inside the angle brackets after the function name.   Summary  We covered a lot in this chapter! Now that you know about generic type parameters, traits and trait bounds, and generic lifetime parameters, you’re ready to write code without repetition that works in many different situations.   Generic type parameters let you apply the code to different types.  Traits and trait bounds ensure that even though the types are generic, they’ll have the behavior the code needs.  You learned how to use lifetime annotations to ensure that this flexible code won’t have any dangling references.  And all of this analysis happens at compile time, which doesn’t affect runtime performance!  ","categories": ["RUST Language"],
        "tags": ["Generic","Traits","Lifetimes","Template"],
        "url": "https://jjungs-lee.github.io//rust/10.Generic-Types,-Traits,-and-Lifetimes",
        "teaser":null},{
        "title": "Microsoft: We're creating a new Rust-like programming language for secure coding",
        "excerpt":"This link will be help :)     KimPopeTV   Article - Zdnet By Liam Tung     Microsoft can’t just throw away older Windows code(C or C++), but the company’s Project Verona aims to make older low-level components in Windows 10 more secure by integrating Mozilla-developed Rust.   Microsofte recently revealed that its trials with Rust over C and C++ to remove insecure code from Windows had hit its targets. The company has partially explained its security-related motives for experimenting with Rust, but hasn’t gone into much detail about the broader reasons for its move.   Microsoft recently revealed that the vast majority of bugs being discovered these days are memory safety flaws, which is also why Microsoft is looking at Rust to improve the situation. Rust was designed to allow developers to code without having to worry about this class of bug. (this mean Rust is good for prenventing memory safety flaws)   Memory safety is the term for coding frameworks that help protect memory space from being abused by malware. Project Verona at Microsoft is meant to progress the company’s work here to close off this attack vector.  The other class of bugs Microsoft is working on to address relates to uninitialized memory in a way that also doesn’t kill performance.   MS project Verona will have a lot of influence on the development of windows, and it will be helpful to the development of rust. I wonder why Microsoft doesn’t participate in rust open source. If Microsoft and other companies are involved in opensource development, it could be a better language than c or C ++.  But Microsofte will be made open souce “soon”(Verona), It is a new language for what Microsoft is calling “safe infrastructure programming”. (I think Rust+MS skill combined)   So the possibilities of developing rust or verona are endless, and I think it will be a better language to solve problems that existed in C and C ++.  ","categories": ["Article"],
        "tags": ["RUST","Microsoft","Verona"],
        "url": "https://jjungs-lee.github.io//article/ms-creating-a-new-Rust-like-programming-language/",
        "teaser":null},{
        "title": "RUST : 11. Writing Automated Tests",
        "excerpt":"In his 1972 essay “The Humble Programmer,” Edsger W. Dijkstra said that “Program testing can be a very effective way to show the presence of bugs,  but it is hopelessly inadequate for showing their absence.” That doesn’t mean we shouldn’t try to test as much as we can!   Correctness in our programs is the extent to which our code does what we intend it to do.  Rust is designed with a high degree of concern about the correctness of programs, but correctness is complex and not easy to prove.  Rust’s type system shoulders a huge part of this burden, but the type system cannot catch every kind of incorrectness.  As such, Rust includes support for writing automated software tests within the language.   As an example, say we write a function called add_two that adds 2 to whatever number is passed to it.  This function’s signature accepts an integer as a parameter and returns an integer as a result.  When we implement and compile that function, Rust does all the type checking and borrow checking that you’ve learned so far to ensure that,  for instance, we aren’t passing a String value or an invalid reference to this function.  But Rust can’t check that this function will do precisely what we intend, which is return the parameter plus 2 rather than, say, the parameter plus 10 or the parameter minus 50! That’s where tests come in.   We can write tests that assert, for example, that when we pass 3 to the add_two function, the returned value is 5.  We can run these tests whenever we make changes to our code to make sure any existing correct behavior has not changed.   Testing is a complex skill: although we can’t cover every detail about how to write good tests in one chapter, we’ll discuss the mechanics of Rust’s testing facilities. We’ll talk about the annotations and macros available to you when writing your tests, the default behavior and options provided for running your tests, and how to organize tests into unit tests and integration tests.   How to Write Tests  Tests are Rust functions that verify that the non-test code is functioning in the expected manner.  The bodies of test functions typically perform these three actions:      Set up any needed data or state.   Run the code you want to test.   Assert the results are what you expect. Let’s look at the features Rust provides specifically for writing tests that take these actions, which include the test attribute, a few macros, and the should_panic attribute.   The Anatomy of a Test Function  At its simplest, a test in Rust is a function that’s annotated with the test attribute.  Attributes are metadata about pieces of Rust code; one example is the derive attribute we used with structs in Chapter 5.  To change a function into a test function, add #[test] on the line before fn.  When you run your tests with the cargo test command, Rust builds a test runner binary that runs the functions annotated with the test attribute and reports on whether each test function passes or fails.   When we make a new library project with Cargo, a test module with a test function in it is automatically generated for us.  This module helps you start writing your tests so you don’t have to look up the exact structure and syntax of test functions every time you start a new project.  You can add as many additional test functions and as many test modules as you want!   We’ll explore some aspects of how tests work by experimenting with the template test generated for us without actually testing any code.  Then we’ll write some real-world tests that call some code that we’ve written and assert that its behavior is correct.   Let’s create a new library project called adder:  $ cargo new adder --lib      Created library `adder` project $ cd adder  The contents of the src/lib.rs file in your adder library should look like Listing 11-1.  // Filename: src/lib.rs #[cfg(test)] mod tests {     #[test]     fn it_works() {         assert_eq!(2 + 2, 4);     } }  Listing 11-1: The test module and function generated automatically by cargo new   For now, let’s ignore the top two lines and focus on the function to see how it works.  Note the #[test] annotation before the fn line: this attribute indicates this is a test function, so the test runner knows to treat this function as a test.  We could also have non-test functions in the tests module to help set up common scenarios or perform common operations, so we need to indicate which functions are tests by using the #[test] attribute.   The function body uses the assert_eq! macro to assert that 2 + 2 equals 4.  This assertion serves as an example of the format for a typical test. Let’s run it to see that this test passes.   The cargo test command runs all tests in our project, as shown in Listing 11-2.   $ cargo test    Compiling adder v0.1.0 (file:///projects/adder)     Finished dev [unoptimized + debuginfo] target(s) in 0.22 secs      Running target/debug/deps/adder-ce99bcc2479f4607  running 1 test test tests::it_works ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out     Doc-tests adder  running 0 tests  test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  Listing 11-2: The output from running the automatically generated test   Cargo compiled and ran the test.  After the Compiling, Finished, and Running lines is the line running 1 test.  The next line shows the name of the generated test function, called it_works, and the result of running that test, ok.  The overall summary of running the tests appears next.  The text test result: ok. means that all the tests passed, and the portion that reads 1 passed; 0 failed totals the number of tests that passed or failed.   Because we don’t have any tests we’ve marked as ignored, the summary shows 0 ignored.  We also haven’t filtered the tests being run, so the end of the summary shows 0 filtered out.  We’ll talk about ignoring and filtering out tests in the next section, “Controlling How Tests Are Run.”   The next part of the test output, which starts with Doc-tests adder, is for the results of any documentation tests.  We don’t have any documentation tests yet, but Rust can compile any code examples that appear in our API documentation.  This feature helps us keep our docs and our code in sync! We’ll discuss how to write documentation tests in the “Documentation Comments as Tests” section of Chapter 14.  For now, we’ll ignore the Doc-tests output.   Let’s change the name of our test to see how that changes the test output.  Change the it_works function to a different name, such as exploration, like so:  #[cfg(test)] mod tests {     #[test]     fn exploration() {         assert_eq!(2 + 2, 4);     } }  Then run cargo test again. The output now shows exploration instead of it_works:  running 1 test test tests::exploration ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out   Let’s add another test, but this time we’ll make a test that fails!  Tests fail when something in the test function panics.  Each test is run in a new thread, and when the main thread sees that a test thread has died, the test is marked as failed.  We talked about the simplest way to cause a panic in Chapter 9, which is to call the panic! macro.  Enter the new test, another, so your src/lib.rs file looks like Listing 11-3.   #[cfg(test)] mod tests {     #[test]     fn exploration() {         assert_eq!(2 + 2, 4);     }      #[test]     fn another() {         panic!(\"Make this test fail\");     }  Listing 11-3: Adding a second test that will fail because we call the panic! macro   Run the tests again using cargo test. The output should look like Listing 11-4, which shows that our exploration test passed and another failed.  running 2 tests test tests::exploration ... ok test tests::another ... FAILED  failures:  ---- tests::another stdout ---- thread 'tests::another' panicked at 'Make this test fail', src/lib.rs:10:9 note: Run with `RUST_BACKTRACE=1` for a backtrace.  failures:     tests::another  test result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  error: test failed  Listing 11-4: Test results when one test passes and one test fails   Instead of ok, the line test tests::another shows FAILED.  Two new sections appear between the individual results and the summary:  the first section displays the detailed reason for each test failure.  In this case, another failed because it panicked at 'Make this test fail', which happened on line 10 in the src/lib.rs file.  The next section lists just the names of all the failing tests, which is useful when there are lots of tests and lots of detailed failing test output.  We can use the name of a failing test to run just that test to more easily debug it;  we’ll talk more about ways to run tests in the “Controlling How Tests Are Run” section.   Checking Results with the assert! Macro  The assert! macro, provided by the standard library, is useful when you want to ensure that some condition in a test evaluates to true.  We give the assert! macro an argument that evaluates to a Boolean.  If the value is true, assert! does nothing and the test passes.  If the value is false, the assert! macro calls the panic! macro, which causes the test to fail.  Using the assert! macro helps us check that our code is functioning in the way we intend.   In Chapter 5, Listing 5-15, we used a Rectangle struct and a can_hold method, which are repeated here in Listing 11-5.  Let’s put this code in the src/lib.rs file and write some tests for it using the assert! macro.   #[derive(Debug)] struct Rectangle {     width: u32,     height: u32, }  impl Rectangle {     fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool {         self.width &gt; other.width &amp;&amp; self.height &gt; other.height     } }  Listing 11-5: Using the Rectangle struct and its can_hold method from Chapter 5   The can_hold method returns a Boolean, which means it’s a perfect use case for the assert! macro.  In Listing 11-6, we write a test that exercises the can_hold method by creating a Rectangle instance that has a width of 8 and a height of 7 and asserting that it can hold another Rectangle instance that has a width of 5 and a height of 1.   #[cfg(test)] mod tests {     use super::*;      #[test]     fn larger_can_hold_smaller() {         let larger = Rectangle { width: 8, height: 7 };         let smaller = Rectangle { width: 5, height: 1 };          assert!(larger.can_hold(&amp;smaller));     } }  Listing 11-6: A test for can_hold that checks whether a larger rectangle can indeed hold a smaller rectangle   Note that we’ve added a new line inside the tests module: use super::*;.  The tests module is a regular module that follows the usual visibility rules we covered in Chapter 7 in the “Paths for Referring to an Item in the Module Tree” section.  Because the tests module is an inner module, we need to bring the code under test in the outer module into the scope of the inner module.  We use a glob(*) here so anything we define in the outer module is available to this tests module.   We’ve named our test larger_can_hold_smaller, and we’ve created the two Rectangle instances that we need.  Then we called the assert! macro and passed it the result of calling larger.can_hold(&amp;smaller).  This expression is supposed to return true, so our test should pass. Let’s find out!   running 1 test test tests::larger_can_hold_smaller ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out   It does pass! Let’s add another test, this time asserting that a smaller rectangle cannot hold a larger rectangle:   #[cfg(test)] mod tests {     use super::*;      #[test]     fn larger_can_hold_smaller() {         // --snip--     }      #[test]     fn smaller_cannot_hold_larger() {         let larger = Rectangle { width: 8, height: 7 };         let smaller = Rectangle { width: 5, height: 1 };          assert!(!smaller.can_hold(&amp;larger));     } }  Because the correct result of the can_hold function in this case is false, we need to negate that result before we pass it to the assert! macro.  As a result, our test will pass if can_hold returns false:   running 2 tests test tests::smaller_cannot_hold_larger ... ok test tests::larger_can_hold_smaller ... ok  test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out   Two tests that pass! Now let’s see what happens to our test results when we introduce a bug in our code.  Let’s change the implementation of the can_hold method by replacing the greater than sign(&gt;) with a less than sign(&lt;) when it compares the widths:   // --snip--  impl Rectangle {     fn can_hold(&amp;self, other: &amp;Rectangle) -&gt; bool {         self.width &lt; other.width &amp;&amp; self.height &gt; other.height     } }  Running the tests now produces the following:   running 2 tests test tests::smaller_cannot_hold_larger ... ok test tests::larger_can_hold_smaller ... FAILED  failures:  ---- tests::larger_can_hold_smaller stdout ---- thread 'tests::larger_can_hold_smaller' panicked at 'assertion failed: larger.can_hold(&amp;smaller)', src/lib.rs:22:9 note: Run with `RUST_BACKTRACE=1` for a backtrace.  failures:     tests::larger_can_hold_smaller  test result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  Our tests caught the bug!  Because larger.width is 8 and smaller.width is 5, the comparison of the widths in can_hold now returns false: 8 is not less than 5.   Testing Equality with the assert_eq! and assert_ne! Macros   A common way to test functionality is to compare the result of the code under test to the value you expect the code to return to make sure they’re equal.  You could do this using the assert! macro and passing it an expression using the == operator.  However, this is such a common test that the standard library provides a pair of macros—assert_eq! and assert_ne!—to perform this test more conveniently.  These macros compare two arguments for equality or inequality, respectively.  They’ll also print the two values if the assertion fails, which makes it easier to see why the test failed;  conversely, the assert! macro only indicates that it got a false value for the == expression, not the values that lead to the false value.   In Listing 11-7, we write a function named add_two that adds 2 to its parameter and returns the result.  Then we test this function using the assert_eq! macro.   pub fn add_two(a: i32) -&gt; i32 {     a + 2 }  #[cfg(test)] mod tests {     use super::*;      #[test]     fn it_adds_two() {         assert_eq!(4, add_two(2));     } }  Listing 11-7: Testing the function add_two using the assert_eq! macro   Let’s check that it passes!   running 1 test test tests::it_adds_two ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out   The first argument we gave to the assert_eq! macro, 4, is equal to the result of calling add_two(2).  The line for this test is test tests::it_adds_two ... ok, and the ok text indicates that our test passed!   Let’s introduce a bug into our code to see what it looks like when a test that uses assert_eq! fails.  Change the implementation of the add_two function to instead add 3:  pub fn add_two(a: i32) -&gt; i32 {     a + 3 }  Run the tests again:   running 1 test test tests::it_adds_two ... FAILED  failures:  ---- tests::it_adds_two stdout ---- thread 'tests::it_adds_two' panicked at 'assertion failed: `(left == right)`   left: `4`,  right: `5`', src/lib.rs:11:9 note: Run with `RUST_BACKTRACE=1` for a backtrace.  failures:     tests::it_adds_two  test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  Our test caught the bug!  The it_adds_two test failed, displaying the message assertion failed: '(left == right)' and showing that left was 4 and right was 5.  This message is useful and helps us start debugging: it means the left argument to assert_eq! was 4 but the right argument, where we had add_two(2), was 5.   Note that in some languages and test frameworks, the parameters to the functions that assert two values are equal are called expected and actual, and the order in which we specify the arguments matters.  However, in Rust, they’re called left and right, and the order in which we specify the value we expect and the value that the code under test produces doesn’t matter.  We could write the assertion in this test as assert_eq!(add_two(2), 4), which would result in a failure message that displays assertion failed: '(left == right)' and that left was 5 and right was 4.   The assert_ne! macro will pass if the two values we give it are not equal and fail if they’re equal.  This macro is most useful for cases when we’re not sure what a value will be, but we know what the value definitely won’t be if our code is functioning as we intend.  For example, if we’re testing a function that is guaranteed to change its input in some way, but the way in which the input is changed depends on the day of the week that we run our tests, the best thing to assert might be that the output of the function is not equal to the input.   Under the surface, the assert_eq! and assert_ne! macros use the operators == and !=, respectively.  When the assertions fail, these macros print their arguments using debug formatting, which means the values being compared must implement the PartialEq and Debug traits. A ll the primitive types and most of the standard library types implement these traits.  For structs and enums that you define, you’ll need to implement PartialEq to assert that values of those types are equal or not equal.  You’ll need to implement Debug to print the values when the assertion fails.  Because both traits are derivable traits, as mentioned in Listing 5-12 in Chapter 5, this is usually as straightforward as adding the #[derive(PartialEq, Debug)] annotation to your struct or enum definition.  See Appendix C, “Derivable Traits,” for more details about these and other derivable traits.   Adding Custom Failure Messages  You can also add a custom message to be printed with the failure message as optional arguments to the assert!, assert_eq!, and assert_ne! macros.  Any arguments specified after the one required argument to assert! or the two required arguments to assert_eq! and assert_ne! are passed along to the format! macro (discussed in Chapter 8 in the “Concatenation with the + Operator or the format! Macro” section), so you can pass a format string that contains {} placeholders and values to go in those placeholders.  Custom messages are useful to document what an assertion means; when a test fails, you’ll have a better idea of what the problem is with the code.   For example, let’s say we have a function that greets people by name and we want to test that the name we pass into the function appears in the output:  pub fn greeting(name: &amp;str) -&gt; String {     format!(\"Hello {}!\", name) }  #[cfg(test)] mod tests {     use super::*;      #[test]     fn greeting_contains_name() {         let result = greeting(\"Carol\");         assert!(result.contains(\"Carol\"));     } }  The requirements for this program haven’t been agreed upon yet, and we’re pretty sure the Hello text at the beginning of the greeting will change.  We decided we don’t want to have to update the test when the requirements change, so instead of checking for exact equality to the value returned from the greeting function, we’ll just assert that the output contains the text of the input parameter.   Let’s introduce a bug into this code by changing greeting to not include name to see what this test failure looks like:  pub fn greeting(name: &amp;str) -&gt; String {     String::from(\"Hello!\") }  Running this test produces the following:  running 1 test test tests::greeting_contains_name ... FAILED  failures:  ---- tests::greeting_contains_name stdout ---- thread 'tests::greeting_contains_name' panicked at 'assertion failed: result.contains(\"Carol\")', src/lib.rs:12:9 note: Run with `RUST_BACKTRACE=1` for a backtrace.  failures:     tests::greeting_contains_name   This result just indicates that the assertion failed and which line the assertion is on.  A more useful failure message in this case would print the value we got from the greeting function.  Let’s change the test function, giving it a custom failure message made from a format string with a placeholder filled in with the actual value we got from the greeting function:   #[test] fn greeting_contains_name() {     let result = greeting(\"Carol\");     assert!(         result.contains(\"Carol\"),         \"Greeting did not contain name, value was `{}`\", result     ); }  Now when we run the test, we’ll get a more informative error message:  ---- tests::greeting_contains_name stdout ---- thread 'tests::greeting_contains_name' panicked at 'Greeting did not contain name, value was `Hello!`', src/lib.rs:12:9 note: Run with `RUST_BACKTRACE=1` for a backtrace.  We can see the value we actually got in the test output, which would help us debug what happened instead of what we were expecting to happen.   Checking for Panics with should_panic  In addition to checking that our code returns the correct values we expect, it’s also important to check that our code handles error conditions as we expect.  For example, consider the Guess type that we created in Chapter 9, Listing 9-10.  Other code that uses Guess depends on the guarantee that Guess instances will contain only values between 1 and 100.  We can write a test that ensures that attempting to create a Guess instance with a value outside that range panics.   We do this by adding another attribute, should_panic, to our test function.  This attribute makes a test pass if the code inside the function panics; the test will fail if the code inside the function doesn’t panic.   Listing 11-8 shows a test that checks that the error conditions of Guess::new happen when we expect them to.  fn main() {} pub struct Guess {     value: i32, }  impl Guess {     pub fn new(value: i32) -&gt; Guess {         if value &lt; 1 || value &gt; 100 {             panic!(\"Guess value must be between 1 and 100, got {}.\", value);         }          Guess {             value         }     } }  #[cfg(test)] mod tests {     use super::*;      #[test]     #[should_panic]     fn greater_than_100() {         Guess::new(200);     } }  Listing 11-8: Testing that a condition will cause a panic!   We place the #[should_panic] attribute after the #[test] attribute and before the test function it applies to.  Let’s look at the result when this test passes:   running 1 test test tests::greater_than_100 ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  Looks good! Now let’s introduce a bug in our code by removing the condition that the new function will panic if the value is greater than 100:  // --snip-- impl Guess {     pub fn new(value: i32) -&gt; Guess {         if value &lt; 1  {             panic!(\"Guess value must be between 1 and 100, got {}.\", value);         }          Guess {             value         }     } }  When we run the test in Listing 11-8, it will fail:   running 1 test test tests::greater_than_100 ... FAILED  failures:  failures:     tests::greater_than_100  test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  We don’t get a very helpful message in this case, but when we look at the test function, we see that it’s annotated with #[should_panic].  The failure we got means that the code in the test function did not cause a panic.   Tests that use should_panic can be imprecise because they only indicate that the code has caused some panic.  A should_panic test would pass even if the test panics for a different reason from the one we were expecting to happen.  To make should_panic tests more precise, we can add an optional expected parameter to the should_panic attribute.  The test harness will make sure that the failure message contains the provided text.  For example, consider the modified code for Guess in Listing 11-9 where the new function panics with different messages depending on whether the value is too small or too large.   // --snip--  impl Guess {     pub fn new(value: i32) -&gt; Guess {         if value &lt; 1 {             panic!(\"Guess value must be greater than or equal to 1, got {}.\",                    value);         } else if value &gt; 100 {             panic!(\"Guess value must be less than or equal to 100, got {}.\",                    value);         }          Guess {             value         }     } }  #[cfg(test)] mod tests {     use super::*;      #[test]     #[should_panic(expected = \"Guess value must be less than or equal to 100\")]     fn greater_than_100() {         Guess::new(200);     } }  Listing 11-9: Testing that a condition will cause a panic! with a particular panic message  This test will pass because the value we put in the should_panic attribute’s expected parameter is a substring of the message that the Guess::new function panics with.  We could have specified the entire panic message that we expect, which in this case would be Guess value must be less than or equal to 100, got 200.  What you choose to specify in the expected parameter for should_panic depends on how much of the panic message is unique or dynamic and how precise you want your test to be.  In this case, a substring of the panic message is enough to ensure that the code in the test function executes the else if value &gt; 100 case.   To see what happens when a should_panic test with an expected message fails, let’s again introduce a bug into our code by swapping the bodies of the if value &lt; 1 and the else if value &gt; 100 blocks:   if value &lt; 1 {     panic!(\"Guess value must be less than or equal to 100, got {}.\", value); } else if value &gt; 100 {     panic!(\"Guess value must be greater than or equal to 1, got {}.\", value); }   This time when we run the should_panic test, it will fail:   running 1 test test tests::greater_than_100 ... FAILED  failures:  ---- tests::greater_than_100 stdout ---- thread 'tests::greater_than_100' panicked at 'Guess value must be greater than or equal to 1, got 200.', src/lib.rs:11:13 note: Run with `RUST_BACKTRACE=1` for a backtrace. note: Panic did not include expected string 'Guess value must be less than or equal to 100'  failures:     tests::greater_than_100  test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  The failure message indicates that this test did indeed panic as we expected, but the panic message did not include the expected string ‘Guess value must be less than or equal to 100'.  The panic message that we did get in this case was Guess value must be greater than or equal to 1, got 200. Now we can start figuring out where our bug is!   Using Result&lt;T, E&gt; in Tests  So far, we’ve written tests that panic when they fail.  We can also write tests that use Result&lt;T, E&gt;!  Here’s the test from Listing 11-1, rewritten to use Result&lt;T, E&gt; and return an Err instead of panicking:   #[cfg(test)] mod tests {     #[test]     fn it_works() -&gt; Result&lt;(), String&gt; {         if 2 + 2 == 4 {             Ok(())         } else {             Err(String::from(\"two plus two does not equal four\"))         }     } }   The it_works function now has a return type, Result&lt;(), String&gt;.  In the body of the function, rather than calling the assert_eq! macro, we return Ok(()) when the test passes and an Err with a String inside when the test fails.   Writing tests so they return a Result&lt;T, E&gt; enables you to use the question mark operator in the body of tests,  which can be a convenient way to write tests that should fail if any operation within them returns an Err variant.   You can’t use the #[should_panic] annotation on tests that use Result&lt;T, E&gt;.  Instead, you should return an Err value directly when the test should fail.   Now that you know several ways to write tests, let’s look at what is happening when we run our tests and explore the different options we can use with cargo test.   Controlling How Tests Are Run  Just as cargo run compiles your code and then runs the resulting binary, cargo test compiles your code in test mode and runs the resulting test binary.  You can specify command line options to change the default behavior of cargo test.  For example, the default behavior of the binary produced by cargo test is to run all the tests in parallel and capture output generated during test runs, preventing the output from being displayed and making it easier to read the output related to the test results.   Some command line options go to cargo test, and some go to the resulting test binary.  To separate these two types of arguments, you list the arguments that go to cargo test followed by the separator -- and then the ones that go to the test binary.  Running cargo test --help displays the options you can use with cargo test, and running cargo test -- --help displays the options you can use after the separator --.   Running Tests in Parallel or Consecutively  When you run multiple tests, by default they run in parallel using threads.  This means the tests will finish running faster so you can get feedback quicker on whether or not your code is working.  Because the tests are running at the same time, make sure your tests don’t depend on each other or on any shared state, including a shared environment, such as the current working directory or environment variables.   For example, say each of your tests runs some code that creates a file on disk named test-output.txt and writes some data to that file.  Then each test reads the data in that file and asserts that the file contains a particular value, which is different in each test.  Because the tests run at the same time, one test might overwrite the file between when another test writes and reads the file.  The second test will then fail, not because the code is incorrect but because the tests have interfered with each other while running in parallel.  One solution is to make sure each test writes to a different file; another solution is to run the tests one at a time.   If you don’t want to run the tests in parallel or if you want more fine-grained control over the number of threads used, you can send the --test-threads flag and the number of threads you want to use to the test binary. Take a look at the following example:   $ cargo test -- --test-threads=1  We set the number of test threads to 1, telling the program not to use any parallelism.  Running the tests using one thread will take longer than running them in parallel, but the tests won’t interfere with each other if they share state.   Showing Function Output  By default, if a test passes, Rust’s test library captures anything printed to standard output.  For example, if we call println! in a test and the test passes, we won’t see the println! output in the terminal; we’ll see only the line that indicates the test passed.  If a test fails, we’ll see whatever was printed to standard output with the rest of the failure message.   As an example, Listing 11-10 has a silly function that prints the value of its parameter and returns 10, as well as a test that passes and a test that fails.   fn prints_and_returns_10(a: i32) -&gt; i32 {     println!(\"I got the value {}\", a);     10 }  #[cfg(test)] mod tests {     use super::*;      #[test]     fn this_test_will_pass() {         let value = prints_and_returns_10(4);         assert_eq!(10, value);     }      #[test]     fn this_test_will_fail() {         let value = prints_and_returns_10(8);         assert_eq!(5, value);     } }  Listing 11-10: Tests for a function that calls println!   When we run these tests with cargo test, we’ll see the following output:   running 2 tests test tests::this_test_will_pass ... ok test tests::this_test_will_fail ... FAILED  failures:  ---- tests::this_test_will_fail stdout ---- I got the value 8 thread 'tests::this_test_will_fail' panicked at 'assertion failed: `(left == right)`   left: `5`,  right: `10`', src/lib.rs:19:9 note: Run with `RUST_BACKTRACE=1` for a backtrace.  failures:     tests::this_test_will_fail  test result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  Note that nowhere in this output do we see I got the value 4, which is what is printed when the test that passes runs.  That output has been captured.  The output from the test that failed, I got the value 8, appears in the section of the test summary output, which also shows the cause of the test failure.   If we want to see printed values for passing tests as well, we can disable the output capture behavior by using the --nocapture flag:   $ cargo test -- --nocapture  When we run the tests in Listing 11-10 again with the --nocapture flag, we see the following output:   running 2 tests I got the value 4 I got the value 8 test tests::this_test_will_pass ... ok thread 'tests::this_test_will_fail' panicked at 'assertion failed: `(left == right)`   left: `5`,  right: `10`', src/lib.rs:19:9 note: Run with `RUST_BACKTRACE=1` for a backtrace. test tests::this_test_will_fail ... FAILED  failures:  failures:     tests::this_test_will_fail  test result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  Note that the output for the tests and the test results are interleaved;  the reason is that the tests are running in parallel, as we talked about in the previous section.  Try using the --test-threads=1 option and the --nocapture flag, and see what the output looks like then!   Running a Subset of Tests by Name  Sometimes, running a full test suite can take a long time.  If you’re working on code in a particular area, you might want to run only the tests pertaining to that code.  You can choose which tests to run by passing cargo test the name or names of the test(s) you want to run as an argument.   To demonstrate how to run a subset of tests, we’ll create three tests for our add_two function, as shown in Listing 11-11, and choose which ones to run.   pub fn add_two(a: i32) -&gt; i32 {     a + 2 }  #[cfg(test)] mod tests {     use super::*;      #[test]     fn add_two_and_two() {         assert_eq!(4, add_two(2));     }      #[test]     fn add_three_and_two() {         assert_eq!(5, add_two(3));     }      #[test]     fn one_hundred() {         assert_eq!(102, add_two(100));     } }  Listing 11-11: Three tests with three different names   If we run the tests without passing any arguments, as we saw earlier, all the tests will run in parallel:   running 3 tests test tests::add_two_and_two ... ok test tests::add_three_and_two ... ok test tests::one_hundred ... ok  test result: ok. 3 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  Running Single Tests  We can pass the name of any test function to cargo test to run only that test:   $ cargo test one_hundred     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running target/debug/deps/adder-06a75b4a1f2515e9  running 1 test test tests::one_hundred ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 2 filtered out  Only the test with the name one_hundred ran; the other two tests didn’t match that name.  The test output lets us know we had more tests than what this command ran by displaying 2 filtered out at the end of the summary line.   We can’t specify the names of multiple tests in this way; only the first value given to cargo test will be used.  But there is a way to run multiple tests.   Filtering to Run Multiple Tests  We can specify part of a test name, and any test whose name matches that value will be run.  For example, because two of our tests’ names contain add, we can run those two by running cargo test add:   $ cargo test add     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running target/debug/deps/adder-06a75b4a1f2515e9  running 2 tests test tests::add_two_and_two ... ok test tests::add_three_and_two ... ok  test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 1 filtered out  This command ran all tests with add in the name and filtered out the test named one_hundred.  Also note that the module in which a test appears becomes part of the test’s name, so we can run all the tests in a module by filtering on the module’s name.   Ignoring Some Tests Unless Specifically Requested  Sometimes a few specific tests can be very time-consuming to execute, so you might want to exclude them during most runs of cargo test.  Rather than listing as arguments all tests you do want to run, you can instead annotate the time-consuming tests using the ignore attribute to exclude them, as shown here:   #![allow(unused_variables)]   fn main() {   #[test]   fn it_works() {       assert_eq!(2 + 2, 4);   }    #[test]   #[ignore]   fn expensive_test() {       // code that takes an hour to run   } }  After #[test] we add the #[ignore] line to the test we want to exclude.  Now when we run our tests, it_works runs, but expensive_test doesn’t:   $ cargo test    Compiling adder v0.1.0 (file:///projects/adder)     Finished dev [unoptimized + debuginfo] target(s) in 0.24 secs      Running target/debug/deps/adder-ce99bcc2479f4607  running 2 tests test expensive_test ... ignored test it_works ... ok  test result: ok. 1 passed; 0 failed; 1 ignored; 0 measured; 0 filtered out  The expensive_test function is listed as ignored.  If we want to run only the ignored tests, we can use cargo test -- --ignored:   $ cargo test -- --ignored     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running target/debug/deps/adder-ce99bcc2479f4607  running 1 test test expensive_test ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 1 filtered out  By controlling which tests run, you can make sure your cargo test results will be fast.  When you’re at a point where it makes sense to check the results of the ignored tests and you have time to wait for the results, you can run cargo test -- --ignored instead.   Test Organization  As mentioned at the start of the chapter, testing is a complex discipline, and different people use different terminology and organization.  The Rust community thinks about tests in terms of two main categories: unit tests and integration tests.  Unit tests are small and more focused, testing one module in isolation at a time, and can test private interfaces.  Integration tests are entirely external to your library and use your code in the same way any other external code would, using only the public interface and potentially exercising multiple modules per test.   Writing both kinds of tests is important to ensure that the pieces of your library are doing what you expect them to, separately and together.   Unit Tests  The purpose of unit tests is to test each unit of code in isolation from the rest of the code to quickly pinpoint where code is and isn’t working as expected.  You’ll put unit tests in the src directory in each file with the code that they’re testing.  The convention is to create a module named tests in each file to contain the test functions and to annotate the module with cfg(test).   The Tests Module and #[cfg(test)]  The #[cfg(test)] annotation on the tests module tells Rust to compile and run the test code only when you run cargo test, not when you run cargo build.  This saves compile time when you only want to build the library and saves space in the resulting compiled artifact because the tests are not included.  You’ll see that because integration tests go in a different directory, they don’t need the #[cfg(test)] annotation.  However, because unit tests go in the same files as the code, you’ll use #[cfg(test)] to specify that they shouldn’t be included in the compiled result.   Recall that when we generated the new adder project in the first section of this chapter, Cargo generated this code for us:   #[cfg(test)] mod tests {     #[test]     fn it_works() {         assert_eq!(2 + 2, 4);     } }  This code is the automatically generated test module.  The attribute cfg stands for configuration and tells Rust that the following item should only be included given a certain configuration option.  In this case, the configuration option is test, which is provided by Rust for compiling and running tests.  By using the cfg attribute, Cargo compiles our test code only if we actively run the tests with cargo test.  This includes any helper functions that might be within this module, in addition to the functions annotated with #[test].   Testing Private Functions  There’s debate within the testing community about whether or not private functions should be tested directly, and other languages make it difficult or impossible to test private functions.  Regardless of which testing ideology you adhere to, Rust’s privacy rules do allow you to test private functions. Consider the code in Listing 11-12 with the private function internal_adder.   pub fn add_two(a: i32) -&gt; i32 {     internal_adder(a, 2) }  fn internal_adder(a: i32, b: i32) -&gt; i32 {     a + b }  #[cfg(test)] mod tests {     use super::*;      #[test]     fn internal() {         assert_eq!(4, internal_adder(2, 2));     } }  Listing 11-12: Testing a private function   Note that the internal_adder function is not marked as pub, but because tests are just Rust code and the tests module is just another module, you can bring internal_adder into a test’s scope and call it.  If you don’t think private functions should be tested, there’s nothing in Rust that will compel you to do so.   Integration Tests  In Rust, integration tests are entirely external to your library.  They use your library in the same way any other code would, which means they can only call functions that are part of your library’s public API.  Their purpose is to test whether many parts of your library work together correctly.  Units of code that work correctly on their own could have problems when integrated, so test coverage of the integrated code is important as well.  To create integration tests, you first need a tests directory.   The tests Directory  We create a tests directory at the top level of our project directory, next to src.  Cargo knows to look for integration test files in this directory.  We can then make as many test files as we want to in this directory, and Cargo will compile each of the files as an individual crate.   Let’s create an integration test.  With the code in Listing 11-12 still in the src/lib.rs file, make a tests directory, create a new file named tests/integration_test.rs, and enter the code in Listing 11-13.   //Filename: tests/integration_test.rs use adder;  #[test] fn it_adds_two() {     assert_eq!(4, adder::add_two(2)); }  Listing 11-13: An integration test of a function in the adder crate   We’ve added use adder at the top of the code, which we didn’t need in the unit tests.  The reason is that each file in the tests directory is a separate crate, so we need to bring our library into each test crate’s scope.   We don’t need to annotate any code in tests/integration_test.rs with #[cfg(test)].  Cargo treats the tests directory specially and compiles files in this directory only when we run cargo test.  Run cargo test now:   $ cargo test    Compiling adder v0.1.0 (file:///projects/adder)     Finished dev [unoptimized + debuginfo] target(s) in 0.31 secs      Running target/debug/deps/adder-abcabcabc  running 1 test test tests::internal ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out       Running target/debug/deps/integration_test-ce99bcc2479f4607  running 1 test test it_adds_two ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out     Doc-tests adder  running 0 tests  test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out   The three sections of output include the unit tests, the integration test, and the doc tests.  The first section for the unit tests is the same as we’ve been seeing: one line for each unit test (one named internal that we added in Listing 11-12) and then a summary line for the unit tests.   The integration tests section starts with the line Running target/debug/deps/integration_test-ce99bcc2479f4607 (the hash at the end of your output will be different).  Next, there is a line for each test function in that integration test and a summary line for the results of the integration test just before the Doc-tests adder section starts.   Similarly to how adding more unit test functions adds more result lines to the unit tests section, adding more test functions to the integration test file adds more result lines to this integration test file’s section.  Each integration test file has its own section, so if we add more files in the tests directory, there will be more integration test sections.   We can still run a particular integration test function by specifying the test function’s name as an argument to cargo test.  To run all the tests in a particular integration test file, use the --test argument of cargo test followed by the name of the file:   $ cargo test --test integration_test     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running target/debug/integration_test-952a27e0126bb565  running 1 test test it_adds_two ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  This command runs only the tests in the tests/integration_test.rs file.   Submodules in Integration Tests  As you add more integration tests, you might want to make more than one file in the tests directory to help organize them; for example, you can group the test functions by the functionality they’re testing.  As mentioned earlier, each file in the tests directory is compiled as its own separate crate.   Treating each integration test file as its own crate is useful to create separate scopes that are more like the way end users will be using your crate.  However, this means files in the tests directory don’t share the same behavior as files in src do, as you learned in Chapter 7 regarding how to separate code into modules and files.   The different behavior of files in the tests directory is most noticeable when you have a set of helper functions that would be useful in multiple integration test files and you try to follow the steps in the “Separating Modules into Different Files” section of Chapter 7 to extract them into a common module.  For example, if we create tests/common.rs and place a function named setup in it, we can add some code to setup that we want to call from multiple test functions in multiple test files:   //Filename: tests/common.rs pub fn setup() {     // setup code specific to your library's tests would go here }  When we run the tests again, we’ll see a new section in the test output for the common.rs file, even though this file doesn’t contain any test functions nor did we call the setup function from anywhere:   running 1 test test tests::internal ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out       Running target/debug/deps/common-b8b07b6f1be2db70  running 0 tests  test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out       Running target/debug/deps/integration_test-d993c68b431d39df  running 1 test test it_adds_two ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out     Doc-tests adder  running 0 tests  test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  Having common appear in the test results with running 0 tests displayed for it is not what we wanted.  We just wanted to share some code with the other integration test files.   To avoid having common appear in the test output, instead of creating tests/common.rs, we’ll create tests/common/mod.rs.  This is an alternate naming convention that Rust also understands.  Naming the file this way tells Rust not to treat the common module as an integration test file.  When we move the setup function code into tests/common/mod.rs and delete the tests/common.rs file, the section in the test output will no longer appear.  Files in subdirectories of the tests directory don’t get compiled as separate crates or have sections in the test output.   After we’ve created tests/common/mod.rs, we can use it from any of the integration test files as a module.  Here’s an example of calling the setup function from the it_adds_two test in tests/integration_test.rs:   //Filename: tests/integration_test.rs use adder;  mod common;  #[test] fn it_adds_two() {     common::setup();     assert_eq!(4, adder::add_two(2)); }  Note that the mod common; declaration is the same as the module declaration we demonstrated in Listing 7-21.  Then in the test function, we can call the common::setup() function.   Integration Tests for Binary Crates  If our project is a binary crate that only contains a src/main.rs file and doesn’t have a src/lib.rs file, we can’t create integration tests in the tests directory and bring functions defined in the src/main.rs file into scope with a use statement.  Only library crates expose functions that other crates can use; binary crates are meant to be run on their own.   This is one of the reasons Rust projects that provide a binary have a straightforward src/main.rs file that calls logic that lives in the src/lib.rs file.  Using that structure, integration tests can test the library crate with use to make the important functionality available.  If the important functionality works, the small amount of code in the src/main.rs file will work as well, and that small amount of code doesn’t need to be tested.   Summary  Rust’s testing features provide a way to specify how code should function to ensure it continues to work as you expect, even as you make changes.  Unit tests exercise different parts of a library separately and can test private implementation details.  Integration tests check that many parts of the library work together correctly, and they use the library’s public API to test the code in the same way external code will use it.  Even though Rust’s type system and ownership rules help prevent some kinds of bugs, tests are still important to reduce logic bugs having to do with how your code is expected to behave.  ","categories": ["RUST Language"],
        "tags": ["Automataion","Test","Bug"],
        "url": "https://jjungs-lee.github.io//rust/11.Writing-Automated-Tests",
        "teaser":null},{
        "title": "RUST : 12. An I/O Project: Building a Command Line Program",
        "excerpt":"An I/O Project: Building a Command Line Program  This chapter is a recap of the many skills you’ve learned so far and an exploration of a few more standard library features.  We’ll build a command line tool that interacts with file and command line input/output to practice some of the Rust concepts you now have under your belt.   Rust’s speed, safety, single binary output, and cross-platform support make it an ideal language for creating command line tools, so for our project, we’ll make our own version of the classic command line tool grep (globally search a regular expression and print).  In the simplest use case, grep searches a specified file for a specified string.  To do so, grep takes as its arguments a filename and a string.  Then it reads the file, finds lines in that file that contain the string argument, and prints those lines.   Along the way, we’ll show how to make our command line tool use features of the terminal that many command line tools use.  We’ll read the value of an environment variable to allow the user to configure the behavior of our tool.  We’ll also print error messages to the standard error console stream (stderr) instead of standard output (stdout), so, for example, the user can redirect successful output to a file while still seeing error messages onscreen.   One Rust community member, Andrew Gallant, has already created a fully featured, very fast version of grep, called ripgrep.  By comparison, our version of grep will be fairly simple, but this chapter will give you some of the background knowledge you need to understand a real-world project such as ripgrep.   Our grep project will combine a number of concepts you’ve learned so far:      Organizing code (using what you learned about modules in Chapter 7)   Using vectors and strings (collections, Chapter 8)   Handling errors (Chapter 9)   Using traits and lifetimes where appropriate (Chapter 10)   Writing tests (Chapter 11) We’ll also briefly introduce closures, iterators, and trait objects, which Chapters 13 and 17 will cover in detail.   Accepting Command Line Arguments  Let’s create a new project with, as always, cargo new.  We’ll call our project minigrep to distinguish it from the grep tool that you might already have on your system.   $ cargo new minigrep      Created binary (application) `minigrep` project $ cd minigrep  The first task is to make minigrep accept its two command line arguments: the filename and a string to search for.  That is, we want to be able to run our program with cargo run, a string to search for, and a path to a file to search in, like so:  $ cargo run searchstring example-filename.txt  Right now, the program generated by cargo new cannot process arguments we give it.  Some existing libraries on crates.io can help with writing a program that accepts command line arguments, but because you’re just learning this concept, let’s implement this capability ourselves.   Reading the Argument Values  To enable minigrep to read the values of command line arguments we pass to it, we’ll need a function provided in Rust’s standard library, which is std::env::args.  This function returns an iterator of the command line arguments that were given to minigrep.  We’ll cover iterators fully in Chapter 13.  For now, you only need to know two details about iterators: iterators produce a series of values, and we can call the collect method on an iterator to turn it into a collection, such as a vector, containing all the elements the iterator produces.   Use the code in Listing 12-1 to allow your minigrep program to read any command line arguments passed to it and then collect the values into a vector.  // Filename: src/main.rs use std::env;  fn main() {     let args: Vec&lt;String&gt; = env::args().collect();     println!(\"{:?}\", args); }  Listing 12-1: Collecting the command line arguments into a vector and printing them   First, we bring the std::env module into scope with a use statement so we can use its args function.  Notice that the std::env::args function is nested in two levels of modules.  As we discussed in Chapter 7, in cases where the desired function is nested in more than one module, it’s conventional to bring the parent module into scope rather than the function.  By doing so, we can easily use other functions from std::env.  It’s also less ambiguous than adding use std::env::args and then calling the function with just args, because args might easily be mistaken for a function that’s defined in the current module.   The args Function and Invalid Unicode   Note that std::env::args will panic if any argument contains invalid Unicode.  If your program needs to accept arguments containing invalid Unicode, use std::env::args_os instead.  That function returns an iterator that produces OsString values instead of String values.  We’ve chosen to use std::env::args here for simplicity, because OsString values differ per platform and are more complex to work with than String values.   On the first line of main, we call env::args, and we immediately use collect to turn the iterator into a vector containing all the values produced by the iterator.  We can use the collect function to create many kinds of collections, so we explicitly annotate the type of args to specify that we want a vector of strings.  Although we very rarely need to annotate types in Rust, collect is one function you do often need to annotate because Rust isn’t able to infer the kind of collection you want.   Finally, we print the vector using the debug formatter, :?.  Let’s try running the code first with no arguments and then with two arguments:   $ cargo run --snip-- [\"target/debug/minigrep\"]  $ cargo run needle haystack --snip-- [\"target/debug/minigrep\", \"needle\", \"haystack\"]   Notice that the first value in the vector is \"target/debug/minigrep\", which is the name of our binary.  This matches the behavior of the arguments list in C, letting programs use the name by which they were invoked in their execution.  It’s often convenient to have access to the program name in case you want to print it in messages or change behavior of the program based on what command line alias was used to invoke the program.  But for the purposes of this chapter, we’ll ignore it and save only the two arguments we need.   Saving the Argument Values in Variables  Printing the value of the vector of arguments illustrated that the program is able to access the values specified as command line arguments.  Now we need to save the values of the two arguments in variables so we can use the values throughout the rest of the program. We do that in Listing 12-2.   // Filename: src/main.rs use std::env;  fn main() {     let args: Vec&lt;String&gt; = env::args().collect();      let query = &amp;args[1];     let filename = &amp;args[2];      println!(\"Searching for {}\", query);     println!(\"In file {}\", filename); }  Listing 12-2: Creating variables to hold the query argument and filename argument   As we saw when we printed the vector, the program’s name takes up the first value in the vector at args[0], so we’re starting at index 1.  The first argument minigrep takes is the string we’re searching for, so we put a reference to the first argument in the variable query.  The second argument will be the filename, so we put a reference to the second argument in the variable filename.   We temporarily print the values of these variables to prove that the code is working as we intend.  Let’s run this program again with the arguments test and sample.txt:   $ cargo run test sample.txt    Compiling minigrep v0.1.0 (file:///projects/minigrep)     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep test sample.txt` Searching for test In file sample.txt   Great, the program is working! The values of the arguments we need are being saved into the right variables. Later we’ll add some error handling to deal with certain potential erroneous situations, such as when the user provides no arguments; for now, we’ll ignore that situation and work on adding file-reading capabilities instead.   Reading a File  Now we’ll add functionality to read the file that is specified in the filename command line argument.  First, we need a sample file to test it with: the best kind of file to use to make sure minigrep is working is one with a small amount of text over multiple lines with some repeated words.  Listing 12-3 has an Emily Dickinson poem that will work well!  Create a file called poem.txt at the root level of your project, and enter the poem “I’m Nobody! Who are you?”   // Filename: poem.txt I'm nobody! Who are you? Are you nobody, too? Then there's a pair of us - don't tell! They'd banish us, you know.  How dreary to be somebody! How public, like a frog To tell your name the livelong day To an admiring bog!  Listing 12-3: A poem by Emily Dickinson makes a good test case   With the text in place, edit src/main.rs and add code to read the file, as shown in Listing 12-4.   Filename: src/main.rs use std::env; use std::fs;  fn main() {     // --snip--     println!(\"In file {}\", filename);      let contents = fs::read_to_string(filename)         .expect(\"Something went wrong reading the file\");      println!(\"With text:\\n{}\", contents); }  Listing 12-4: Reading the contents of the file specified by the second argument   First, we add another use statement to bring in a relevant part of the standard library: we need std::fs to handle files.   In main, we’ve added a new statement: fs::read_to_string takes the filename, opens that file, and returns a Result&lt;String&gt; of the file’s contents.   After that statement, we’ve again added a temporary println! statement that prints the value of contents after the file is read, so we can check that the program is working so far.   Let’s run this code with any string as the first command line argument (because we haven’t implemented the searching part yet) and the poem.txt file as the second argument:   $ cargo run the poem.txt    Compiling minigrep v0.1.0 (file:///projects/minigrep)     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep the poem.txt` Searching for the In file poem.txt With text: I'm nobody! Who are you? Are you nobody, too? Then there's a pair of us — don't tell! They'd banish us, you know.  How dreary to be somebody! How public, like a frog To tell your name the livelong day To an admiring bog!   Great! The code read and then printed the contents of the file.  But the code has a few flaws.  The main function has multiple responsibilities: generally, functions are clearer and easier to maintain if each function is responsible for only one idea.  The other problem is that we’re not handling errors as well as we could.  The program is still small, so these flaws aren’t a big problem, but as the program grows, it will be harder to fix them cleanly.  It’s good practice to begin refactoring early on when developing a program, because it’s much easier to refactor smaller amounts of code. We’ll do that next.   Refactoring to Improve Modularity and Error Handling  To improve our program, we’ll fix four problems that have to do with the program’s structure and how it’s handling potential errors.   First, our main function now performs two tasks: it parses arguments and reads files.  For such a small function, this isn’t a major problem.  However, if we continue to grow our program inside main, the number of separate tasks the main function handles will increase.  As a function gains responsibilities, it becomes more difficult to reason about, harder to test, and harder to change without breaking one of its parts.  It’s best to separate functionality so each function is responsible for one task.   This issue also ties into the second problem: although query and filename are configuration variables to our program, variables like contents are used to perform the program’s logic.  The longer main becomes, the more variables we’ll need to bring into scope; the more variables we have in scope, the harder it will be to keep track of the purpose of each.  It’s best to group the configuration variables into one structure to make their purpose clear.   The third problem is that we’ve used expect to print an error message when reading the file fails, but the error message just prints Something went wrong reading the file.  Reading a file can fail in a number of ways: for example, the file could be missing, or we might not have permission to open it.  Right now, regardless of the situation, we’d print the Something went wrong reading the file error message, which wouldn’t give the user any information!   Fourth, we use expect repeatedly to handle different errors, and if the user runs our program without specifying enough arguments, they’ll get an index out of bounds error from Rust that doesn’t clearly explain the problem.  It would be best if all the error-handling code were in one place so future maintainers had only one place to consult in the code if the error-handling logic needed to change. Having all the error-handling code in one place will also ensure that we’re printing messages that will be meaningful to our end users.   Let’s address these four problems by refactoring our project.   Separation of Concerns for Binary Projects  The organizational problem of allocating responsibility for multiple tasks to the main function is common to many binary projects.  As a result, the Rust community has developed a process to use as a guideline for splitting the separate concerns of a binary program when main starts getting large. The process has the following steps:      Split your program into a main.rs and a lib.rs and move your program’s logic to lib.rs.   As long as your command line parsing logic is small, it can remain in main.rs.   When the command line parsing logic starts getting complicated, extract it from main.rs and move it to lib.rs.   The responsibilities that remain in the main function after this process should be limited to the following:      Calling the command line parsing logic with the argument values   Setting up any other configuration   Calling a run function in lib.rs   Handling the error if run returns an error   This pattern is about separating concerns: main.rs handles running the program, and lib.rs handles all the logic of the task at hand.  Because you can’t test the main function directly, this structure lets you test all of your program’s logic by moving it into functions in lib.rs.  The only code that remains in main.rs will be small enough to verify its correctness by reading it.  Let’s rework our program by following this process.   Extracting the Argument Parser  We’ll extract the functionality for parsing arguments into a function that main will call to prepare for moving the command line parsing logic to src/lib.rs.  Listing 12-5 shows the new start of main that calls a new function parse_config, which we’ll define in src/main.rs for the moment.   // Filename: src/main.rs fn main() {     let args: Vec&lt;String&gt; = env::args().collect();      let (query, filename) = parse_config(&amp;args);      // --snip-- }  fn parse_config(args: &amp;[String]) -&gt; (&amp;str, &amp;str) {     let query = &amp;args[1];     let filename = &amp;args[2];      (query, filename) }  Listing 12-5: Extracting a parse_config function from main  We’re still collecting the command line arguments into a vector, but instead of assigning the argument value at index 1 to the variable query and the argument value at index 2 to the variable filename within the main function, we pass the whole vector to the parse_config function.  The parse_config function then holds the logic that determines which argument goes in which variable and passes the values back to main.  We still create the query and filename variables in main, but main no longer has the responsibility of determining how the command line arguments and variables correspond.   This rework may seem like overkill for our small program, but we’re refactoring in small, incremental steps.  After making this change, run the program again to verify that the argument parsing still works.  It’s good to check your progress often, to help identify the cause of problems when they occur.   Grouping Configuration Values  We can take another small step to improve the parse_config function further.  At the moment, we’re returning a tuple, but then we immediately break that tuple into individual parts again.  This is a sign that perhaps we don’t have the right abstraction yet.   Another indicator that shows there’s room for improvement is the config part of parse_config, which implies that the two values we return are related and are both part of one configuration value.  We’re not currently conveying this meaning in the structure of the data other than by grouping the two values into a tuple; we could put the two values into one struct and give each of the struct fields a meaningful name. Doing so will make it easier for future maintainers of this code to understand how the different values relate to each other and what their purpose is.   Note: Using primitive values when a complex type would be more appropriate is an anti-pattern known as primitive obsession.   Listing 12-6 shows the improvements to the parse_config function.  fn main() {     let args: Vec&lt;String&gt; = env::args().collect();      let config = parse_config(&amp;args);      println!(\"Searching for {}\", config.query);     println!(\"In file {}\", config.filename);      let contents = fs::read_to_string(config.filename)         .expect(\"Something went wrong reading the file\");      // --snip-- }  struct Config {     query: String,     filename: String, }  fn parse_config(args: &amp;[String]) -&gt; Config {     let query = args[1].clone();     let filename = args[2].clone();      Config { query, filename } }  Listing 12-6: Refactoring parse_config to return an instance of a Config struct   We’ve added a struct named Config defined to have fields named query and filename.  The signature of parse_config now indicates that it returns a Config value.  In the body of parse_config, where we used to return string slices that reference String values in args, we now define Config to contain owned String values.  The args variable in main is the owner of the argument values and is only letting the parse_config function borrow them, which means we’d violate Rust’s borrowing rules if Config tried to take ownership of the values in args.   We could manage the String data in a number of different ways, but the easiest, though somewhat inefficient, route is to call the clone method on the values.  This will make a full copy of the data for the Config instance to own, which takes more time and memory than storing a reference to the string data.  However, cloning the data also makes our code very straightforward because we don’t have to manage the lifetimes of the references; in this circumstance, giving up a little performance to gain simplicity is a worthwhile trade-off.   The Trade-Offs of Using clone  There’s a tendency among many Rustaceans to avoid using clone to fix ownership problems because of its runtime cost.  In Chapter 13, you’ll learn how to use more efficient methods in this type of situation.  But for now, it’s okay to copy a few strings to continue making progress because you’ll make these copies only once and your filename and query string are very small.  It’s better to have a working program that’s a bit inefficient than to try to hyperoptimize code on your first pass.  As you become more experienced with Rust, it’ll be easier to start with the most efficient solution, but for now, it’s perfectly acceptable to call clone.   We’ve updated main so it places the instance of Config returned by parse_config into a variable named config, and we updated the code that previously used the separate query and filename variables so it now uses the fields on the Config struct instead.   Now our code more clearly conveys that query and filename are related and that their purpose is to configure how the program will work.  Any code that uses these values knows to find them in the config instance in the fields named for their purpose.   Creating a Constructor for Config  So far, we’ve extracted the logic responsible for parsing the command line arguments from main and placed it in the parse_config function.  Doing so helped us to see that the query and filename values were related and that relationship should be conveyed in our code. We then added a Config struct to name the related purpose of query and filename and to be able to return the values’ names as struct field names from the parse_config function.   So now that the purpose of the parse_config function is to create a Config instance, we can change parse_config from a plain function to a function named new that is associated with the Config struct.  Making this change will make the code more idiomatic.  We can create instances of types in the standard library, such as String, by calling String::new.  Similarly, by changing parse_config into a new function associated with Config, we’ll be able to create instances of Config by calling Config::new. Listing 12-7 shows the changes we need to make.   Filename: src/main.rs  fn main() {     let args: Vec&lt;String&gt; = env::args().collect();      let config = Config::new(&amp;args);      // --snip-- }  // --snip--  impl Config {     fn new(args: &amp;[String]) -&gt; Config {         let query = args[1].clone();         let filename = args[2].clone();          Config { query, filename }     } }  Listing 12-7: Changing parse_config into Config::new  We’ve updated main where we were calling parse_config to instead call Config::new.  We’ve changed the name of parse_config to new and moved it within an impl block, which associates the new function with Config. Try compiling this code again to make sure it works.   Fixing the Error Handling  Now we’ll work on fixing our error handling.  Recall that attempting to access the values in the args vector at index 1 or index 2 will cause the program to panic if the vector contains fewer than three items.  Try running the program without any arguments; it will look like this:   $ cargo run    Compiling minigrep v0.1.0 (file:///projects/minigrep)     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep` thread 'main' panicked at 'index out of bounds: the len is 1 but the index is 1', src/main.rs:25:21 note: Run with `RUST_BACKTRACE=1` for a backtrace.  The line index out of bounds: the len is 1 but the index is 1 is an error message intended for programmers. It won’t help our end users understand what happened and what they should do instead. Let’s fix that now.   Improving the Error Message  In Listing 12-8, we add a check in the new function that will verify that the slice is long enough before accessing index 1 and 2.  If the slice isn’t long enough, the program panics and displays a better error message than the index out of bounds message.   // Filename: src/main.rs  // --snip-- fn new(args: &amp;[String]) -&gt; Config {     if args.len() &lt; 3 {         panic!(\"not enough arguments\");     }     // --snip--  Listing 12-8: Adding a check for the number of arguments   This code is similar to the Guess::new function we wrote in Listing 9-10, where we called panic! when the value argument was out of the range of valid values.  Instead of checking for a range of values here, we’re checking that the length of args is at least 3 and the rest of the function can operate under the assumption that this condition has been met.  If args has fewer than three items, this condition will be true, and we call the panic! macro to end the program immediately.   With these extra few lines of code in new, let’s run the program without any arguments again to see what the error looks like now:   $ cargo run    Compiling minigrep v0.1.0 (file:///projects/minigrep)     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep` thread 'main' panicked at 'not enough arguments', src/main.rs:26:13 note: Run with `RUST_BACKTRACE=1` for a backtrace.   This output is better: we now have a reasonable error message.  However, we also have extraneous information we don’t want to give to our users.  Perhaps using the technique we used in Listing 9-10 isn’t the best to use here: a call to panic! is more appropriate for a programming problem than a usage problem, as discussed in Chapter 9.  Instead, we can use the other technique you learned about in Chapter 9—returning a Result that indicates either success or an error.   Returning a Result from new Instead of Calling panic!  We can instead return a Result value that will contain a Config instance in the successful case and will describe the problem in the error case.  When Config::new is communicating to main, we can use the Result type to signal there was a problem.  Then we can change main to convert an Err variant into a more practical error for our users without the surrounding text about thread 'main' and RUST_BACKTRACE that a call to panic! causes.   Listing 12-9 shows the changes we need to make to the return value of Config::new and the body of the function needed to return a Result.  Note that this won’t compile until we update main as well, which we’ll do in the next listing.  // Filename: src/main.rs  impl Config {     fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;'static str&gt; {         if args.len() &lt; 3 {             return Err(\"not enough arguments\");         }          let query = args[1].clone();         let filename = args[2].clone();          Ok(Config { query, filename })     } }  Listing 12-9: Returning a Result from Config::new   Our new function now returns a Result with a Config instance in the success case and a &amp;'static str in the error case.  Recall from “The Static Lifetime” section in Chapter 10 that &amp;'static str is the type of string literals, which is our error message type for now.   We’ve made two changes in the body of the new function: instead of calling panic! when the user doesn’t pass enough arguments, we now return an Err value, and we’ve wrapped the Config return value in an Ok.  These changes make the function conform to its new type signature.   Returning an Err value from Config::new allows the main function to handle the Result value returned from the new function and exit the process more cleanly in the error case.   Calling Config::new and Handling Errors  To handle the error case and print a user-friendly message, we need to update main to handle the Result being returned by Config::new, as shown in Listing 12-10.  We’ll also take the responsibility of exiting the command line tool with a nonzero error code from panic! and implement it by hand. A nonzero exit status is a convention to signal to the process that called our program that the program exited with an error state.  // Filename: src/main.rs  use std::process;  fn main() {     let args: Vec&lt;String&gt; = env::args().collect();      let config = Config::new(&amp;args).unwrap_or_else(|err| {         println!(\"Problem parsing arguments: {}\", err);         process::exit(1);     });      // --snip--  Listing 12-10: Exiting with an error code if creating a new Config fails   In this listing, we’ve used a method we haven’t covered before: unwrap_or_else, which is defined on Result&lt;T, E&gt; by the standard library.  Using unwrap_or_else allows us to define some custom, non-panic! error handling.  If the Result is an Ok value, this method’s behavior is similar to unwrap: it returns the inner value Ok is wrapping.  However, if the value is an Err value, this method calls the code in the closure, which is an anonymous function we define and pass as an argument to unwrap_or_else.  We’ll cover closures in more detail in Chapter 13.  For now, you just need to know that unwrap_or_else will pass the inner value of the Err, which in this case is the static string not enough arguments that we added in Listing 12-9, to our closure in the argument err that appears between the vertical pipes.  The code in the closure can then use the err value when it runs.   We’ve added a new use line to bring process from the standard library into scope.  The code in the closure that will be run in the error case is only two lines: we print the err value and then call process::exit.  The process::exit function will stop the program immediately and return the number that was passed as the exit status code.  This is similar to the panic!-based handling we used in Listing 12-8, but we no longer get all the extra output. Let’s try it:   $ cargo run    Compiling minigrep v0.1.0 (file:///projects/minigrep)     Finished dev [unoptimized + debuginfo] target(s) in 0.48 secs      Running `target/debug/minigrep` Problem parsing arguments: not enough arguments  Great! This output is much friendlier for our users.   Extracting Logic from main  Now that we’ve finished refactoring the configuration parsing, let’s turn to the program’s logic.  As we stated in “Separation of Concerns for Binary Projects”, we’ll extract a function named run that will hold all the logic currently in the main function that isn’t involved with setting up configuration or handling errors.  When we’re done, main will be concise and easy to verify by inspection, and we’ll be able to write tests for all the other logic.   Listing 12-11 shows the extracted run function. For now, we’re just making the small, incremental improvement of extracting the function. We’re still defining the function in src/main.rs.   // Filename: src/main.rs  fn main() {     // --snip--      println!(\"Searching for {}\", config.query);     println!(\"In file {}\", config.filename);      run(config); }  fn run(config: Config) {     let contents = fs::read_to_string(config.filename)         .expect(\"Something went wrong reading the file\");      println!(\"With text:\\n{}\", contents); }  // --snip--  Listing 12-11: Extracting a run function containing the rest of the program logic   The run function now contains all the remaining logic from main, starting from reading the file.  The run function takes the Config instance as an argument.   Returning Errors from the run Function  With the remaining program logic separated into the run function, we can improve the error handling, as we did with Config::new in Listing 12-9.  Instead of allowing the program to panic by calling expect, the run function will return a Result&lt;T, E&gt; when something goes wrong.  This will let us further consolidate into main the logic around handling errors in a user-friendly way. Listing 12-12 shows the changes we need to make to the signature and body of run.   // Filename: src/main.rs use std::error::Error;  // --snip--  fn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     let contents = fs::read_to_string(config.filename)?;      println!(\"With text:\\n{}\", contents);      Ok(()) }  Listing 12-12: Changing the run function to return Result   We’ve made three significant changes here.  First, we changed the return type of the run function to Result&lt;(), Box&lt;dyn Error&gt;&gt;.  This function previously returned the unit type, (), and we keep that as the value returned in the Ok case.   For the error type, we used the trait object Box&lt;dyn Error&gt; (and we’ve brought std::error::Error into scope with a use statement at the top). We’ll cover trait objects in Chapter 17.  For now, just know that Box&lt;dyn Error&gt; means the function will return a type that implements the Error trait, but we don’t have to specify what particular type the return value will be.  This gives us flexibility to return error values that may be of different types in different error cases.  The dyn keyword is short for “dynamic.”   Second, we’ve removed the call to expect in favor of the ? operator, as we talked about in Chapter 9.  Rather than panic! on an error, ? will return the error value from the current function for the caller to handle.   Third, the run function now returns an Ok value in the success case.  We’ve declared the run function’s success type as () in the signature, which means we need to wrap the unit type value in the Ok value.  This Ok(()) syntax might look a bit strange at first, but using () like this is the idiomatic way to indicate that we’re calling run for its side effects only; it doesn’t return a value we need.   When you run this code, it will compile but will display a warning:   warning: unused `std::result::Result` that must be used   --&gt; src/main.rs:17:5    | 17 |     run(config);    |     ^^^^^^^^^^^^    |    = note: #[warn(unused_must_use)] on by default    = note: this `Result` may be an `Err` variant, which should be handled  Rust tells us that our code ignored the Result value and the Result value might indicate that an error occurred.  But we’re not checking to see whether or not there was an error, and the compiler reminds us that we probably meant to have some error-handling code here! Let’s rectify that problem now.   Handling Errors Returned from run in main  We’ll check for errors and handle them using a technique similar to one we used with Config::new in Listing 12-10, but with a slight difference:   // Filename: src/main.rs  fn main() {     // --snip--      println!(\"Searching for {}\", config.query);     println!(\"In file {}\", config.filename);      if let Err(e) = run(config) {         println!(\"Application error: {}\", e);          process::exit(1);     } }   We use if let rather than unwrap_or_else to check whether run returns an Err value and call process::exit(1) if it does.  The run function doesn’t return a value that we want to unwrap in the same way that Config::new returns the Config instance.  Because run returns () in the success case, we only care about detecting an error, so we don’t need unwrap_or_else to return the unwrapped value because it would only be ().   The bodies of the if let and the unwrap_or_else functions are the same in both cases: we print the error and exit.   Splitting Code into a Library Crate  Our minigrep project is looking good so far! Now we’ll split the src/main.rs file and put some code into the src/lib.rs file so we can test it and have a src/main.rs file with fewer responsibilities.   Let’s move all the code that isn’t the main function from src/main.rs to src/lib.rs:      The run function definition   The relevant use statements   The definition of Config   The Config::new function definition   The contents of src/lib.rs should have the signatures shown in Listing 12-13 (we’ve omitted the bodies of the functions for brevity). Note that this won’t compile until we modify src/main.rs in Listing 12-14.  // Filename: src/lib.rs  use std::error::Error; use std::fs;  pub struct Config {     pub query: String,     pub filename: String, }  impl Config {     pub fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;'static str&gt; {         // --snip--     } }  pub fn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     // --snip-- }  Listing 12-13: Moving Config and run into src/lib.rs   We’ve made liberal use of the pub keyword: on Config, on its fields and its new method, and on the run function.  We now have a library crate that has a public API that we can test!   Now we need to bring the code we moved to src/lib.rs into the scope of the binary crate in src/main.rs, as shown in Listing 12-14.   // Filename: src/main.rs use std::env; use std::process;  use minigrep::Config;  fn main() {     // --snip--     if let Err(e) = minigrep::run(config) {         // --snip--     } }  Listing 12-14: Using the minigrep library crate in src/main.rs   We add a use minigrep::Config line to bring the Config type from the library crate into the binary crate’s scope, and we prefix the run function with our crate name.  Now all the functionality should be connected and should work.  Run the program with cargo run and make sure everything works correctly.   Whew! That was a lot of work, but we’ve set ourselves up for success in the future.  Now it’s much easier to handle errors, and we’ve made the code more modular.  Almost all of our work will be done in src/lib.rs from here on out.   Let’s take advantage of this newfound modularity by doing something that would have been difficult with the old code but is easy with the new code: we’ll write some tests!   Developing the Library’s Functionality with Test-Driven Development   Now that we’ve extracted the logic into src/lib.rs and left the argument collecting and error handling in src/main.rs, it’s much easier to write tests for the core functionality of our code.  We can call functions directly with various arguments and check return values without having to call our binary from the command line.  Feel free to write some tests for the functionality in the Config::new and run functions on your own.   In this section, we’ll add the searching logic to the minigrep program by using the Test-driven development (TDD) process. This software development technique follows these steps:      Write a test that fails and run it to make sure it fails for the reason you expect.   Write or modify just enough code to make the new test pass.   Refactor the code you just added or changed and make sure the tests continue to pass.   Repeat from step 1!   This process is just one of many ways to write software, but TDD can help drive code design as well.  Writing the test before you write the code that makes the test pass helps to maintain high test coverage throughout the process.   We’ll test drive the implementation of the functionality that will actually do the searching for the query string in the file contents and produce a list of lines that match the query.  We’ll add this functionality in a function called search.   Writing a Failing Test  Because we don’t need them anymore, let’s remove the println! statements from src/lib.rs and src/main.rs that we used to check the program’s behavior.  Then, in src/lib.rs, we’ll add a tests module with a test function, as we did in Chapter 11.  The test function specifies the behavior we want the search function to have: it will take a query and the text to search for the query in, and it will return only the lines from the text that contain the query. Listing 12-15 shows this test, which won’t compile yet.   // Filename: src/lib.rs  #[cfg(test)] mod tests {     use super::*;      #[test]     fn one_result() {         let query = \"duct\";         let contents = \"\\ Rust: safe, fast, productive. Pick three.\";          assert_eq!(             vec![\"safe, fast, productive.\"],             search(query, contents)         );     } }  Listing 12-15: Creating a failing test for the search function we wish we had   This test searches for the string \"duct\". The text we’re searching is three lines, only one of which contains \"duct\".  We assert that the value returned from the search function contains only the line we expect.   We aren’t able to run this test and watch it fail because the test doesn’t even compile: the search function doesn’t exist yet!  So now we’ll add just enough code to get the test to compile and run by adding a definition of the search function that always returns an empty vector, as shown in Listing 12-16.  Then the test should compile and fail because an empty vector doesn’t match a vector containing the line \"safe, fast, productive.”   // Filename: src/lib.rs pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {     vec![] }  Listing 12-16: Defining just enough of the search function so our test will compile   Notice that we need an explicit lifetime 'a defined in the signature of search and used with the contents argument and the return value.  Recall in Chapter 10 that the lifetime parameters specify which argument lifetime is connected to the lifetime of the return value.  In this case, we indicate that the returned vector should contain string slices that reference slices of the argument contents (rather than the argument query).   In other words, we tell Rust that the data returned by the search function will live as long as the data passed into the search function in the contents argument.  This is important! The data referenced by a slice needs to be valid for the reference to be valid; if the compiler assumes we’re making string slices of query rather than contents, it will do its safety checking incorrectly.   If we forget the lifetime annotations and try to compile this function, we’ll get this error:   error[E0106]: missing lifetime specifier  --&gt; src/lib.rs:5:51   | 5 | pub fn search(query: &amp;str, contents: &amp;str) -&gt; Vec&lt;&amp;str&gt; {   |                                                   ^ expected lifetime parameter   |   = help: this function's return type contains a borrowed value, but the   signature does not say whether it is borrowed from `query` or `contents`   Rust can’t possibly know which of the two arguments we need, so we need to tell it.  Because contents is the argument that contains all of our text and we want to return the parts of that text that match, we know contents is the argument that should be connected to the return value using the lifetime syntax.   Other programming languages don’t require you to connect arguments to return values in the signature.  Although this might seem strange, it will get easier over time.  You might want to compare this example with the “Validating References with Lifetimes” section in Chapter 10.   Now let’s run the test:   $ cargo test    Compiling minigrep v0.1.0 (file:///projects/minigrep) --warnings--     Finished dev [unoptimized + debuginfo] target(s) in 0.43 secs      Running target/debug/deps/minigrep-abcabcabc  running 1 test test tests::one_result ... FAILED  failures:  ---- tests::one_result stdout ----         thread 'tests::one_result' panicked at 'assertion failed: `(left == right)` left: `[\"safe, fast, productive.\"]`, right: `[]`)', src/lib.rs:48:8 note: Run with `RUST_BACKTRACE=1` for a backtrace.   failures:     tests::one_result  test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  error: test failed, to rerun pass '--lib'  Great, the test fails, exactly as we expected. Let’s get the test to pass!   Writing Code to Pass the Test  Currently, our test is failing because we always return an empty vector.  To fix that and implement search, our program needs to follow these steps:      Iterate through each line of the contents.   Check whether the line contains our query string.   If it does, add it to the list of values we’re returning.   If it doesn’t, do nothing.   Return the list of results that match.   Let’s work through each step, starting with iterating through lines.   Iterating Through Lines with the lines Method  Rust has a helpful method to handle line-by-line iteration of strings, conveniently named lines, that works as shown in Listing 12-17. Note this won’t compile yet.   // Filename: src/lib.rs  pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {     for line in contents.lines() {         // do something with line     } }  Listing 12-17: Iterating through each line in contents   The lines method returns an iterator.  We’ll talk about iterators in depth in Chapter 13, but recall that you saw this way of using an iterator in Listing 3-5, where we used a for loop with an iterator to run some code on each item in a collection.   Searching Each Line for the Query  Next, we’ll check whether the current line contains our query string.  Fortunately, strings have a helpful method named contains that does this for us!  Add a call to the contains method in the search function, as shown in Listing 12-18.  Note this still won’t compile yet.   // Filename: src/lib.rs pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {     for line in contents.lines() {         if line.contains(query) {             // do something with line         }     } }  Listing 12-18: Adding functionality to see whether the line contains the string in query   Storing Matching Lines  We also need a way to store the lines that contain our query string.  For that, we can make a mutable vector before the for loop and call the push method to store a line in the vector.  After the for loop, we return the vector, as shown in Listing 12-19.   // Filename: src/lib.rs pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {     let mut results = Vec::new();      for line in contents.lines() {         if line.contains(query) {             results.push(line);         }     }      results }  Listing 12-19: Storing the lines that match so we can return them   Now the search function should return only the lines that contain query, and our test should pass.  Let’s run the test:  $ cargo test --snip-- running 1 test test tests::one_result ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  Our test passed, so we know it works!   At this point, we could consider opportunities for refactoring the implementation of the search function while keeping the tests passing to maintain the same functionality.  The code in the search function isn’t too bad, but it doesn’t take advantage of some useful features of iterators.  We’ll return to this example in Chapter 13, where we’ll explore iterators in detail, and look at how to improve it.   Using the search Function in the run Function  Now that the search function is working and tested, we need to call search from our run function.  We need to pass the config.query value and the contents that run reads from the file to the search function. Then run will print each line returned from search:   // Filename: src/lib.rs pub fn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     let contents = fs::read_to_string(config.filename)?;      for line in search(&amp;config.query, &amp;contents) {         println!(\"{}\", line);     }      Ok(()) }  We’re still using a for loop to return each line from search and print it.   Now the entire program should work! Let’s try it out, first with a word that should return exactly one line from the Emily Dickinson poem, “frog”:   $ cargo run frog poem.txt    Compiling minigrep v0.1.0 (file:///projects/minigrep)     Finished dev [unoptimized + debuginfo] target(s) in 0.38 secs      Running `target/debug/minigrep frog poem.txt` How public, like a frog   Cool! Now let’s try a word that will match multiple lines, like “body”:   $ cargo run body poem.txt     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep body poem.txt` I’m nobody! Who are you? Are you nobody, too? How dreary to be somebody!   And finally, let’s make sure that we don’t get any lines when we search for a word that isn’t anywhere in the poem, such as “monomorphization”:   $ cargo run monomorphization poem.txt     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep monomorphization poem.txt`  Excellent! We’ve built our own mini version of a classic tool and learned a lot about how to structure applications.  We’ve also learned a bit about file input and output, lifetimes, testing, and command line parsing.   To round out this project, we’ll briefly demonstrate how to work with environment variables and how to print to standard error, both of which are useful when you’re writing command line programs.   Working with Environment Variables   We’ll improve minigrep by adding an extra feature: an option for case-insensitive searching that the user can turn on via an environment variable.  We could make this feature a command line option and require that users enter it each time they want it to apply, but instead we’ll use an environment variable.  Doing so allows our users to set the environment variable once and have all their searches be case insensitive in that terminal session.   Writing a Failing Test for the Case-Insensitive search Function  We want to add a new search_case_insensitive function that we’ll call when the environment variable is on.  We’ll continue to follow the TDD process, so the first step is again to write a failing test.  We’ll add a new test for the new search_case_insensitive function and rename our old test from one_result to case_sensitive to clarify the differences between the two tests, as shown in Listing 12-20.   // Filename: src/lib.rs #[cfg(test)] mod tests {     use super::*;      #[test]     fn case_sensitive() {         let query = \"duct\";         let contents = \"\\ Rust: safe, fast, productive. Pick three. Duct tape.\";          assert_eq!(             vec![\"safe, fast, productive.\"],             search(query, contents)         );     }      #[test]     fn case_insensitive() {         let query = \"rUsT\";         let contents = \"\\ Rust: safe, fast, productive. Pick three. Trust me.\";          assert_eq!(             vec![\"Rust:\", \"Trust me.\"],             search_case_insensitive(query, contents)         );     } }  Listing 12-20: Adding a new failing test for the case-insensitive function we’re about to add   Note that we’ve edited the old test’s contents too.  We’ve added a new line with the text \"Duct tape.\" using a capital D that shouldn’t match the query \"duct\" when we’re searching in a case-sensitive manner.  Changing the old test in this way helps ensure that we don’t accidentally break the case-sensitive search functionality that we’ve already implemented.  This test should pass now and should continue to pass as we work on the case-insensitive search.   The new test for the case-insensitive search uses \"rUsT\" as its query.  In the search_case_insensitive function we’re about to add, the query \"rUsT\" should match the line containing \"Rust:\" with a capital R and match the line \"Trust me.\"  even though both have different casing from the query.  This is our failing test, and it will fail to compile because we haven’t yet defined the search_case_insensitive function.  Feel free to add a skeleton implementation that always returns an empty vector, similar to the way we did for the search function in Listing 12-16 to see the test compile and fail.   Implementing the search_case_insensitive Function   The search_case_insensitive function, shown in Listing 12-21, will be almost the same as the search function.  The only difference is that we’ll lowercase the query and each line so whatever the case of the input arguments, they’ll be the same case when we check whether the line contains the query.   // Filename: src/lib.rs pub fn search_case_insensitive&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {     let query = query.to_lowercase();     let mut results = Vec::new();      for line in contents.lines() {         if line.to_lowercase().contains(&amp;query) {             results.push(line);         }     }      results }  Listing 12-21: Defining the search_case_insensitive function to lowercase the query and the line before comparing them   First, we lowercase the query string and store it in a shadowed variable with the same name.  Calling to_lowercase on the query is necessary so no matter whether the user’s query is \"rust\", \"RUST\", \"Rust\", or \"rUsT\", we’ll treat the query as if it were \"rust\" and be insensitive to the case.   Note that query is now a String rather than a string slice, because calling to_lowercase creates new data rather than referencing existing data.  Say the query is \"rUsT\", as an example: that string slice doesn’t contain a lowercase u or t for us to use, so we have to allocate a new String containing \"rust\".  When we pass query as an argument to the contains method now, we need to add an ampersand(&amp;) because the signature of contains is defined to take a string slice.   Next, we add a call to to_lowercase on each line before we check whether it contains query to lowercase all characters.  Now that we’ve converted line and query to lowercase, we’ll find matches no matter what the case of the query is.   Let’s see if this implementation passes the tests:   running 2 tests test tests::case_insensitive ... ok test tests::case_sensitive ... ok  test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out   Great! They passed. Now, let’s call the new search_case_insensitive function from the run function.  First, we’ll add a configuration option to the Config struct to switch between case-sensitive and case-insensitive search. Adding this field will cause compiler errors because we aren’t initializing this field anywhere yet:   // Filename: src/lib.rs pub struct Config {     pub query: String,     pub filename: String,     pub case_sensitive: bool, }  Note that we added the case_sensitive field that holds a Boolean.  Next, we need the run function to check the case_sensitive field’s value and use that to decide whether to call the search function or the search_case_insensitive function, as shown in Listing 12-22. Note this still won’t compile yet.   // Filename: src/lib.rs  pub fn run(config: Config) -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     let contents = fs::read_to_string(config.filename)?;      let results = if config.case_sensitive {         search(&amp;config.query, &amp;contents)     } else {         search_case_insensitive(&amp;config.query, &amp;contents)     };      for line in results {         println!(\"{}\", line);     }      Ok(()) }   Listing 12-22: Calling either search or search_case_insensitive based on the value in config.case_sensitive   Finally, we need to check for the environment variable.  The functions for working with environment variables are in the env module in the standard library, so we want to bring that module into scope with a use std::env; line at the top of src/lib.rs.  Then we’ll use the var function from the env module to check for an environment variable named CASE_INSENSITIVE, as shown in Listing 12-23.   // Filename: src/lib.rs  use std::env;  // --snip--  impl Config {     pub fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;'static str&gt; {         if args.len() &lt; 3 {             return Err(\"not enough arguments\");         }          let query = args[1].clone();         let filename = args[2].clone();          let case_sensitive = env::var(\"CASE_INSENSITIVE\").is_err();          Ok(Config { query, filename, case_sensitive })     } }   Listing 12-23: Checking for an environment variable named CASE_INSENSITIVE   Here, we create a new variable case_sensitive.  To set its value, we call the env::var function and pass it the name of the CASE_INSENSITIVE environment variable.  The env::var function returns a Result that will be the successful Ok variant that contains the value of the environment variable if the environment variable is set.  It will return the Err variant if the environment variable is not set.   We’re using the is_err method on the Result to check whether it’s an error and therefore unset, which means it should do a case-sensitive search.  If the CASE_INSENSITIVE environment variable is set to anything, is_err will return false and the program will perform a case-insensitive search.  We don’t care about the value of the environment variable, just whether it’s set or unset, so we’re checking is_err rather than using unwrap, expect, or any of the other methods we’ve seen on Result.   We pass the value in the case_sensitive variable to the Config instance so the run function can read that value and decide whether to call search or search_case_insensitive, as we implemented in Listing 12-22.   Let’s give it a try! First, we’ll run our program without the environment variable set and with the query to, which should match any line that contains the word “to” in all lowercase:   $ cargo run to poem.txt    Compiling minigrep v0.1.0 (file:///projects/minigrep)     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep to poem.txt` Are you nobody, too? How dreary to be somebody!   Looks like that still works! Now, let’s run the program with CASE_INSENSITIVE set to 1 but with the same query to.   If you’re using PowerShell, you will need to set the environment variable and run the program in two commands rather than one:  $ $env:CASE_INSENSITIVE=1 $ cargo run to poem.txt   We should get lines that contain “to” that might have uppercase letters:   $ CASE_INSENSITIVE=1 cargo run to poem.txt     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/minigrep to poem.txt` Are you nobody, too? How dreary to be somebody! To tell your name the livelong day To an admiring bog!   Excellent, we also got lines containing “To”! Our minigrep program can now do case-insensitive searching controlled by an environment variable.  Now you know how to manage options set using either command line arguments or environment variables.   Some programs allow arguments and environment variables for the same configuration.  In those cases, the programs decide that one or the other takes precedence.  For another exercise on your own, try controlling case insensitivity through either a command line argument or an environment variable.  Decide whether the command line argument or the environment variable should take precedence if the program is run with one set to case sensitive and one set to case insensitive.   The std::env module contains many more useful features for dealing with environment variables: check out its documentation to see what is available.   Writing Error Messages to Standard Error Instead of Standard Output  At the moment, we’re writing all of our output to the terminal using the println! function.  Most terminals provide two kinds of output: standard output (stdout) for general information and standard error (stderr) for error messages.  This distinction enables users to choose to direct the successful output of a program to a file but still print error messages to the screen.   The println! function is only capable of printing to standard output, so we have to use something else to print to standard error.   Checking Where Errors Are Written  First, let’s observe how the content printed by minigrep is currently being written to standard output, including any error messages we want to write to standard error instead.  We’ll do that by redirecting the standard output stream to a file while also intentionally causing an error.  We won’t redirect the standard error stream, so any content sent to standard error will continue to display on the screen.   Command line programs are expected to send error messages to the standard error stream so we can still see error messages on the screen even if we redirect the standard output stream to a file.  Our program is not currently well-behaved: we’re about to see that it saves the error message output to a file instead!   The way to demonstrate this behavior is by running the program with &gt; and the filename, output.txt, that we want to redirect the standard output stream to. We won’t pass any arguments, which should cause an error:   $ cargo run &gt; output.txt  The &gt; syntax tells the shell to write the contents of standard output to output.txt instead of the screen.  We didn’t see the error message we were expecting printed to the screen, so that means it must have ended up in the file. This is what output.txt contains:   Problem parsing arguments: not enough arguments  Yup, our error message is being printed to standard output.  It’s much more useful for error messages like this to be printed to standard error so only data from a successful run ends up in the file. We’ll change that.   Printing Errors to Standard Error  We’ll use the code in Listing 12-24 to change how error messages are printed.  Because of the refactoring we did earlier in this chapter, all the code that prints error messages is in one function, main.  The standard library provides the eprintln! macro that prints to the standard error stream, so let’s change the two places we were calling println! to print errors to use eprintln! instead.   // Filename: src/main.rs  fn main() {     let args: Vec&lt;String&gt; = env::args().collect();      let config = Config::new(&amp;args).unwrap_or_else(|err| {         eprintln!(\"Problem parsing arguments: {}\", err);         process::exit(1);     });      if let Err(e) = minigrep::run(config) {         eprintln!(\"Application error: {}\", e);          process::exit(1);     } }  Listing 12-24: Writing error messages to standard error instead of standard output using eprintln!   After changing println! to eprintln!, let’s run the program again in the same way, without any arguments and redirecting standard output with &gt;:   $ cargo run &gt; output.txt Problem parsing arguments: not enough arguments  Now we see the error onscreen and output.txt contains nothing, which is the behavior we expect of command line programs.   Let’s run the program again with arguments that don’t cause an error but still redirect standard output to a file, like so:   $ cargo run to poem.txt &gt; output.txt  We won’t see any output to the terminal, and output.txt will contain our results:   // Filename: output.txt  Are you nobody, too? How dreary to be somebody!  This demonstrates that we’re now using standard output for successful output and standard error for error output as  appropriate.   Summary  This chapter recapped some of the major concepts you’ve learned so far and covered how to perform common I/O operations in Rust.  By using command line arguments, files, environment variables, and the eprintln! macro for printing errors, you’re now prepared to write command line applications.  By using the concepts in previous chapters, your code will be well organized, store data effectively in the appropriate data structures, handle errors nicely, and be well tested.  ","categories": ["RUST Language"],
        "tags": ["Project","Summury","Practice"],
        "url": "https://jjungs-lee.github.io//rust/12.An-IO-Project-Building-a-Command-Line-Program",
        "teaser":null},{
        "title": "RUST : 13. Functional Language Features: Iterators and Closures",
        "excerpt":"Functional Language Features: Iterators and Closures  Rust’s design has taken inspiration from many existing languages and techniques, and one significant influence is functional programming.  Programming in a functional style often includes using functions as values by passing them in arguments, returning them from other functions, assigning them to variables for later execution, and so forth.   In this chapter, we won’t debate the issue of what functional programming is or isn’t but will instead discuss some features of Rust that are similar to features in many languages often referred to as functional.   More specifically, we’ll cover:      Closures, a function-like construct you can store in a variable   Iterators, a way of processing a series of elements   How to use these two features to improve the I/O project in Chapter 12   The performance of these two features (Spoiler alert: they’re faster than you might think!)   Other Rust features, such as pattern matching and enums, which we’ve covered in other chapters, are influenced by the functional style as well. Mastering closures and iterators is an important part of writing idiomatic, fast Rust code, so we’ll devote this entire chapter to them.   Closures: Anonymous Functions that Can Capture Their Environment  Rust’s closures are anonymous functions you can save in a variable or pass as arguments to other functions.  You can create the closure in one place and then call the closure to evaluate it in a different context.  Unlike functions, closures can capture values from the scope in which they’re defined.  We’ll demonstrate how these closure features allow for code reuse and behavior customization.   Creating an Abstraction of Behavior with Closures  Let’s work on an example of a situation in which it’s useful to store a closure to be executed later.  Along the way, we’ll talk about the syntax of closures, type inference, and traits.   Consider this hypothetical situation: we work at a startup that’s making an app to generate custom exercise workout plans.  The backend is written in Rust, and the algorithm that generates the workout plan takes into account many factors, such as the app user’s age, body mass index, exercise preferences, recent workouts, and an intensity number they specify.  The actual algorithm used isn’t important in this example; what’s important is that this calculation takes a few seconds.  We want to call this algorithm only when we need to and only call it once so we don’t make the user wait more than necessary.   We’ll simulate calling this hypothetical algorithm with the function simulated_expensive_calculation shown in Listing 13-1, which will print calculating slowly..., wait for two seconds, and then return whatever number we passed in.   // Filename: src/main.rs  use std::thread; use std::time::Duration;  fn simulated_expensive_calculation(intensity: u32) -&gt; u32 {     println!(\"calculating slowly...\");     thread::sleep(Duration::from_secs(2));     intensity }  Listing 13-1: A function to stand in for a hypothetical calculation that takes about 2 seconds to run   Next is the main function, which contains the parts of the workout app important for this example. This function represents the code that the app will call when a user asks for a workout plan. Because the interaction with the app’s frontend isn’t relevant to the use of closures, we’ll hardcode values representing inputs to our program and print the outputs.   The required inputs are these:     An intensity number from the user, which is specified when they request a workout to indicate whether they want a low-intensity workout or a high-intensity workout   A random number that will generate some variety in the workout plans The output will be the recommended workout plan. Listing 13-2 shows the main function we’ll use.   // Filename: src/main.rs  fn main() {     let simulated_user_specified_value = 10;     let simulated_random_number = 7;      generate_workout(         simulated_user_specified_value,         simulated_random_number     ); }  Listing 13-2: A main function with hardcoded values to simulate user input and random number generation   We’ve hardcoded the variable simulated_user_specified_value as 10 and the variable simulated_random_number as 7 for simplicity’s sake; in an actual program, we’d get the intensity number from the app frontend, and we’d use the rand crate to generate a random number, as we did in the Guessing Game example in Chapter 2. The main function calls a generate_workout function with the simulated input values.   Now that we have the context, let’s get to the algorithm. The function generate_workout in Listing 13-3 contains the business logic of the app that we’re most concerned with in this example. The rest of the code changes in this example will be made to this function.   // Filename: src/main.rs  fn generate_workout(intensity: u32, random_number: u32) {     if intensity &lt; 25 {         println!(             \"Today, do {} pushups!\",             simulated_expensive_calculation(intensity)         );         println!(             \"Next, do {} situps!\",             simulated_expensive_calculation(intensity)         );     } else {         if random_number == 3 {             println!(\"Take a break today! Remember to stay hydrated!\");         } else {             println!(                 \"Today, run for {} minutes!\",                 simulated_expensive_calculation(intensity)             );         }     } }  Listing 13-3: The business logic that prints the workout plans based on the inputs and calls to the simulated_expensive_calculation function   The code in Listing 13-3 has multiple calls to the slow calculation function.  The first if block calls simulated_expensive_calculation twice, the if inside the outer else doesn’t call it at all, and the code inside the second else case calls it once.   The desired behavior of the generate_workout function is to first check whether the user wants a low-intensity workout (indicated by a number less than 25) or a high-intensity workout (a number of 25 or greater).   Low-intensity workout plans will recommend a number of push-ups and sit-ups based on the complex algorithm we’re simulating.   If the user wants a high-intensity workout, there’s some additional logic: if the value of the random number generated by the app happens to be 3, the app will recommend a break and hydration. If not, the user will get a number of minutes of running based on the complex algorithm.   This code works the way the business wants it to now, but let’s say the data science team decides that we need to make some changes to the way we call the simulated_expensive_calculation function in the future.  To simplify the update when those changes happen, we want to refactor this code so it calls the simulated_expensive_calculation function only once.  We also want to cut the place where we’re currently unnecessarily calling the function twice without adding any other calls to that function in the process. That is, we don’t want to call it if the result isn’t needed, and we still want to call it only once.   Refactoring Using Functions  We could restructure the workout program in many ways. First, we’ll try extracting the duplicated call to the simulated_expensive_calculation function into a variable, as shown in Listing 13-4.   // Filename: src/main.rs  fn generate_workout(intensity: u32, random_number: u32) {     let expensive_result =         simulated_expensive_calculation(intensity);      if intensity &lt; 25 {         println!(             \"Today, do {} pushups!\",             expensive_result         );         println!(             \"Next, do {} situps!\",             expensive_result         );     } else {         if random_number == 3 {             println!(\"Take a break today! Remember to stay hydrated!\");         } else {             println!(                 \"Today, run for {} minutes!\",                 expensive_result             );         }     } }  Listing 13-4: Extracting the calls to simulated_expensive_calculation to one place and storing the result in the expensive_result variable   This change unifies all the calls to simulated_expensive_calculation and solves the problem of the first if block unnecessarily calling the function twice. Unfortunately, we’re now calling this function and waiting for the result in all cases, which includes the inner if block that doesn’t use the result value at all.   We want to define code in one place in our program, but only execute that code where we actually need the result. This is a use case for closures!   Refactoring with Closures to Store Code  Instead of always calling the simulated_expensive_calculation function before the if blocks, we can define a closure and store the closure in a variable rather than storing the result of the function call, as shown in Listing 13-5. We can actually move the whole body of simulated_expensive_calculation within the closure we’re introducing here.  // Filename: src/main.rs  let expensive_closure = |num| {     println!(\"calculating slowly...\");     thread::sleep(Duration::from_secs(2));     num };  Listing 13-5: Defining a closure and storing it in the expensive_closure variable   The closure definition comes after the = to assign it to the variable expensive_closure.  To define a closure, we start with a pair of vertical pipes (|), inside which we specify the parameters to the closure; this syntax was chosen because of its similarity to closure definitions in Smalltalk and Ruby.  This closure has one parameter named num: if we had more than one parameter, we would separate them with commas, like |param1, param2|.   After the parameters, we place curly brackets that hold the body of the closure—these are optional if the closure body is a single expression.  The end of the closure, after the curly brackets, needs a semicolon to complete the let statement.  The value returned from the last line in the closure body (num) will be the value returned from the closure when it’s called, because that line doesn’t end in a semicolon; just as in function bodies.   Note that this let statement means expensive_closure contains the definition of an anonymous function, not the resulting value of calling the anonymous function.  Recall that we’re using a closure because we want to define the code to call at one point, store that code, and call it at a later point; the code we want to call is now stored in expensive_closure.   With the closure defined, we can change the code in the if blocks to call the closure to execute the code and get the resulting value. We call a closure like we do a function: we specify the variable name that holds the closure definition and follow it with parentheses containing the argument values we want to use, as shown in Listing 13-6.   // Filename: src/main.rs  fn generate_workout(intensity: u32, random_number: u32) {     let expensive_closure = |num| {         println!(\"calculating slowly...\");         thread::sleep(Duration::from_secs(2));         num     };      if intensity &lt; 25 {         println!(             \"Today, do {} pushups!\",             expensive_closure(intensity)         );         println!(             \"Next, do {} situps!\",             expensive_closure(intensity)         );     } else {         if random_number == 3 {             println!(\"Take a break today! Remember to stay hydrated!\");         } else {             println!(                 \"Today, run for {} minutes!\",                 expensive_closure(intensity)             );         }     } }  Listing 13-6: Calling the expensive_closure we’ve defined   Now the expensive calculation is called in only one place, and we’re only executing that code where we need the results.   However, we’ve reintroduced one of the problems from Listing 13-3: we’re still calling the closure twice in the first if block, which will call the expensive code twice and make the user wait twice as long as they need to. We could fix this problem by creating a variable local to that if block to hold the result of calling the closure, but closures provide us with another solution. We’ll talk about that solution in a bit. But first let’s talk about why there aren’t type annotations in the closure definition and the traits involved with closures.   Closure Type Inference and Annotation  Closures don’t require you to annotate the types of the parameters or the return value like fn functions do. Type annotations are required on functions because they’re part of an explicit interface exposed to your users. Defining this interface rigidly is important for ensuring that everyone agrees on what types of values a function uses and returns. But closures aren’t used in an exposed interface like this: they’re stored in variables and used without naming them and exposing them to users of our library.   Closures are usually short and relevant only within a narrow context rather than in any arbitrary scenario. Within these limited contexts, the compiler is reliably able to infer the types of the parameters and the return type, similar to how it’s able to infer the types of most variables.   Making programmers annotate the types in these small, anonymous functions would be annoying and largely redundant with the information the compiler already has available.   As with variables, we can add type annotations if we want to increase explicitness and clarity at the cost of being more verbose than is strictly necessary. Annotating the types for the closure we defined in Listing 13-5 would look like the definition shown in Listing 13-7.   // Filename: src/main.rs  let expensive_closure = |num: u32| -&gt; u32 {     println!(\"calculating slowly...\");     thread::sleep(Duration::from_secs(2));     num };  Listing 13-7: Adding optional type annotations of the parameter and return value types in the closure   With type annotations added, the syntax of closures looks more similar to the syntax of functions. The following is a vertical comparison of the syntax for the definition of a function that adds 1 to its parameter and a closure that has the same behavior. We’ve added some spaces to line up the relevant parts. This illustrates how closure syntax is similar to function syntax except for the use of pipes and the amount of syntax that is optional:   fn  add_one_v1   (x: u32) -&gt; u32 { x + 1 } let add_one_v2 = |x: u32| -&gt; u32 { x + 1 }; let add_one_v3 = |x|             { x + 1 }; let add_one_v4 = |x|               x + 1  ;  The first line shows a function definition, and the second line shows a fully annotated closure definition. The third line removes the type annotations from the closure definition, and the fourth line removes the brackets, which are optional because the closure body has only one expression. These are all valid definitions that will produce the same behavior when they’re called.   Closure definitions will have one concrete type inferred for each of their parameters and for their return value. For instance, Listing 13-8 shows the definition of a short closure that just returns the value it receives as a parameter. This closure isn’t very useful except for the purposes of this example. Note that we haven’t added any type annotations to the definition: if we then try to call the closure twice, using a String as an argument the first time and a u32 the second time, we’ll get an error.   // Filename: src/main.rs  let example_closure = |x| x;  let s = example_closure(String::from(\"hello\")); let n = example_closure(5);  Listing 13-8: Attempting to call a closure whose types are inferred with two different types   The compiler gives us this error:   error[E0308]: mismatched types  --&gt; src/main.rs   |   | let n = example_closure(5);   |                         ^ expected struct `std::string::String`, found   integer   |   = note: expected type `std::string::String`              found type `{integer}`  The first time we call example_closure with the String value, the compiler infers the type of x and the return type of the closure to be String. Those types are then locked in to the closure in example_closure, and we get a type error if we try to use a different type with the same closure.   Storing Closures Using Generic Parameters and the Fn Traits  Let’s return to our workout generation app. In Listing 13-6, our code was still calling the expensive calculation closure more times than it needed to. One option to solve this issue is to save the result of the expensive closure in a variable for reuse and use the variable in each place we need the result, instead of calling the closure again. However, this method could result in a lot of repeated code.   Fortunately, another solution is available to us. We can create a struct that will hold the closure and the resulting value of calling the closure. The struct will execute the closure only if we need the resulting value, and it will cache the resulting value so the rest of our code doesn’t have to be responsible for saving and reusing the result. You may know this pattern as memoization or lazy evaluation.   To make a struct that holds a closure, we need to specify the type of the closure, because a struct definition needs to know the types of each of its fields. Each closure instance has its own unique anonymous type: that is, even if two closures have the same signature, their types are still considered different. To define structs, enums, or function parameters that use closures, we use generics and trait bounds, as we discussed in Chapter 10.   The Fn traits are provided by the standard library. All closures implement at least one of the traits: Fn, FnMut, or FnOnce. We’ll discuss the difference between these traits in the “Capturing the Environment with Closures” section; in this example, we can use the Fn trait.   We add types to the Fn trait bound to represent the types of the parameters and return values the closures must have to match this trait bound. In this case, our closure has a parameter of type u32 and returns a u32, so the trait bound we specify is Fn(u32) -&gt; u32.   Listing 13-9 shows the definition of the Cacher struct that holds a closure and an optional result value.   // Filename: src/main.rs  struct Cacher&lt;T&gt;     where T: Fn(u32) -&gt; u32 {     calculation: T,     value: Option&lt;u32&gt;, }  Listing 13-9: Defining a Cacher struct that holds a closure in calculation and an optional result in value   The Cacher struct has a calculation field of the generic type T. The trait bounds on T specify that it’s a closure by using the Fn trait. Any closure we want to store in the calculation field must have one u32 parameter (specified within the parentheses after Fn) and must return a u32 (specified after the -&gt;).   Note: Functions can implement all three of the Fn traits too. If what we want to do doesn’t require capturing a value from the environment, we can use a function rather than a closure where we need something that implements an Fn trait.   The value field is of type Option&lt;u32&gt;. Before we execute the closure, value will be None. When code using a Cacher asks for the result of the closure, the Cacher will execute the closure at that time and store the result within a Some variant in the value field. Then if the code asks for the result of the closure again, instead of executing the closure again, the Cacher will return the result held in the Some variant.   The logic around the value field we’ve just described is defined in Listing 13-10.   //Filename: src/main.rs  impl&lt;T&gt; Cacher&lt;T&gt;     where T: Fn(u32) -&gt; u32 {     fn new(calculation: T) -&gt; Cacher&lt;T&gt; {         Cacher {             calculation,             value: None,         }     }      fn value(&amp;mut self, arg: u32) -&gt; u32 {         match self.value {             Some(v) =&gt; v,             None =&gt; {                 let v = (self.calculation)(arg);                 self.value = Some(v);                 v             },         }     } }  Listing 13-10: The caching logic of Cacher   We want Cacher to manage the struct fields’ values rather than letting the calling code potentially change the values in these fields directly, so these fields are private.   The Cacher::new function takes a generic parameter T, which we’ve defined as having the same trait bound as the Cacher struct. Then Cacher::new returns a Cacher instance that holds the closure specified in the calculation field and a None value in the value field, because we haven’t executed the closure yet.   When the calling code needs the result of evaluating the closure, instead of calling the closure directly, it will call the value method. This method checks whether we already have a resulting value in self.value in a Some; if we do, it returns the value within the Some without executing the closure again.   If self.value is None, the code calls the closure stored in self.calculation, saves the result in self.value for future use, and returns the value as well.   Listing 13-11 shows how we can use this Cacher struct in the function generate_workout from Listing 13-6.   // Filename: src/main.rs  fn generate_workout(intensity: u32, random_number: u32) {     let mut expensive_result = Cacher::new(|num| {         println!(\"calculating slowly...\");         thread::sleep(Duration::from_secs(2));         num     });      if intensity &lt; 25 {         println!(             \"Today, do {} pushups!\",             expensive_result.value(intensity)         );         println!(             \"Next, do {} situps!\",             expensive_result.value(intensity)         );     } else {         if random_number == 3 {             println!(\"Take a break today! Remember to stay hydrated!\");         } else {             println!(                 \"Today, run for {} minutes!\",                 expensive_result.value(intensity)             );         }     } }  Listing 13-11: Using Cacher in the generate_workout function to abstract away the caching logic   Instead of saving the closure in a variable directly, we save a new instance of Cacher that holds the closure. Then, in each place we want the result, we call the value method on the Cacher instance. We can call the value method as many times as we want, or not call it at all, and the expensive calculation will be run a maximum of once.   Try running this program with the main function from Listing 13-2. Change the values in the simulated_user_specified_value and simulated_random_number variables to verify that in all the cases in the various if and else blocks, calculating slowly... appears only once and only when needed. The Cacher takes care of the logic necessary to ensure we aren’t calling the expensive calculation more than we need to so generate_workout can focus on the business logic.   Limitations of the Cacher Implementation  Caching values is a generally useful behavior that we might want to use in other parts of our code with different closures. However, there are two problems with the current implementation of Cacher that would make reusing it in different contexts difficult.   The first problem is that a Cacher instance assumes it will always get the same value for the parameter arg to the value method. That is, this test of Cacher will fail:   #[test] fn call_with_different_values() {     let mut c = Cacher::new(|a| a);      let v1 = c.value(1);     let v2 = c.value(2);      assert_eq!(v2, 2); }  This test creates a new Cacher instance with a closure that returns the value passed into it. We call the value method on this Cacher instance with an arg value of 1 and then an arg value of 2, and we expect the call to value with the arg value of 2 to return 2.   Run this test with the Cacher implementation in Listing 13-9 and Listing 13-10, and the test will fail on the assert_eq! with this message:   thread 'call_with_different_values' panicked at 'assertion failed: `(left == right)`   left: `1`,  right: `2`', src/main.rs   The problem is that the first time we called c.value with 1, the Cacher instance saved Some(1) in self.value. Thereafter, no matter what we pass in to the value method, it will always return 1.   Try modifying Cacher to hold a hash map rather than a single value. The keys of the hash map will be the arg values that are passed in, and the values of the hash map will be the result of calling the closure on that key. Instead of looking at whether self.value directly has a Some or a None value, the value function will look up the arg in the hash map and return the value if it’s present. If it’s not present, the Cacher will call the closure and save the resulting value in the hash map associated with its arg value.   The second problem with the current Cacher implementation is that it only accepts closures that take one parameter of type u32 and return a u32. We might want to cache the results of closures that take a string slice and return usize values, for example. To fix this issue, try introducing more generic parameters to increase the flexibility of the Cacher functionality.   Capturing the Environment with Closures  In the workout generator example, we only used closures as inline anonymous functions. However, closures have an additional capability that functions don’t have: they can capture their environment and access variables from the scope in which they’re defined.   Listing 13-12 has an example of a closure stored in the equal_to_x variable that uses the x variable from the closure’s surrounding environment.   // Filename: src/main.rs  fn main() {     let x = 4;      let equal_to_x = |z| z == x;      let y = 4;      assert!(equal_to_x(y)); }  Listing 13-12: Example of a closure that refers to a variable in its enclosing scope   Here, even though x is not one of the parameters of equal_to_x, the equal_to_x closure is allowed to use the x variable that’s defined in the same scope that equal_to_x is defined in.   We can’t do the same with functions; if we try with the following example, our code won’t compile:   // Filename: src/main.rs  fn main() {     let x = 4;      fn equal_to_x(z: i32) -&gt; bool { z == x }      let y = 4;      assert!(equal_to_x(y)); }  We get an error:   error[E0434]: can't capture dynamic environment in a fn item; use the || { ... } closure form instead  --&gt; src/main.rs   | 4 |     fn equal_to_x(z: i32) -&gt; bool { z == x }   |                                          ^  The compiler even reminds us that this only works with closures!   When a closure captures a value from its environment, it uses memory to store the values for use in the closure body. This use of memory is overhead that we don’t want to pay in more common cases where we want to execute code that doesn’t capture its environment. Because functions are never allowed to capture their environment, defining and using functions will never incur this overhead.   Closures can capture values from their environment in three ways, which directly map to the three ways a function can take a parameter: taking ownership, borrowing mutably, and borrowing immutably. These are encoded in the three Fn traits as follows:      FnOnce consumes the variables it captures from its enclosing scope, known as the closure’s environment. To consume the captured variables, the closure must take ownership of these variables and move them into the closure when it is defined. The Once part of the name represents the fact that the closure can’t take ownership of the same variables more than once, so it can be called only once.   FnMut can change the environment because it mutably borrows values.   Fn borrows values from the environment immutably.   When you create a closure, Rust infers which trait to use based on how the closure uses the values from the environment. All closures implement FnOnce because they can all be called at least once. Closures that don’t move the captured variables also implement FnMut, and closures that don’t need mutable access to the captured variables also implement Fn. In Listing 13-12, the equal_to_x closure borrows x immutably (so equal_to_x has the Fn trait) because the body of the closure only needs to read the value in x.   If you want to force the closure to take ownership of the values it uses in the environment, you can use the move keyword before the parameter list. This technique is mostly useful when passing a closure to a new thread to move the data so it’s owned by the new thread.   We’ll have more examples of move closures in Chapter 16 when we talk about concurrency. For now, here’s the code from Listing 13-12 with the move keyword added to the closure definition and using vectors instead of integers, because integers can be copied rather than moved; note that this code will not yet compile.   // Filename: src/main.rs  fn main() {     let x = vec![1, 2, 3];      let equal_to_x = move |z| z == x;      println!(\"can't use x here: {:?}\", x);      let y = vec![1, 2, 3];      assert!(equal_to_x(y)); }  We receive the following error:   error[E0382]: use of moved value: `x`  --&gt; src/main.rs:6:40   | 4 |     let equal_to_x = move |z| z == x;   |                      -------- value moved (into closure) here 5 | 6 |     println!(\"can't use x here: {:?}\", x);   |                                        ^ value used here after move   |U   = note: move occurs because `x` has type `std::vec::Vec&lt;i32&gt;`, which does not   implement the `Copy` trait    The x value is moved into the closure when the closure is defined, because we added the move keyword. The closure then has ownership of x, and main isn’t allowed to use x anymore in the println! statement. Removing println! will fix this example.   Most of the time when specifying one of the Fn trait bounds, you can start with Fn and the compiler will tell you if you need FnMut or FnOnce based on what happens in the closure body.   To illustrate situations where closures that can capture their environment are useful as function parameters, let’s move on to our next topic: iterators.   Processing a Series of Items with Iterators  The iterator pattern allows you to perform some task on a sequence of items in turn. An iterator is responsible for the logic of iterating over each item and determining when the sequence has finished. When you use iterators, you don’t have to reimplement that logic yourself.   In Rust, iterators are lazy, meaning they have no effect until you call methods that consume the iterator to use it up. For example, the code in Listing 13-13 creates an iterator over the items in the vector v1 by calling the iter method defined on Vec&lt;T&gt;. This code by itself doesn’t do anything useful.   let v1 = vec![1, 2, 3];  let v1_iter = v1.iter();  Listing 13-13: Creating an iterator   Once we’ve created an iterator, we can use it in a variety of ways. In Listing 3-5 in Chapter 3, we used iterators with for loops to execute some code on each item, although we glossed over what the call to iter did until now.   The example in Listing 13-14 separates the creation of the iterator from the use of the iterator in the for loop. The iterator is stored in the v1_iter variable, and no iteration takes place at that time. When the for loop is called using the iterator in v1_iter, each element in the iterator is used in one iteration of the loop, which prints out each value.   let v1 = vec![1, 2, 3];  let v1_iter = v1.iter();  for val in v1_iter {     println!(\"Got: {}\", val); }  Listing 13-14: Using an iterator in a for loop   In languages that don’t have iterators provided by their standard libraries, you would likely write this same functionality by starting a variable at index 0, using that variable to index into the vector to get a value, and incrementing the variable value in a loop until it reached the total number of items in the vector.   Iterators handle all that logic for you, cutting down on repetitive code you could potentially mess up. Iterators give you more flexibility to use the same logic with many different kinds of sequences, not just data structures you can index into, like vectors. Let’s examine how iterators do that.   The Iterator Trait and the next Method  All iterators implement a trait named Iterator that is defined in the standard library. The definition of the trait looks like this:   pub trait Iterator {     type Item;      fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;;      // methods with default implementations elided }  Notice this definition uses some new syntax: type Item and Self::Item, which are defining an associated type with this trait. We’ll talk about associated types in depth in Chapter 19. For now, all you need to know is that this code says implementing the Iterator trait requires that you also define an Item type, and this Item type is used in the return type of the next method. In other words, the Item type will be the type returned from the iterator.   The Iterator trait only requires implementors to define one method: the next method, which returns one item of the iterator at a time wrapped in Some and, when iteration is over, returns None.   We can call the next method on iterators directly; Listing 13-15 demonstrates what values are returned from repeated calls to next on the iterator created from the vector.   // Filename: src/lib.rs  #[test] fn iterator_demonstration() {     let v1 = vec![1, 2, 3];      let mut v1_iter = v1.iter();      assert_eq!(v1_iter.next(), Some(&amp;1));     assert_eq!(v1_iter.next(), Some(&amp;2));     assert_eq!(v1_iter.next(), Some(&amp;3));     assert_eq!(v1_iter.next(), None); }  Listing 13-15: Calling the next method on an iterator   Note that we needed to make v1_iter mutable: calling the next method on an iterator changes internal state that the iterator uses to keep track of where it is in the sequence. In other words, this code consumes, or uses up, the iterator. Each call to next eats up an item from the iterator. We didn’t need to make v1_iter mutable when we used a for loop because the loop took ownership of v1_iter and made it mutable behind the scenes.   Also note that the values we get from the calls to next are immutable references to the values in the vector. The iter method produces an iterator over immutable references. If we want to create an iterator that takes ownership of v1 and returns owned values, we can call into_iter instead of iter. Similarly, if we want to iterate over mutable references, we can call iter_mut instead of iter.   Methods that Consume the Iterator  The Iterator trait has a number of different methods with default implementations provided by the standard library; you can find out about these methods by looking in the standard library API documentation for the Iterator trait. Some of these methods call the next method in their definition, which is why you’re required to implement the next method when implementing the Iterator trait.   Methods that call next are called consuming adaptors, because calling them uses up the iterator. One example is the sum method, which takes ownership of the iterator and iterates through the items by repeatedly calling next, thus consuming the iterator. As it iterates through, it adds each item to a running total and returns the total when iteration is complete. Listing 13-16 has a test illustrating a use of the sum method:   // Filename: src/lib.rs  #[test] fn iterator_sum() {     let v1 = vec![1, 2, 3];      let v1_iter = v1.iter();      let total: i32 = v1_iter.sum();      assert_eq!(total, 6); }  Listing 13-16: Calling the sum method to get the total of all items in the iterator  We aren’t allowed to use v1_iter after the call to sum because sum takes ownership of the iterator we call it on.   Methods that Produce Other Iterators  Other methods defined on the Iterator trait, known as iterator adaptors, allow you to change iterators into different kinds of iterators. You can chain multiple calls to iterator adaptors to perform complex actions in a readable way. But because all iterators are lazy, you have to call one of the consuming adaptor methods to get results from calls to iterator adaptors.   Listing 13-17 shows an example of calling the iterator adaptor method map, which takes a closure to call on each item to produce a new iterator. The closure here creates a new iterator in which each item from the vector has been incremented by 1. However, this code produces a warning:   // Filename: src/main.rs  let v1: Vec&lt;i32&gt; = vec![1, 2, 3];  v1.iter().map(|x| x + 1);  Listing 13-17: Calling the iterator adaptor map to create a new iterator   The warning we get is this:   warning: unused `std::iter::Map` which must be used: iterator adaptors are lazy and do nothing unless consumed  --&gt; src/main.rs:4:5   | 4 |     v1.iter().map(|x| x + 1);   |     ^^^^^^^^^^^^^^^^^^^^^^^^^   |   = note: #[warn(unused_must_use)] on by default  The code in Listing 13-17 doesn’t do anything; the closure we’ve specified never gets called. The warning reminds us why: iterator adaptors are lazy, and we need to consume the iterator here.   To fix this and consume the iterator, we’ll use the collect method, which we used in Chapter 12 with env::args in Listing 12-1. This method consumes the iterator and collects the resulting values into a collection data type.   In Listing 13-18, we collect the results of iterating over the iterator that’s returned from the call to map into a vector. This vector will end up containing each item from the original vector incremented by 1.   // Filename: src/main.rs  let v1: Vec&lt;i32&gt; = vec![1, 2, 3];  let v2: Vec&lt;_&gt; = v1.iter().map(|x| x + 1).collect();  assert_eq!(v2, vec![2, 3, 4]);  Listing 13-18: Calling the map method to create a new iterator and then calling the collect method to consume the new iterator and create a vector   Because map takes a closure, we can specify any operation we want to perform on each item. This is a great example of how closures let you customize some behavior while reusing the iteration behavior that the Iterator trait provides.   Using Closures that Capture Their Environment  Now that we’ve introduced iterators, we can demonstrate a common use of closures that capture their environment by using the filter iterator adaptor. The filter method on an iterator takes a closure that takes each item from the iterator and returns a Boolean. If the closure returns true, the value will be included in the iterator produced by filter. If the closure returns false, the value won’t be included in the resulting iterator.   In Listing 13-19, we use filter with a closure that captures the shoe_size variable from its environment to iterate over a collection of Shoe struct instances. It will return only shoes that are the specified size.   // Filename: src/lib.rs  #[derive(PartialEq, Debug)] struct Shoe {     size: u32,     style: String, }  fn shoes_in_my_size(shoes: Vec&lt;Shoe&gt;, shoe_size: u32) -&gt; Vec&lt;Shoe&gt; {     shoes.into_iter()         .filter(|s| s.size == shoe_size)         .collect() }  #[test] fn filters_by_size() {     let shoes = vec![         Shoe { size: 10, style: String::from(\"sneaker\") },         Shoe { size: 13, style: String::from(\"sandal\") },         Shoe { size: 10, style: String::from(\"boot\") },     ];      let in_my_size = shoes_in_my_size(shoes, 10);      assert_eq!(         in_my_size,         vec![             Shoe { size: 10, style: String::from(\"sneaker\") },             Shoe { size: 10, style: String::from(\"boot\") },         ]     ); }  Listing 13-19: Using the filter method with a closure that captures shoe_size   The shoes_in_my_size function takes ownership of a vector of shoes and a shoe size as parameters. It returns a vector containing only shoes of the specified size.   In the body of shoes_in_my_size, we call into_iter to create an iterator that takes ownership of the vector. Then we call filter to adapt that iterator into a new iterator that only contains elements for which the closure returns true.   The closure captures the shoe_size parameter from the environment and compares the value with each shoe’s size, keeping only shoes of the size specified. Finally, calling collect gathers the values returned by the adapted iterator into a vector that’s returned by the function.   The test shows that when we call shoes_in_my_size, we get back only shoes that have the same size as the value we specified.   Creating Our Own Iterators with the Iterator Trait  We’ve shown that you can create an iterator by calling iter, into_iter, or iter_mut on a vector. You can create iterators from the other collection types in the standard library, such as hash map. You can also create iterators that do anything you want by implementing the Iterator trait on your own types. As previously mentioned, the only method you’re required to provide a definition for is the next method. Once you’ve done that, you can use all other methods that have default implementations provided by the Iterator trait!   To demonstrate, let’s create an iterator that will only ever count from 1 to 5. First, we’ll create a struct to hold some values. Then we’ll make this struct into an iterator by implementing the Iterator trait and using the values in that implementation.   Listing 13-20 has the definition of the Counter struct and an associated new function to create instances of Counter:   // Filename: src/lib.rs  struct Counter {     count: u32, }  impl Counter {     fn new() -&gt; Counter {         Counter { count: 0 }     } }  Listing 13-20: Defining the Counter struct and a new function that creates instances of Counter with an initial value of 0 for count   The Counter struct has one field named count. This field holds a u32 value that will keep track of where we are in the process of iterating from 1 to 5. The count field is private because we want the implementation of Counter to manage its value. The new function enforces the behavior of always starting new instances with a value of 0 in the count field.   Next, we’ll implement the Iterator trait for our Counter type by defining the body of the next method to specify what we want to happen when this iterator is used, as shown in Listing 13-21:   // Filename: src/lib.rs  impl Iterator for Counter {     type Item = u32;      fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {         self.count += 1;          if self.count &lt; 6 {             Some(self.count)         } else {             None         }     } }  Listing 13-21: Implementing the Iterator trait on our Counter struct   We set the associated Item type for our iterator to u32, meaning the iterator will return u32 values. Again, don’t worry about associated types yet, we’ll cover them in Chapter 19.   We want our iterator to add 1 to the current state, so we initialized count to 0 so it would return 1 first. If the value of count is less than 6, next will return the current value wrapped in Some, but if count is 6 or higher, our iterator will return None.   Using Our Counter Iterator’s next Method  Once we’ve implemented the Iterator trait, we have an iterator! Listing 13-22 shows a test demonstrating that we can use the iterator functionality of our Counter struct by calling the next method on it directly, just as we did with the iterator created from a vector in Listing 13-15.   // Filename: src/lib.rs  #[test] fn calling_next_directly() {     let mut counter = Counter::new();      assert_eq!(counter.next(), Some(1));     assert_eq!(counter.next(), Some(2));     assert_eq!(counter.next(), Some(3));     assert_eq!(counter.next(), Some(4));     assert_eq!(counter.next(), Some(5));     assert_eq!(counter.next(), None); }  Listing 13-22: Testing the functionality of the next method implementation   This test creates a new Counter instance in the counter variable and then calls next repeatedly, verifying that we have implemented the behavior we want this iterator to have: returning the values from 1 to 5.   Using Other Iterator Trait Methods  We implemented the Iterator trait by defining the next method, so we can now use any Iterator trait method’s default implementations as defined in the standard library, because they all use the next method’s functionality.   For example, if for some reason we wanted to take the values produced by an instance of Counter, pair them with values produced by another Counter instance after skipping the first value, multiply each pair together, keep only those results that are divisible by 3, and add all the resulting values together, we could do so, as shown in the test in Listing 13-23:   //Filename: src/lib.rs  #[test] fn using_other_iterator_trait_methods() {     let sum: u32 = Counter::new().zip(Counter::new().skip(1))                                  .map(|(a, b)| a * b)                                  .filter(|x| x % 3 == 0)                                  .sum();     assert_eq!(18, sum); }  Listing 13-23: Using a variety of Iterator trait methods on our Counter iterator   Note that zip produces only four pairs; the theoretical fifth pair (5, None) is never produced because zip returns None when either of its input iterators return None.   All of these method calls are possible because we specified how the next method works, and the standard library provides default implementations for other methods that call next.   Improving Our I/O Project  With this new knowledge about iterators, we can improve the I/O project in Chapter 12 by using iterators to make places in the code clearer and more concise. Let’s look at how iterators can improve our implementation of the Config::new function and the search function.   Removing a clone Using an Iterator  In Listing 12-6, we added code that took a slice of String values and created an instance of the Config struct by indexing into the slice and cloning the values, allowing the Config struct to own those values. In Listing 13-24, we’ve reproduced the implementation of the Config::new function as it was in Listing 12-23:   // Filename: src/lib.rs  impl Config {     pub fn new(args: &amp;[String]) -&gt; Result&lt;Config, &amp;'static str&gt; {         if args.len() &lt; 3 {             return Err(\"not enough arguments\");         }          let query = args[1].clone();         let filename = args[2].clone();          let case_sensitive = env::var(\"CASE_INSENSITIVE\").is_err();          Ok(Config { query, filename, case_sensitive })     } }  Listing 13-24: Reproduction of the Config::new function from Listing 12-23   At the time, we said not to worry about the inefficient clone calls because we would remove them in the future. Well, that time is now!   We needed clone here because we have a slice with String elements in the parameter args, but the new function doesn’t own args. To return ownership of a Config instance, we had to clone the values from the query and filename fields of Config so the Config instance can own its values.   With our new knowledge about iterators, we can change the new function to take ownership of an iterator as its argument instead of borrowing a slice. We’ll use the iterator functionality instead of the code that checks the length of the slice and indexes into specific locations. This will clarify what the Config::new function is doing because the iterator will access the values.   Once Config::new takes ownership of the iterator and stops using indexing operations that borrow, we can move the String values from the iterator into Config rather than calling clone and making a new allocation.   Using the Returned Iterator Directly  Open your I/O project’s src/main.rs file, which should look like this:  // Filename: src/main.rs  fn main() {     let args: Vec&lt;String&gt; = env::args().collect();      let config = Config::new(&amp;args).unwrap_or_else(|err| {         eprintln!(\"Problem parsing arguments: {}\", err);         process::exit(1);     });      // --snip-- }   We’ll change the start of the main function that we had in Listing 12-24 to the code in Listing 13-25. This won’t compile until we update Config::new as well.   // Filename: src/main.rs  fn main() {     let config = Config::new(env::args()).unwrap_or_else(|err| {         eprintln!(\"Problem parsing arguments: {}\", err);         process::exit(1);     });      // --snip-- }  Listing 13-25: Passing the return value of env::args to Config::new   The env::args function returns an iterator! Rather than collecting the iterator values into a vector and then passing a slice to Config::new, now we’re passing ownership of the iterator returned from env::args to Config::new directly.   Next, we need to update the definition of Config::new.  In your I/O project’s src/lib.rs file, let’s change the signature of Config::new to look like Listing 13-26.  This still won’t compile because we need to update the function body.   // Filename: src/lib.rs  impl Config {     pub fn new(mut args: std::env::Args) -&gt; Result&lt;Config, &amp;'static str&gt; {         // --snip--  Listing 13-26: Updating the signature of Config::new to expect an iterator   The standard library documentation for the env::args function shows that the type of the iterator it returns is std::env::Args. We’ve updated the signature of the Config::new function so the parameter args has the type std::env::Args instead of &amp;[String]. Because we’re taking ownership of args and we’ll be mutating args by iterating over it, we can add the mut keyword into the specification of the args parameter to make it mutable.   Using Iterator Trait Methods Instead of Indexing  Next, we’ll fix the body of Config::new. The standard library documentation also mentions that std::env::Args implements the Iterator trait, so we know we can call the next method on it! Listing 13-27 updates the code from Listing 12-23 to use the next method:   // Filename: src/lib.rs  impl Config {     pub fn new(mut args: std::env::Args) -&gt; Result&lt;Config, &amp;'static str&gt; {         args.next();          let query = match args.next() {             Some(arg) =&gt; arg,             None =&gt; return Err(\"Didn't get a query string\"),         };          let filename = match args.next() {             Some(arg) =&gt; arg,             None =&gt; return Err(\"Didn't get a file name\"),         };          let case_sensitive = env::var(\"CASE_INSENSITIVE\").is_err();          Ok(Config { query, filename, case_sensitive })     } }  Listing 13-27: Changing the body of Config::new to use iterator methods   Remember that the first value in the return value of env::args is the name of the program. We want to ignore that and get to the next value, so first we call next and do nothing with the return value. Second, we call next to get the value we want to put in the query field of Config. If next returns a Some, we use a match to extract the value. If it returns None, it means not enough arguments were given and we return early with an Err value. We do the same thing for the filename value.   Making Code Clearer with Iterator Adaptors  We can also take advantage of iterators in the search function in our I/O project, which is reproduced here in Listing 13-28 as it was in Listing 12-19:   // Filename: src/lib.rs  pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {     let mut results = Vec::new();      for line in contents.lines() {         if line.contains(query) {             results.push(line);         }     }      results }  Listing 13-28: The implementation of the search function from Listing 12-19   We can write this code in a more concise way using iterator adaptor methods. Doing so also lets us avoid having a mutable intermediate results vector. The functional programming style prefers to minimize the amount of mutable state to make code clearer. Removing the mutable state might enable a future enhancement to make searching happen in parallel, because we wouldn’t have to manage concurrent access to the results vector. Listing 13-29 shows this change:   // Filename: src/lib.rs  pub fn search&lt;'a&gt;(query: &amp;str, contents: &amp;'a str) -&gt; Vec&lt;&amp;'a str&gt; {     contents.lines()         .filter(|line| line.contains(query))         .collect() }  Listing 13-29: Using iterator adaptor methods in the implementation of the search function   Recall that the purpose of the search function is to return all lines in contents that contain the query. Similar to the filter example in Listing 13-19, this code uses the filter adaptor to keep only the lines that line.contains(query) returns true for. We then collect the matching lines into another vector with collect. Much simpler! Feel free to make the same change to use iterator methods in the search_case_insensitive function as well.   The next logical question is which style you should choose in your own code and why: the original implementation in Listing 13-28 or the version using iterators in Listing 13-29.  Most Rust programmers prefer to use the iterator style.  It’s a bit tougher to get the hang of at first, but once you get a feel for the various iterator adaptors and what they do, iterators can be easier to understand.  Instead of fiddling with the various bits of looping and building new vectors, the code focuses on the high-level objective of the loop.  This abstracts away some of the commonplace code so it’s easier to see the concepts that are unique to this code, such as the filtering condition each element in the iterator must pass.   But are the two implementations truly equivalent? The intuitive assumption might be that the more low-level loop will be faster. Let’s talk about performance.   Comparing Performance: Loops vs. Iterators  To determine whether to use loops or iterators, you need to know which version of our search functions is faster: the version with an explicit for loop or the version with iterators.   We ran a benchmark by loading the entire contents of The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle into a String and looking for the word the in the contents. Here are the results of the benchmark on the version of search using the for loop and the version using iterators:   test bench_search_for  ... bench:  19,620,300 ns/iter (+/- 915,700) test bench_search_iter ... bench:  19,234,900 ns/iter (+/- 657,200)   The iterator version was slightly faster! We won’t explain the benchmark code here, because the point is not to prove that the two versions are equivalent but to get a general sense of how these two implementations compare performance-wise.   For a more comprehensive benchmark, you should check using various texts of various sizes as the contents, different words and words of different lengths as the query, and all kinds of other variations. The point is this: iterators, although a high-level abstraction, get compiled down to roughly the same code as if you’d written the lower-level code yourself. Iterators are one of Rust’s zero-cost abstractions, by which we mean using the abstraction imposes no additional runtime overhead. This is analogous to how Bjarne Stroustrup, the original designer and implementor of C++, defines zero-overhead in “Foundations of C++” (2012):      In general, C++ implementations obey the zero-overhead principle: What you don’t use, you don’t pay for. And further: What you do use, you couldn’t hand code any better.    As another example, the following code is taken from an audio decoder. The decoding algorithm uses the linear prediction mathematical operation to estimate future values based on a linear function of the previous samples. This code uses an iterator chain to do some math on three variables in scope: a buffer slice of data, an array of 12 coefficients, and an amount by which to shift data in qlp_shift. We’ve declared the variables within this example but not given them any values; although this code doesn’t have much meaning outside of its context, it’s still a concise, real-world example of how Rust translates high-level ideas to low-level code.   let buffer: &amp;mut [i32]; let coefficients: [i64; 12]; let qlp_shift: i16;  for i in 12..buffer.len() {     let prediction = coefficients.iter()                                  .zip(&amp;buffer[i - 12..i])                                  .map(|(&amp;c, &amp;s)| c * s as i64)                                  .sum::&lt;i64&gt;() &gt;&gt; qlp_shift;     let delta = buffer[i];     buffer[i] = prediction as i32 + delta; }  To calculate the value of prediction, this code iterates through each of the 12 values in coefficients and uses the zip method to pair the coefficient values with the previous 12 values in buffer. Then, for each pair, we multiply the values together, sum all the results, and shift the bits in the sum qlp_shift bits to the right.   Calculations in applications like audio decoders often prioritize performance most highly. Here, we’re creating an iterator, using two adaptors, and then consuming the value. What assembly code would this Rust code compile to? Well, as of this writing, it compiles down to the same assembly you’d write by hand. There’s no loop at all corresponding to the iteration over the values in coefficients: Rust knows that there are 12 iterations, so it “unrolls” the loop. Unrolling is an optimization that removes the overhead of the loop controlling code and instead generates repetitive code for each iteration of the loop.   All of the coefficients get stored in registers, which means accessing the values is very fast. There are no bounds checks on the array access at runtime. All these optimizations that Rust is able to apply make the resulting code extremely efficient. Now that you know this, you can use iterators and closures without fear! They make code seem like it’s higher level but don’t impose a runtime performance penalty for doing so.   Summary  Closures and iterators are Rust features inspired by functional programming language ideas. They contribute to Rust’s capability to clearly express high-level ideas at low-level performance. The implementations of closures and iterators are such that runtime performance is not affected. This is part of Rust’s goal to strive to provide zero-cost abstractions.   Now that we’ve improved the expressiveness of our I/O project, let’s look at some more features of cargo that will help us share the project with the world.  ","categories": ["RUST Language"],
        "tags": ["Functional programming","Iterator","Closure"],
        "url": "https://jjungs-lee.github.io//rust/13.Functional-Language-Features_Iterators-and-Closures",
        "teaser":null},{
        "title": "RUST : How To call RUST function from C",
        "excerpt":"How to call RUST function from C  I make programs mainly using C or C++ language. Recently I’m studying Rust. So I wanna interact to C and Rust.  I want to link each source file(.c or .rs) after compiling it as an object file.   Let’s say I want to call a Rust function in C.   How do I do this? What does it mean to call a rust function from c? Actually I don’t know about it. So I Searching for google. So many keyword pop out!!  What caught my eye was the FFI(Foreign Function Interface).   Let’s implement C function for use in Rust. Plz follow this code.  // file : main.c #include &lt;stdio.h&gt;  void c_print_fn(const char* str) {   printf(\"%s\\n\", str); }  int main() {   printf(\"Hello World\\n\");    // this is test for c   c_print_fn(\"Calling in C\");    return 0; }   When We build and run the program, We’ll see something like this:   $ clang -o main main.c $ ./main Hello World Calling in C   Great! Next step is make Rust source file.  First, let’s make the main function.   // file : rust_ffi.rs fn call_from_c(num: i32) -&gt; i32 {   num }  // Just function test... later delete fn main() {   println!(\"{}\", call_from_c(10)); }  We will use only rustc is the compiler for the Rust programming language, provided by the project itself.  $ rustc rust_ffi.rs $ ./rust_ffi 10  Now step is calling RUST function from C.  Write the function to use for c in pub extern \"C\" fn ~~~.    At this time, it should be the same as the prototype implemented in C.  // file : rust_ffi.rs pub extern \"C\" fn call_from_c(num: i32) -&gt; i32 {   num }  // TODO : later remove main function fn main() {   println!(\"{}\", rust_pow_num(10)); }  Okay… Let’s call the rust function from C  // file : main.c #include &lt;stdio.h&gt;  void c_print_fn(const char* str) {   printf(\"%s\\n\", str); }  int main() {   printf(\"Hello World\\n\");      // this is test for c   c_print_fn(\"Calling in C\");    // rust fn call!!   printf(\"%d\\n\", call_from_c(211));    return 0; }  When this code(main.c) build, maybe we can see the error.  $ clang -o main main.c main.c:14:18: warning: implicit declaration of function 'call_from_c' is        invalid in C99 [-Wimplicit-function-declaration]   printf(\"%d\\n\", call_from_c(211));                   ^ 1 warning generated. /tmp/main-c995b9.o: In function `main': main.c:(.text+0x6a): undefined reference to `call_from_c' clang: error: linker command failed with exit code 1  (use -v to see invocation)   Compiler doesn’t know where call_from_c function is. So declare function and parameter same as rust function.   fix like this :   // file : main.c #include &lt;stdio.h&gt;  // declare function name and parameter int call_from_c(int num);  void c_print_fn(const char* str) {   printf(\"%s\\n\", str); }  int main() {   printf(\"Hello World\\n\");    // this is test for c   c_print_fn(\"Calling in C\");    // rust fn call!!   printf(\"%d\\n\", call_from_c(211));    return 0; }  We will make object files in C and Rust. Then linking togeter. leggo~   $ clang -c main.c $ rustc --emit=obj rust_ffi.rs  $ clang -o test main.o rust_ffi.o rust_ffi.o: In function `main': rust_ffi.7rcbfp3g-cgu.0:(.text.main+0x0): multiple definition of `main' main.o:main.c:(.text+0x30): first defined here main.o: In function `main': main.c:(.text+0x68): undefined reference to `call_from_c' rust_ffi.o: In function `std::rt::lang_start::h8e054b1a1a6cb597': rust_ffi.7rcbfp3g-cgu.0:(.text._ZN3std2rt10lang_start17h8e054b1a1a6cb597E+0x34):  undefined reference to `std::rt::lang_start_internal::h9cf8802361ad86c2' rust_ffi.o: In function `rust_ffi::main::h7f93501ccd4adba5': rust_ffi.7rcbfp3g-cgu.0:(.text._ZN8rust_ffi4main17h7f93501ccd4adba5E+0x21):  undefined reference to `core::fmt::num::imp::_$LT$ impl$u20$core..fmt..Display$u20$for$u20$i32$GT$::fmt::h765415089818841e' rust_ffi.7rcbfp3g-cgu.0:(.text._ZN8rust_ffi4main17h7f93501ccd4adba5E+0x8b):  undefined reference to `std::io::stdio::_print::h7e1d4022dd9ebaea' rust_ffi.o:(.data.DW.ref.rust_eh_personality[DW.ref.rust_eh_personality]+0x0):  undefined reference to `rust_eh_personality' clang: error: linker command failed with exit code 1 (use -v to see invocation)   Hum… Why don’t make executable file?… Let’s fix it.   I think rust file(rust_ffi.rs) has a problem…   because multiple definition of ‘main’ is very important build error.   rust_ffi.rs file has a main function and comment.   Delete main as comment. like this:   // file : rust_ffi.rs pub extern \"C\" fn call_from_c(num: i32) -&gt; i32 {   num }   $ rustc --emit=obj rust_ffi.rs  error[E0601]: `main` function not found in crate `rust_ffi`  --&gt; rust_ffi.rs:1:1   | 1 | / pub extern \"C\" fn rust_pow_num(num: i32) -&gt; i32 2 | | { 3 | |     num 4 | | }   | |_^ consider adding a `main` function to `rust_ffi.rs`  error: aborting due to previous error  For more information about this error, try `rustc --explain E0601`.  This time, error occureed because there is now main function (error[E0601]: main function not found in crate 'rust_ffi') Let me know “main function” isn’t exist in file to RUST using #![no_main] keyword.  // file : rust_ffi.rs #![no_main] pub extern \"C\" fn rust_pow_num(num: i32) -&gt; i32 {   num }  Okay… It’s too difficult… check for build.   Oh finally!! Can we complete it successfully?   Let’s linking one more time  $ clang -c main.c $ rustc --emit=obj rust_ffi.rs  $ clang -o test main.o rust_ffi.o main.o: In function `main': main.c:(.text+0x68): undefined reference to `call_from_c' clang: error: linker command failed with exit code 1  (use -v to see invocation)  call_from_c function could not be found. Maybe We should declare something more in the rust file.   Oh other sample code has #[no_mangle] keyword.. we will try this..  // file : rust_ffi.rs #![no_main] #[no_mangle] pub extern \"C\" fn rust_pow_num(num: i32) -&gt; i32 {   num }   Please let me succeed 🙏 :(   $ clang -c main.c $ rustc --emit=obj rust_ffi.rs  $ clang -o test main.o rust_ffi.o $ ./test  Hello World Calling in C 211   Finally, We succeeded and we got the desired result.   Now let’s apply it and learn more about ‘rust’.     reference     Calling a C function from Rust   Calling Rust Functions from Other Languages   FFI   FFI rust object file missing linking for c compilation using clang  ","categories": ["RUST"],
        "tags": ["FFI","Foreign Function Interface","C / C++","Too hard"],
        "url": "https://jjungs-lee.github.io//rust/How-to-call-C-function-in-rust",
        "teaser":null},{
        "title": "RUST : 14. More About Cargo and Crates.io",
        "excerpt":"More About Cargo and Crates.io   So far we’ve used only the most basic features of Cargo to build, run, and test our code, but it can do a lot more. In this chapter, we’ll discuss some of its other, more advanced features to show you how to do the following:      Customize your build through release profiles   Publish libraries on crates.io   Organize large projects with workspaces   Install binaries from crates.io   Extend Cargo using custom commands   Cargo can do even more than what we cover in this chapter, so for a full explanation of all its features, see its documentation.   Customizing Builds with Release Profiles  In Rust, release profiles are predefined and customizable profiles with different configurations that allow a programmer to have more control over various options for compiling code. Each profile is configured independently of the others.   Cargo has two main profiles: the dev profile Cargo uses when you run cargo build and the release profile Cargo uses when you run cargo build --release.  The dev profile is defined with good defaults for development, and the release profile has good defaults for release builds.   These profile names might be familiar from the output of your builds:   $ cargo build     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs $ cargo build --release     Finished release [optimized] target(s) in 0.0 secs   The dev and release shown in this build output indicate that the compiler is using different profiles.   Cargo has default settings for each of the profiles that apply when there aren’t any [profile.*] sections in the project’s Cargo.toml file.  By adding [profile.*] sections for any profile you want to customize, you can override any subset of the default settings.  For example, here are the default values for the opt-level setting for the dev and release profiles:   // Filename: Cargo.toml  [profile.dev] opt-level = 0  [profile.release] opt-level = 3   The opt-level setting controls the number of optimizations Rust will apply to your code, with a range of 0 to 3.  Applying more optimizations extends compiling time, so if you’re in development and compiling your code often, you’ll want faster compiling even if the resulting code runs slower.  That is the reason the default opt-level for dev is 0.  When you’re ready to release your code, it’s best to spend more time compiling.  You’ll only compile in release mode once, but you’ll run the compiled program many times, so release mode trades longer compile time for code that runs faster.  That is why the default opt-level for the release profile is 3.   You can override any default setting by adding a different value for it in Cargo.toml.  For example, if we want to use optimization level 1 in the development profile, we can add these two lines to our project’s Cargo.toml file:   // Filename: Cargo.toml  [profile.dev] opt-level = 1   This code overrides the default setting of 0.  Now when we run cargo build, Cargo will use the defaults for the dev profile plus our customization to opt-level.  Because we set opt-level to 1, Cargo will apply more optimizations than the default, but not as many as in a release build.   For the full list of configuration options and defaults for each profile, see Cargo’s documentation.   Publishing a Crate to Crates.io  We’ve used packages from crates.io as dependencies of our project, but you can also share your code with other people by publishing your own packages.  The crate registry at crates.io distributes the source code of your packages, so it primarily hosts code that is open source.   Rust and Cargo have features that help make your published package easier for people to use and to find in the first place.  We’ll talk about some of these features next and then explain how to publish a package.   Making Useful Documentation Comments  Accurately documenting your packages will help other users know how and when to use them, so it’s worth investing the time to write documentation.  In Chapter 3, we discussed how to comment Rust code using two slashes, //.  Rust also has a particular kind of comment for documentation, known conveniently as a documentation comment, that will generate HTML documentation.  The HTML displays the contents of documentation comments for public API items intended for programmers interested in knowing how to use your crate as opposed to how your crate is implemented.   Documentation comments use three slashes, ///, instead of two and support Markdown notation for formatting the text.  Place documentation comments just before the item they’re documenting.  Listing 14-1 shows documentation comments for an add_one function in a crate named my_crate:   //Filename: src/lib.rs  /// Adds one to the number given. /// /// # Examples /// /// ``` /// let arg = 5; /// let answer = my_crate::add_one(arg); /// /// assert_eq!(6, answer); /// ``` pub fn add_one(x: i32) -&gt; i32 {     x + 1 }  Listing 14-1: A documentation comment for a function   Here, we give a description of what the add_one function does, start a section with the heading Examples, and then provide code that demonstrates how to use the add_one function.  We can generate the HTML documentation from this documentation comment by running cargo doc.  This command runs the rustdoc tool distributed with Rust and puts the generated HTML documentation in the target/doc directory.   For convenience, running cargo doc --open will build the HTML for your current crate’s documentation (as well as the documentation for all of your crate’s dependencies) and open the result in a web browser.  Navigate to the add_one function and you’ll see how the text in the documentation comments is rendered, as shown in Figure 14-1:   Figure 14-1: HTML documentation for the add_one function   Commonly Used Sections  We used the # Examples Markdown heading in Listing 14-1 to create a section in the HTML with the title “Examples.”  Here are some other sections that crate authors commonly use in their documentation:     Panics: The scenarios in which the function being documented could panic.  Callers of the function who don’t want their programs to panic should make sure they don’t call the function in these situations.   Errors: If the function returns a Result, describing the kinds of errors that might occur and what conditions might cause those errors to be returned can be helpful to callers so they can write code to handle the different kinds of errors in different ways.   Safety: If the function is unsafe to call (we discuss unsafety in Chapter 19), there should be a section explaining why the function is unsafe and covering the invariants that the function expects callers to uphold.   Most documentation comments don’t need all of these sections, but this is a good checklist to remind you of the aspects of your code that people calling your code will be interested in knowing about.   Documentation Comments as Tests  Adding example code blocks in your documentation comments can help demonstrate how to use your library, and doing so has an additional bonus: running cargo test will run the code examples in your documentation as tests! Nothing is better than documentation with examples.  But nothing is worse than examples that don’t work because the code has changed since the documentation was written.  If we run cargo test with the documentation for the add_one function from Listing 14-1, we will see a section in the test results like this:      Doc-tests my_crate  running 1 test test src/lib.rs - add_one (line 5) ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  Now if we change either the function or the example so the assert_eq! in the example panics and run cargo test again, we’ll see that the doc tests catch that the example and the code are out of sync with each other!   Commenting Contained Items  Another style of doc comment, //!, adds documentation to the item that contains the comments rather than adding documentation to the items following the comments.  We typically use these doc comments inside the crate root file (src/lib.rs by convention) or inside a module to document the crate or the module as a whole.   For example, if we want to add documentation that describes the purpose of the my_crate crate that contains the add_one function, we can add documentation comments that start with //! to the beginning of the src/lib.rs file, as shown in Listing 14-2:   // Filename: src/lib.rs  //! # My Crate //! //! `my_crate` is a collection of utilities to make performing certain //! calculations more convenient.  /// Adds one to the number given. // --snip--  Listing 14-2: Documentation for the my_crate crate as a whole   Notice there isn’t any code after the last line that begins with //!.  Because we started the comments with //! instead of ///, we’re documenting the item that contains this comment rather than an item that follows this comment.  In this case, the item that contains this comment is the src/lib.rs file, which is the crate root. These comments describe the entire crate.   When we run cargo doc --open, these comments will display on the front page of the documentation for my_crate above the list of public items in the crate, as shown in Figure 14-2:     Figure 14-2: Rendered documentation for my_crate, including the comment describing the crate as a whole  Documentation comments within items are useful for describing crates and modules especially. Use them to explain the overall purpose of the container to help your users understand the crate’s organization.   Exporting a Convenient Public API with pub use  In Chapter 7, we covered how to organize our code into modules using the mod keyword, how to make items public using the pub keyword, and how to bring items into a scope with the use keyword.  However, the structure that makes sense to you while you’re developing a crate might not be very convenient for your users.  You might want to organize your structs in a hierarchy containing multiple levels, but then people who want to use a type you’ve defined deep in the hierarchy might have trouble finding out that type exists.  They might also be annoyed at having to enter use my_crate::some_module::another_module::UsefulType; rather than use my_crate::UsefulType;.   The structure of your public API is a major consideration when publishing a crate.  People who use your crate are less familiar with the structure than you are and might have difficulty finding the pieces they want to use if your crate has a large module hierarchy.   The good news is that if the structure isn’t convenient for others to use from another library, you don’t have to rearrange your internal organization: instead, you can re-export items to make a public structure that’s different from your private structure by using pub use.  Re-exporting takes a public item in one location and makes it public in another location, as if it were defined in the other location instead.   For example, say we made a library named art for modeling artistic concepts.  Within this library are two modules: a kinds module containing two enums named PrimaryColor and SecondaryColor and a utils module containing a function named mix, as shown in Listing 14-3:  // Filename: src/lib.rs  //! # Art //! //! A library for modeling artistic concepts.  pub mod kinds {     /// The primary colors according to the RYB color model.     pub enum PrimaryColor {         Red,         Yellow,         Blue,     }      /// The secondary colors according to the RYB color model.     pub enum SecondaryColor {         Orange,         Green,         Purple,     } }  pub mod utils {     use crate::kinds::*;      /// Combines two primary colors in equal amounts to create     /// a secondary color.     pub fn mix(c1: PrimaryColor, c2: PrimaryColor) -&gt; SecondaryColor {         // --snip--         SecondaryColor::Orange     } } fn main() {}  Listing 14-3: An art library with items organized into kinds and utils modules   Figure 14-3 shows what the front page of the documentation for this crate generated by cargo doc would look like:     Figure 14-3: Front page of the documentation for art that lists the kinds and utils modules   Note that the PrimaryColor and SecondaryColor types aren’t listed on the front page, nor is the mix function. We have to click kinds and utils to see them.   Another crate that depends on this library would need use statements that bring the items from art into scope, specifying the module structure that’s currently defined.  Listing 14-4 shows an example of a crate that uses the PrimaryColor and mix items from the art crate:  // Filename: src/main.rs  use art::kinds::PrimaryColor; use art::utils::mix;  fn main() {     let red = PrimaryColor::Red;     let yellow = PrimaryColor::Yellow;     mix(red, yellow); }  Listing 14-4: A crate using the art crate’s items with its internal structure exported   The author of the code in Listing 14-4, which uses the art crate, had to figure out that PrimaryColor is in the kinds module and mix is in the utils module.  The module structure of the art crate is more relevant to developers working on the art crate than to developers using the art crate.  The internal structure that organizes parts of the crate into the kinds module and the utils module doesn’t contain any useful information for someone trying to understand how to use the art crate.  Instead, the art crate’s module structure causes confusion because developers have to figure out where to look, and the structure is inconvenient because developers must specify the module names in the use statements.   To remove the internal organization from the public API, we can modify the art crate code in Listing 14-3 to add pub use statements to re-export the items at the top level, as shown in Listing 14-5:   // Filename: src/lib.rs  //! # Art //! //! A library for modeling artistic concepts.  pub use self::kinds::PrimaryColor; pub use self::kinds::SecondaryColor; pub use self::utils::mix;  pub mod kinds {     // --snip-- }  pub mod utils {     // --snip-- }  Listing 14-5: Adding pub use statements to re-export items   The API documentation that cargo doc generates for this crate will now list and link re-exports on the front page, as shown in Figure 14-4, making the PrimaryColor and SecondaryColor types and the mix function easier to find.     Figure 14-4: The front page of the documentation for art that lists the re-exports   The art crate users can still see and use the internal structure from Listing 14-3 as demonstrated in Listing 14-4, or they can use the more convenient structure in Listing 14-5, as shown in Listing 14-6:  //Filename: src/main.rs  use art::PrimaryColor; use art::mix;  fn main() {     // --snip-- }  Listing 14-6: A program using the re-exported items from the art crate   In cases where there are many nested modules, re-exporting the types at the top level with pub use can make a significant difference in the experience of people who use the crate.   Creating a useful public API structure is more of an art than a science, and you can iterate to find the API that works best for your users.  Choosing pub use gives you flexibility in how you structure your crate internally and decouples that internal structure from what you present to your users.  Look at some of the code of crates you’ve installed to see if their internal structure differs from their public API.   Setting Up a Crates.io Account  Before you can publish any crates, you need to create an account on crates.io and get an API token.  To do so, visit the home page at crates.io and log in via a GitHub account. (The GitHub account is currently a requirement, but the site might support other ways of creating an account in the future.)  Once you’re logged in, visit your account settings at https://crates.io/me/ and retrieve your API key. Then run the cargo login command with your API key, like this:   $ cargo login abcdefghijklmnopqrstuvwxyz012345  This command will inform Cargo of your API token and store it locally in ~/.cargo/credentials.  Note that this token is a secret: do not share it with anyone else.  If you do share it with anyone for any reason, you should revoke it and generate a new token on crates.io.   Adding Metadata to a New Crate  Now that you have an account, let’s say you have a crate you want to publish.  Before publishing, you’ll need to add some metadata to your crate by adding it to the [package] section of the crate’s Cargo.toml file.   Your crate will need a unique name.  While you’re working on a crate locally, you can name a crate whatever you’d like.  However, crate names on crates.io are allocated on a first-come, first-served basis.  Once a crate name is taken, no one else can publish a crate with that name.  Before attempting to publish a crate, search for the name you want to use on the site.  If the name has been used by another crate, you will need to find another name and edit the name field in the Cargo.toml file under the [package] section to use the new name for publishing, like so:  // Filename: Cargo.toml  [package] name = \"guessing_game\"  Even if you’ve chosen a unique name, when you run cargo publish to publish the crate at this point, you’ll get a warning and then an error:   $ cargo publish     Updating registry `https://github.com/rust-lang/crates.io-index` warning: manifest has no description, license, license-file, documentation, homepage or repository. --snip-- error: api errors: missing or empty metadata fields: description, license.  The reason is that you’re missing some crucial information: a description and license are required so people will know what your crate does and under what terms they can use it. To rectify this error, you need to include this information in the Cargo.toml file.   Add a description that is just a sentence or two, because it will appear with your crate in search results.  For the license field, you need to give a license identifier value.  The Linux Foundation’s Software Package Data Exchange (SPDX) lists the identifiers you can use for this value. For example, to specify that you’ve licensed your crate using the MIT License, add the MIT identifier:  // Filename: Cargo.toml  [package] name = \"guessing_game\" license = \"MIT\"  If you want to use a license that doesn’t appear in the SPDX, you need to place the text of that license in a file, include the file in your project, and then use license-file to specify the name of that file instead of using the license key.   Guidance on which license is appropriate for your project is beyond the scope of this book. Many people in the Rust community license their projects in the same way as Rust by using a dual license of MIT OR Apache-2.0. This practice demonstrates that you can also specify multiple license identifiers separated by OR to have multiple licenses for your project.   With a unique name, the version, the author details that cargo new added when you created the crate, your description, and a license added, the Cargo.toml file for a project that is ready to publish might look like this:   // Filename: Cargo.toml  [package] name = \"guessing_game\" version = \"0.1.0\" authors = [\"Your Name &lt;you@example.com&gt;\"] edition = \"2018\" description = \"A fun game where you guess what number the computer has chosen.\" license = \"MIT OR Apache-2.0\"  [dependencies]  Cargo’s documentation describes other metadata you can specify to ensure others can discover and use your crate more easily.   Publishing to Crates.io  Now that you’ve created an account, saved your API token, chosen a name for your crate, and specified the required metadata, you’re ready to publish! Publishing a crate uploads a specific version to crates.io for others to use.   Be careful when publishing a crate because a publish is permanent.  The version can never be overwritten, and the code cannot be deleted.  One major goal of crates.io is to act as a permanent archive of code so that builds of all projects that depend on crates from crates.io will continue to work.  Allowing version deletions would make fulfilling that goal impossible.  However, there is no limit to the number of crate versions you can publish.   Run the cargo publish command again. It should succeed now:   $ cargo publish  Updating registry `https://github.com/rust-lang/crates.io-index` Packaging guessing_game v0.1.0 (file:///projects/guessing_game) Verifying guessing_game v0.1.0 (file:///projects/guessing_game) Compiling guessing_game v0.1.0 (file:///projects/guessing_game/target/package/guessing_game-0.1.0)  Finished dev [unoptimized + debuginfo] target(s) in 0.19 secs Uploading guessing_game v0.1.0 (file:///projects/guessing_game)  Congratulations! You’ve now shared your code with the Rust community, and anyone can easily add your crate as a dependency of their project.   Publishing a New Version of an Existing Crate  When you’ve made changes to your crate and are ready to release a new version, you change the version value specified in your Cargo.toml file and republish.  Use the Semantic Versioning rules to decide what an appropriate next version number is based on the kinds of changes you’ve made.  Then run cargo publish to upload the new version.   Removing Versions from Crates.io with cargo yank  Although you can’t remove previous versions of a crate, you can prevent any future projects from adding them as a new dependency.  This is useful when a crate version is broken for one reason or another. In such situations, Cargo supports yanking a crate version.   Yanking a version prevents new projects from starting to depend on that version while allowing all existing projects that depend on it to continue to download and depend on that version. Essentially, a yank means that all projects with a Cargo.lock will not break, and any future Cargo.lock files generated will not use the yanked version.   To yank a version of a crate, run cargo yank and specify which version you want to yank:   $ cargo yank --vers 1.0.1   By adding --undo to the command, you can also undo a yank and allow projects to start depending on a version again:   $ cargo yank --vers 1.0.1 --undo   A yank does not delete any code.  For example, the yank feature is not intended for deleting accidentally uploaded secrets. If that happens, you must reset those secrets immediately.   Cargo Workspaces  In Chapter 12, we built a package that included a binary crate and a library crate.  As your project develops, you might find that the library crate continues to get bigger and you want to split up your package further into multiple library crates.  In this situation, Cargo offers a feature called workspaces that can help manage multiple related packages that are developed in tandem.   Creating a Workspace  A workspace is a set of packages that share the same Cargo.lock and output directory. Let’s make a project using a workspace—we’ll use trivial code so we can concentrate on the structure of the workspace.  There are multiple ways to structure a workspace;  we’re going to show one common way. We’ll have a workspace containing a binary and two libraries.  The binary, which will provide the main functionality, will depend on the two libraries.  One library will provide an add_one function, and a second library an add_two function.  These three crates will be part of the same workspace.  We’ll start by creating a new directory for the workspace:   $ mkdir add $ cd add  Next, in the add directory, we create the Cargo.toml file that will configure the entire workspace.  This file won’t have a [package] section or the metadata we’ve seen in other Cargo.toml files.  Instead, it will start with a [workspace] section that will allow us to add members to the workspace by specifying the path to our binary crate; in this case, that path is adder:  // Filename: Cargo.toml [workspace]  members = [     \"adder\", ]  Next, we’ll create the adder binary crate by running cargo new within the add directory:   $ cargo new adder      Created binary (application) `adder` project  At this point, we can build the workspace by running cargo build.  The files in your add directory should look like this:   ├── Cargo.lock ├── Cargo.toml ├── adder │   ├── Cargo.toml │   └── src │       └── main.rs └── target  The workspace has one target directory at the top level for the compiled artifacts to be placed into; the adder crate doesn’t have its own target directory.  Even if we were to run cargo build from inside the adder directory, the compiled artifacts would still end up in add/target rather than add/adder/target.  Cargo structures the target directory in a workspace like this because the crates in a workspace are meant to depend on each other.  If each crate had its own target directory, each crate would have to recompile each of the other crates in the workspace to have the artifacts in its own target directory.  By sharing one target directory, the crates can avoid unnecessary rebuilding.   Creating the Second Crate in the Workspace  Next, let’s create another member crate in the workspace and call it add-one.  Change the top-level Cargo.toml to specify the add-one path in the members list:   // Filename: Cargo.toml  [workspace]  members = [     \"adder\",     \"add-one\", ]  Then generate a new library crate named add-one:   $ cargo new add-one --lib      Created library `add-one` project  Your add directory should now have these directories and files:   ├── Cargo.lock ├── Cargo.toml ├── add-one │   ├── Cargo.toml │   └── src │       └── lib.rs ├── adder │   ├── Cargo.toml │   └── src │       └── main.rs └── target  In the add-one/src/lib.rs file, let’s add an add_one function:   // Filename: add-one/src/lib.rs  fn main() { pub fn add_one(x: i32) -&gt; i32 {     x + 1 }  Now that we have a library crate in the workspace, we can have the binary crate adder depend on the library crate add-one.  First, we’ll need to add a path dependency on add-one to adder/Cargo.toml.   // Filename: adder/Cargo.toml  [dependencies]  add-one = { path = \"../add-one\" }   Cargo doesn’t assume that crates in a workspace will depend on each other, so we need to be explicit about the dependency relationships between the crates.   Next, let’s use the add_one function from the add-one crate in the adder crate.  Open the adder/src/main.rs file and add a use line at the top to bring the new add-one library crate into scope.  Then change the main function to call the add_one function, as in Listing 14-7.   // Filename: adder/src/main.rs  extern crate add_one;  fn main() {     let num = 10;     println!(\"Hello, world! {} plus one is {}!\", num, add_one::add_one(num)); }  Listing 14-7: Using the add-one library crate from the adder crate   Let’s build the workspace by running cargo build in the top-level add directory!   $ cargo build    Compiling add-one v0.1.0 (file:///projects/add/add-one)    Compiling adder v0.1.0 (file:///projects/add/adder)     Finished dev [unoptimized + debuginfo] target(s) in 0.68 secs  To run the binary crate from the add directory, we need to specify which package in the workspace we want to use by using the -p argument and the package name with cargo run:   $ cargo run -p adder     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running `target/debug/adder` Hello, world! 10 plus one is 11!  This runs the code in adder/src/main.rs, which depends on the add-one crate.   Depending on an External Crate in a Workspace  Notice that the workspace has only one Cargo.lock file at the top level of the workspace rather than having a Cargo.lock in each crate’s directory.  This ensures that all crates are using the same version of all dependencies.  If we add the rand crate to the adder/Cargo.toml and add-one/Cargo.toml files, Cargo will resolve both of those to one version of rand and record that in the one Cargo.lock.  Making all crates in the workspace use the same dependencies means the crates in the workspace will always be compatible with each other.  Let’s add the rand crate to the [dependencies] section in the add-one/Cargo.toml file to be able to use the rand crate in the add-one crate:   // Filename: add-one/Cargo.toml  [dependencies] rand = \"0.5.5\"   We can now add use rand; to the add-one/src/lib.rs file, and building the whole workspace by running cargo build in the add directory will bring in and compile the rand crate:   $ cargo build     Updating crates.io index   Downloaded rand v0.5.5    --snip--    Compiling rand v0.5.5    Compiling add-one v0.1.0 (file:///projects/add/add-one)    Compiling adder v0.1.0 (file:///projects/add/adder)     Finished dev [unoptimized + debuginfo] target(s) in 10.18 secs  The top-level Cargo.lock now contains information about the dependency of add-one on rand.  However, even though rand is used somewhere in the workspace, we can’t use it in other crates in the workspace unless we add rand to their Cargo.toml files as well.  For example, if we add use rand; to the adder/src/main.rs file for the adder crate, we’ll get an error:   $ cargo build    Compiling adder v0.1.0 (file:///projects/add/adder) error: use of unstable library feature 'rand': use `rand` from crates.io (see issue #27703)  --&gt; adder/src/main.rs:1:1   | 1 | use rand;   To fix this, edit the Cargo.toml file for the adder crate and indicate that rand is a dependency for that crate as well.  Building the adder crate will add rand to the list of dependencies for adder in Cargo.lock, but no additional copies of rand will be downloaded.  Cargo has ensured that every crate in the workspace using the rand crate will be using the same version.  Using the same version of rand across the workspace saves space because we won’t have multiple copies and ensures that the crates in the workspace will be compatible with each other.   Adding a Test to a Workspace  For another enhancement, let’s add a test of the add_one::add_one function within the add_one crate:   // Filename: add-one/src/lib.rs  pub fn add_one(x: i32) -&gt; i32 {     x + 1 }  #[cfg(test)] mod tests {     use super::*;      #[test]     fn it_works() {         assert_eq!(3, add_one(2));     } }  Now run cargo test in the top-level add directory:   $ cargo test    Compiling add-one v0.1.0 (file:///projects/add/add-one)    Compiling adder v0.1.0 (file:///projects/add/adder)     Finished dev [unoptimized + debuginfo] target(s) in 0.27 secs      Running target/debug/deps/add_one-f0253159197f7841  running 1 test test tests::it_works ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out       Running target/debug/deps/adder-f88af9d2cc175a5e  running 0 tests  test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out     Doc-tests add-one  running 0 tests  test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  The first section of the output shows that the it_works test in the add-one crate passed.  The next section shows that zero tests were found in the adder crate, and then the last section shows zero documentation tests were found in the add-one crate. Running cargo test in a workspace structured like this one will run the tests for all the crates in the workspace.   We can also run tests for one particular crate in a workspace from the top-level directory by using the -p flag and specifying the name of the crate we want to test:   $ cargo test -p add-one     Finished dev [unoptimized + debuginfo] target(s) in 0.0 secs      Running target/debug/deps/add_one-b3235fea9a156f74  running 1 test test tests::it_works ... ok  test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out     Doc-tests add-one  running 0 tests  test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out  This output shows cargo test only ran the tests for the add-one crate and didn’t run the adder crate tests.   If you publish the crates in the workspace to crates.io, each crate in the workspace will need to be published separately.  The cargo publish command does not have an --all flag or a -p flag, so you must change to each crate’s directory and run cargo publish on each crate in the workspace to publish the crates.   For additional practice, add an add-two crate to this workspace in a similar way as the add-one crate!   As your project grows, consider using a workspace: it’s easier to understand smaller, individual components than one big blob of code. Furthermore, keeping the crates in a workspace can make coordination between them easier if they are often changed at the same time.   Installing Binaries from Crates.io with cargo install  The cargo install command allows you to install and use binary crates locally.  This isn’t intended to replace system packages; it’s meant to be a convenient way for Rust developers to install tools that others have shared on crates.io.  Note that you can only install packages that have binary targets.  A binary target is the runnable program that is created if the crate has a src/main.rs file or another file specified as a binary, as opposed to a library target that isn’t runnable on its own but is suitable for including within other programs.  Usually, crates have information in the README file about whether a crate is a library, has a binary target, or both.   All binaries installed with cargo install are stored in the installation root’s bin folder.  If you installed Rust using rustup.rs and don’t have any custom configurations, this directory will be $HOME/.cargo/bin.  Ensure that directory is in your $PATH to be able to run programs you’ve installed with cargo install.   For example, in Chapter 12 we mentioned that there’s a Rust implementation of the grep tool called ripgrep for searching files.  If we want to install ripgrep, we can run the following:   $ cargo install ripgrep Updating registry `https://github.com/rust-lang/crates.io-index`  Downloading ripgrep v0.3.2  --snip--    Compiling ripgrep v0.3.2     Finished release [optimized + debuginfo] target(s) in 97.91 secs   Installing ~/.cargo/bin/rg  The last line of the output shows the location and the name of the installed binary, which in the case of ripgrep is rg. As long as the installation directory is in your $PATH, as mentioned previously, you can then run rg --help and start using a faster, rustier tool for searching files!   Extending Cargo with Custom Commands   Cargo is designed so you can extend it with new subcommands without having to modify Cargo.  If a binary in your $PATH is named cargo-something, you can run it as if it was a Cargo subcommand by running cargo something.  Custom commands like this are also listed when you run cargo --list.  Being able to use cargo install to install extensions and then run them just like the built-in Cargo tools is a super convenient benefit of Cargo’s design!   Summary  Sharing code with Cargo and crates.io is part of what makes the Rust ecosystem useful for many different tasks. Rust’s standard library is small and stable, but crates are easy to share, use, and improve on a timeline different from that of the language. Don’t be shy about sharing code that’s useful to you on crates.io; it’s likely that it will be useful to someone else as well!   ","categories": ["RUST Language"],
        "tags": ["Cargo","Crates.io","Open Source"],
        "url": "https://jjungs-lee.github.io//rust/14.More-About-Cargo-and-Crates.io",
        "teaser":null},{
        "title": "RUST : 15. Smart Pointers",
        "excerpt":"Smart Pointers  A pointer is a general concept for a variable that contains an address in memory.  This address refers to, or “points at,” some other data.  The most common kind of pointer in Rust is a reference, which you learned about in Chapter 4.  References are indicated by the &amp; symbol and borrow the value they point to.  They don’t have any special capabilities other than referring to data.  Also, they don’t have any overhead and are the kind of pointer we use most often.   Smart pointers, on the other hand, are data structures that not only act like a pointer but also have additional metadata and capabilities.  The concept of smart pointers isn’t unique to Rust: smart pointers originated in C++ and exist in other languages as well.  In Rust, the different smart pointers defined in the standard library provide functionality beyond that provided by references.  One example that we’ll explore in this chapter is the reference counting smart pointer type.  This pointer enables you to have multiple owners of data by keeping track of the number of owners and, when no owners remain, cleaning up the data.   In Rust, which uses the concept of ownership and borrowing, an additional difference between references and smart pointers is that references are pointers that only borrow data; in contrast, in many cases, smart pointers own the data they point to.   We’ve already encountered a few smart pointers in this book, such as String and Vec&lt;T&gt; in Chapter 8, although we didn’t call them smart pointers at the time.  Both these types count as smart pointers because they own some memory and allow you to manipulate it.  They also have metadata (such as their capacity) and extra capabilities or guarantees (such as with String ensuring its data will always be valid UTF-8).   Smart pointers are usually implemented using structs.  The characteristic that distinguishes a smart pointer from an ordinary struct is that smart pointers implement the Deref and Drop traits.  The Deref trait allows an instance of the smart pointer struct to behave like a reference so you can write code that works with either references or smart pointers.  The Drop trait allows you to customize the code that is run when an instance of the smart pointer goes out of scope. In this chapter, we’ll discuss both traits and demonstrate why they’re important to smart pointers.   Given that the smart pointer pattern is a general design pattern used frequently in Rust, this chapter won’t cover every existing smart pointer.  Many libraries have their own smart pointers, and you can even write your own.  We’ll cover the most common smart pointers in the standard library:      Box&lt;T&gt; for allocating values on the heap   Rc&lt;T&gt;, a reference counting type that enables multiple ownership   Ref&lt;T&gt; and RefMut&lt;T&gt;, accessed through RefCell&lt;T&gt;, a type that enforces the borrowing rules at runtime instead of compile time   In addition, we’ll cover the interior mutability pattern where an immutable type exposes an API for mutating an interior value.  We’ll also discuss reference cycles: how they can leak memory and how to prevent them.   Let’s dive in!   Using Box to Point to Data on the Heap  The most straightforward smart pointer is a box, whose type is written Box&lt;T&gt;.  Boxes allow you to store data on the heap rather than the stack. What remains on the stack is the pointer to the heap data.  Refer to Chapter 4 to review the difference between the stack and the heap.   Boxes don’t have performance overhead, other than storing their data on the heap instead of on the stack.  But they don’t have many extra capabilities either. You’ll use them most often in these situations:      When you have a type whose size can’t be known at compile time and you want to use a value of that type in a context that requires an exact size   When you have a large amount of data and you want to transfer ownership but ensure the data won’t be copied when you do so   When you want to own a value and you care only that it’s a type that implements a particular trait rather than being of a specific type   We’ll demonstrate the first situation in the “Enabling Recursive Types with Boxes” section.  In the second case, transferring ownership of a large amount of data can take a long time because the data is copied around on the stack.  To improve performance in this situation, we can store the large amount of data on the heap in a box.  Then, only the small amount of pointer data is copied around on the stack, while the data it references stays in one place on the heap.  The third case is known as a trait object, and Chapter 17 devotes an entire section, “Using Trait Objects That Allow for Values of Different Types,” just to that topic.  So what you learn here you’ll apply again in Chapter 17!   Using a Box to Store Data on the Heap  Before we discuss this use case for Box&lt;T&gt;, we’ll cover the syntax and how to interact with values stored within a Box&lt;T&gt;.   Listing 15-1 shows how to use a box to store an i32 value on the heap:   // Filename: src/main.rs fn main() {     let b = Box::new(5);     println!(\"b = {}\", b); }  Listing 15-1: Storing an i32 value on the heap using a box   We define the variable b to have the value of a Box that points to the value 5, which is allocated on the heap.  This program will print b = 5; in this case, we can access the data in the box similar to how we would if this data were on the stack.  Just like any owned value, when a box goes out of scope, as b does at the end of main, it will be deallocated.  The deallocation happens for the box (stored on the stack) and the data it points to (stored on the heap).   Putting a single value on the heap isn’t very useful, so you won’t use boxes by themselves in this way very often.  Having values like a single i32 on the stack, where they’re stored by default, is more appropriate in the majority of situations.  Let’s look at a case where boxes allow us to define types that we wouldn’t be allowed to if we didn’t have boxes.   Enabling Recursive Types with Boxes  At compile time, Rust needs to know how much space a type takes up.  One type whose size can’t be known at compile time is a recursive type, where a value can have as part of itself another value of the same type.  Because this nesting of values could theoretically continue infinitely, Rust doesn’t know how much space a value of a recursive type needs.  However, boxes have a known size, so by inserting a box in a recursive type definition, you can have recursive types.   Let’s explore the cons list, which is a data type common in functional programming languages, as an example of a recursive type.  The cons list type we’ll define is straightforward except for the recursion; therefore, the concepts in the example we’ll work with will be useful any time you get into more complex situations involving recursive types.   More Information About the Cons List  A cons list is a data structure that comes from the Lisp programming language and its dialects.  In Lisp, the cons function (short for “construct function”) constructs a new pair from its two arguments, which usually are a single value and another pair.  These pairs containing pairs form a list.   The cons function concept has made its way into more general functional programming jargon: “to cons x onto y” informally means to construct a new container instance by putting the element x at the start of this new container, followed by the container y.   Each item in a cons list contains two elements: the value of the current item and the next item.  The last item in the list contains only a value called Nil without a next item.  A cons list is produced by recursively calling the cons function.  The canonical name to denote the base case of the recursion is Nil.  Note that this is not the same as the “null” or “nil” concept in Chapter 6, which is an invalid or absent value.   Although functional programming languages use cons lists frequently, the cons list isn’t a commonly used data structure in Rust.  Most of the time when you have a list of items in Rust, Vec&lt;T&gt; is a better choice to use.  Other, more complex recursive data types are useful in various situations, but by starting with the cons list, we can explore how boxes let us define a recursive data type without much distraction.   Listing 15-2 contains an enum definition for a cons list.  Note that this code won’t compile yet because the List type doesn’t have a known size, which we’ll demonstrate.   // Filename: src/main.rs enum List {     Cons(i32, List),     Nil, }  Listing 15-2: The first attempt at defining an enum to represent a cons list data structure of i32 values   Note: We’re implementing a cons list that holds only i32 values for the purposes of this example. We could have implemented it using generics, as we discussed in Chapter 10, to define a cons list type that could store values of any type.   Using the List type to store the list 1, 2, 3 would look like the code in Listing 15-3:   // Filename: src/main.rs use crate::List::{Cons, Nil};  fn main() {     let list = Cons(1, Cons(2, Cons(3, Nil))); }  Listing 15-3: Using the List enum to store the list 1, 2, 3   The first Cons value holds 1 and another List value.  This List value is another Cons value that holds 2 and another List value.  This List value is one more Cons value that holds 3 and a List value, which is finally Nil, the non-recursive variant that signals the end of the list.   If we try to compile the code in Listing 15-3, we get the error shown in Listing 15-4:   $ cargo run    Compiling cons-list v0.1.0 (file:///projects/cons-list) error[E0072]: recursive type `List` has infinite size  --&gt; src/main.rs:1:1   | 1 | enum List {   | ^^^^^^^^^ recursive type has infinite size 2 |     Cons(i32, List),   |               ---- recursive without indirection   |   = help: insert indirection (e.g., a `Box`, `Rc`, or `&amp;`)    at some point to make `List` representable  error[E0391]: cycle detected when processing `List`  --&gt; src/main.rs:1:1   | 1 | enum List {   | ^^^^^^^^^   |   = note: ...which again requires processing `List`, completing the cycle   = note: cycle used when computing dropck types for    `Canonical { max_universe: U0, variables: [], value: ParamEnvAnd {      param_env: ParamEnv { caller_bounds: [], reveal: UserFacing, def_id: None }, value: List }    }`  error: aborting due to 2 previous errors  Some errors have detailed explanations: E0072, E0391. For more information about an error, try `rustc --explain E0072`. error: could not compile `cons-list`.  To learn more, run the command again with --verbose.  Listing 15-4: The error we get when attempting to define a recursive enum   The error shows this type “has infinite size.” The reason is that we’ve defined List with a variant that is recursive: it holds another value of itself directly.  As a result, Rust can’t figure out how much space it needs to store a List value.  Let’s break down why we get this error a bit. First, let’s look at how Rust decides how much space it needs to store a value of a non-recursive type.   Computing the Size of a Non-Recursive Type  Recall the Message enum we defined in Listing 6-2 when we discussed enum definitions in Chapter 6:   enum Message {     Quit,     Move { x: i32, y: i32 },     Write(String),     ChangeColor(i32, i32, i32), }  To determine how much space to allocate for a Message value, Rust goes through each of the variants to see which variant needs the most space.  Rust sees that Message::Quit doesn’t need any space, Message::Move needs enough space to store two i32 values, and so forth.  Because only one variant will be used, the most space a Message value will need is the space it would take to store the largest of its variants.   Contrast this with what happens when Rust tries to determine how much space a recursive type like the List enum in Listing 15-2 needs.  The compiler starts by looking at the Cons variant, which holds a value of type i32 and a value of type List.  Therefore, Cons needs an amount of space equal to the size of an i32 plus the size of a List.  To figure out how much memory the List type needs, the compiler looks at the variants, starting with the Cons variant.  The Cons variant holds a value of type i32 and a value of type List, and this process continues infinitely, as shown in Figure 15-1.     Figure 15-1: An infinite List consisting of infinite Cons variants   Using Box to Get a Recursive Type with a Known Size  Rust can’t figure out how much space to allocate for recursively defined types, so the compiler gives the error in Listing 15-4. But the error does include this helpful suggestion:     = help: insert indirection (e.g., a `Box`, `Rc`, or `&amp;`)    at some point to make `List` representable  In this suggestion, “indirection” means that instead of storing a value directly, we’ll change the data structure to store the value indirectly by storing a pointer to the value instead.   Because a Box&lt;T&gt; is a pointer, Rust always knows how much space a Box&lt;T&gt; needs: a pointer’s size doesn’t change based on the amount of data it’s pointing to.  This means we can put a Box&lt;T&gt; inside the Cons variant instead of another List value directly.  The Box&lt;T&gt; will point to the next List value that will be on the heap rather than inside the Cons variant.  Conceptually, we still have a list, created with lists “holding” other lists, but this implementation is now more like placing the items next to one another rather than inside one another.   We can change the definition of the List enum in Listing 15-2 and the usage of the List in Listing 15-3 to the code in Listing 15-5, which will compile:   // Filename: src/main.rs enum List {     Cons(i32, Box&lt;List&gt;),     Nil, }  use crate::List::{Cons, Nil};  fn main() {     let list = Cons(1, Box::new(Cons(2, Box::new(Cons(3, Box::new(Nil)))))); }  Listing 15-5: Definition of List that uses Box in order to have a known size   The Cons variant will need the size of an i32 plus the space to store the box’s pointer data.  The Nil variant stores no values, so it needs less space than the Cons variant.  We now know that any List value will take up the size of an i32 plus the size of a box’s pointer data.  By using a box, we’ve broken the infinite, recursive chain, so the compiler can figure out the size it needs to store a List value.  Figure 15-2 shows what the Cons variant looks like now.     Figure 15-2: A List that is not infinitely sized because Cons holds a Box   Boxes provide only the indirection and heap allocation;  they don’t have any other special capabilities, like those we’ll see with the other smart pointer types.  They also don’t have any performance overhead that these special capabilities incur, so they can be useful in cases like the cons list where the indirection is the only feature we need.  We’ll look at more use cases for boxes in Chapter 17, too.   The Box&lt;T&gt; type is a smart pointer because it implements the Deref trait, which allows Box&lt;T&gt; values to be treated like references.  When a Box&lt;T&gt; value goes out of scope, the heap data that the box is pointing to is cleaned up as well because of the Drop trait implementation.  Let’s explore these two traits in more detail.  These two traits will be even more important to the functionality provided by the other smart pointer types we’ll discuss in the rest of this chapter.   Treating Smart Pointers Like Regular References with the Deref Trait  Implementing the Deref trait allows you to customize the behavior of the dereference operator, * (as opposed to the multiplication or glob operator).  By implementing Deref in such a way that a smart pointer can be treated like a regular reference, you can write code that operates on references and use that code with smart pointers too.   Let’s first look at how the dereference operator works with regular references.  Then we’ll try to define a custom type that behaves like Box&lt;T&gt;, and see why the dereference operator doesn’t work like a reference on our newly defined type.  We’ll explore how implementing the Deref trait makes it possible for smart pointers to work in ways similar to references.  Then we’ll look at Rust’s deref coercion feature and how it lets us work with either references or smart pointers.   Note: there’s one big difference between the MyBox&lt;T&gt; type we’re about to build and the real Box&lt;T&gt;: our version will not store its data on the heap. We are focusing this example on Deref, so where the data is actually stored is less important than the pointer-like behavior.   Following the Pointer to the Value with the Dereference Operator  A regular reference is a type of pointer, and one way to think of a pointer is as an arrow to a value stored somewhere else.  In Listing 15-6, we create a reference to an i32 value and then use the dereference operator to follow the reference to the data:  // Filename: src/main.rs fn main() {     let x = 5;     let y = &amp;x;      assert_eq!(5, x);     assert_eq!(5, *y); }  Listing 15-6: Using the dereference operator to follow a reference to an i32 value   The variable x holds an i32 value, 5.  We set y equal to a reference to x.  We can assert that x is equal to 5.  However, if we want to make an assertion about the value in y, we have to use *y to follow the reference to the value it’s pointing to (hence dereference).  Once we dereference y, we have access to the integer value y is pointing to that we can compare with 5.   If we tried to write assert_eq!(5, y); instead, we would get this compilation error:  $ cargo run    Compiling deref-example v0.1.0 (file:///projects/deref-example) error[E0277]: can't compare `{integer}` with `&amp;{integer}`  --&gt; src/main.rs:6:5   | 6 |     assert_eq!(5, y);   |     ^^^^^^^^^^^^^^^^^ no implementation for `{integer} == &amp;{integer}`   |   = help: the trait `std::cmp::PartialEq&lt;&amp;{integer}&gt;` is not implemented for `{integer}`   = note: this error originates in a macro outside of the current crate    (in Nightly builds, run with -Z external-macro-backtrace for more info)  error: aborting due to previous error  For more information about this error, try `rustc --explain E0277`. error: could not compile `deref-example`.  To learn more, run the command again with --verbose.  Comparing a number and a reference to a number isn’t allowed because they’re different types.  We must use the dereference operator to follow the reference to the value it’s pointing to.   Using Box Like a Reference  We can rewrite the code in Listing 15-6 to use a Box&lt;T&gt; instead of a reference; the dereference operator will work as shown in Listing 15-7:  // Filename: src/main.rs fn main() {     let x = 5;     let y = Box::new(x);      assert_eq!(5, x);     assert_eq!(5, *y); }  Listing 15-7: Using the dereference operator on a Box   The only difference between Listing 15-7 and Listing 15-6 is that here we set y to be an instance of a box pointing to the value in x rather than a reference pointing to the value of x.  In the last assertion, we can use the dereference operator to follow the box’s pointer in the same way that we did when y was a reference.  Next, we’ll explore what is special about Box&lt;T&gt; that enables us to use the dereference operator by defining our own box type.   Defining Our Own Smart Pointer  Let’s build a smart pointer similar to the Box&lt;T&gt; type provided by the standard library to experience how smart pointers behave differently from references by default.  Then we’ll look at how to add the ability to use the dereference operator.   The Box&lt;T&gt; type is ultimately defined as a tuple struct with one element, so Listing 15-8 defines a MyBox&lt;T&gt; type in the same way. We’ll also define a new function to match the new function defined on Box&lt;T&gt;.   // Filename: src/main.rs struct MyBox&lt;T&gt;(T);  impl&lt;T&gt; MyBox&lt;T&gt; {     fn new(x: T) -&gt; MyBox&lt;T&gt; {         MyBox(x)     } }  Listing 15-8: Defining a MyBox&lt;T&gt; type   We define a struct named MyBox and declare a generic parameter T, because we want our type to hold values of any type. The MyBox type is a tuple struct with one element of type T. The MyBox::new function takes one parameter of type T and returns a MyBox instance that holds the value passed in.   Let’s try adding the main function in Listing 15-7 to Listing 15-8 and changing it to use the MyBox&lt;T&gt; type we’ve defined instead of Box&lt;T&gt;. The code in Listing 15-9 won’t compile because Rust doesn’t know how to dereference MyBox.  // Filename: src/main.rs fn main() {     let x = 5;     let y = MyBox::new(x);      assert_eq!(5, x);     assert_eq!(5, *y); }  Listing 15-9: Attempting to use MyBox in the same way we used references and Box   Here’s the resulting compilation error:  $ cargo run    Compiling deref-example v0.1.0 (file:///projects/deref-example) error[E0614]: type `MyBox&lt;{integer}&gt;` cannot be dereferenced   --&gt; src/main.rs:14:19    | 14 |     assert_eq!(5, *y);    |                   ^^  error: aborting due to previous error  For more information about this error, try `rustc --explain E0614`. error: could not compile `deref-example`.  To learn more, run the command again with --verbose.   Our MyBox&lt;T&gt; type can’t be dereferenced because we haven’t implemented that ability on our type.  To enable dereferencing with the * operator, we implement the Deref trait.   Treating a Type Like a Reference by Implementing the Deref Trait  As discussed in Chapter 10, to implement a trait, we need to provide implementations for the trait’s required methods.  The Deref trait, provided by the standard library, requires us to implement one method named deref that borrows self and returns a reference to the inner data.  Listing 15-10 contains an implementation of Deref to add to the definition of MyBox:  // Filename: src/main.rs use std::ops::Deref;  impl&lt;T&gt; Deref for MyBox&lt;T&gt; {     type Target = T;      fn deref(&amp;self) -&gt; &amp;T {         &amp;self.0     } }  Listing 15-10: Implementing Deref on MyBox&lt;T&gt;   The type Target = T; syntax defines an associated type for the Deref trait to use.  Associated types are a slightly different way of declaring a generic parameter, but you don’t need to worry about them for now; we’ll cover them in more detail in Chapter 19.   We fill in the body of the deref method with &amp;self.0 so deref returns a reference to the value we want to access with the * operator.  The main function in Listing 15-9 that calls * on the MyBox&lt;T&gt; value now compiles, and the assertions pass!   Without the Deref trait, the compiler can only dereference &amp; references.  The deref method gives the compiler the ability to take a value of any type that implements Deref and call the deref method to get a &amp; reference that it knows how to dereference.   When we entered *y in Listing 15-9, behind the scenes Rust actually ran this code:  *(y.deref())  Rust substitutes the * operator with a call to the deref method and then a plain dereference so we don’t have to think about whether or not we need to call the deref method.  This Rust feature lets us write code that functions identically whether we have a regular reference or a type that implements Deref.   The reason the deref method returns a reference to a value, and that the plain dereference outside the parentheses in *(y.deref()) is still necessary, is the ownership system.  If the deref method returned the value directly instead of a reference to the value, the value would be moved out of self.  We don’t want to take ownership of the inner value inside MyBox&lt;T&gt; in this case or in most cases where we use the dereference operator.   Note that the * operator is replaced with a call to the deref method and then a call to the * operator just once, each time we use a * in our code.  Because the substitution of the * operator does not recurse infinitely, we end up with data of type i32, which matches the 5 in assert_eq! in Listing 15-9.   Implicit Deref Coercions with Functions and Methods  Deref coercion is a convenience that Rust performs on arguments to functions and methods.  Deref coercion works only on types that implement the Deref trait.  Deref coercion converts such a type into a reference to another type.  For example, deref coercion can convert &amp;String to &amp;str because String implements the Deref trait such that it returns str.  Deref coercion happens automatically when we pass a reference to a particular type’s value as an argument to a function or method that doesn’t match the parameter type in the function or method definition.  A sequence of calls to the deref method converts the type we provided into the type the parameter needs.   Deref coercion was added to Rust so that programmers writing function and method calls don’t need to add as many explicit references and dereferences with &amp; and *.  The deref coercion feature also lets us write more code that can work for either references or smart pointers.   To see deref coercion in action, let’s use the MyBox&lt;T&gt; type we defined in Listing 15-8 as well as the implementation of Deref that we added in Listing 15-10.  Listing 15-11 shows the definition of a function that has a string slice parameter:  // Filename: src/main.rs fn hello(name: &amp;str) {     println!(\"Hello, {}!\", name); }  Listing 15-11: A hello function that has the parameter name of type &amp;str   We can call the hello function with a string slice as an argument, such as hello(\"Rust\"); for example.  Deref coercion makes it possible to call hello with a reference to a value of type MyBox&lt;String&gt;, as shown in Listing 15-12:  // Filename: src/main.rs fn main() {     let m = MyBox::new(String::from(\"Rust\"));     hello(&amp;m); }  Listing 15-12: Calling hello with a reference to a MyBox value, which works because of deref coercion   Here we’re calling the hello function with the argument &amp;m, which is a reference to a MyBox&lt;String&gt; value.  Because we implemented the Deref trait on MyBox&lt;T&gt; in Listing 15-10, Rust can turn &amp;MyBox&lt;String&gt; into &amp;String by calling deref.  The standard library provides an implementation of Deref on String that returns a string slice, and this is in the API documentation for Deref.  Rust calls deref again to turn the &amp;String into &amp;str, which matches the hello function’s definition.   If Rust didn’t implement deref coercion, we would have to write the code in Listing 15-13 instead of the code in Listing 15-12 to call hello with a value of type &amp;MyBox&lt;String&gt;.   // Filename: src/main.rs fn main() {     let m = MyBox::new(String::from(\"Rust\"));     hello(&amp;(*m)[..]); }  Listing 15-13: The code we would have to write if Rust didn’t have deref coercion   The (*m) dereferences the MyBox&lt;String&gt; into a String.  Then the &amp; and [..] take a string slice of the String that is equal to the whole string to match the signature of hello.  The code without deref coercions is harder to read, write, and understand with all of these symbols involved.  Deref coercion allows Rust to handle these conversions for us automatically.   When the Deref trait is defined for the types involved, Rust will analyze the types and use Deref::deref as many times as necessary to get a reference to match the parameter’s type.  The number of times that Deref::deref needs to be inserted is resolved at compile time, so there is no runtime penalty for taking advantage of deref coercion!   How Deref Coercion Interacts with Mutability  Similar to how you use the Deref trait to override the * operator on immutable references, you can use the DerefMut trait to override the * operator on mutable references.   Rust does deref coercion when it finds types and trait implementations in three cases:      From &amp;T to &amp;U when T: Deref&lt;Target=U&gt;   From &amp;mut T to &amp;mut U when T: DerefMut&lt;Target=U&gt;   From &amp;mut T to &amp;U when T: Deref&lt;Target=U&gt;   The first two cases are the same except for mutability.  The first case states that if you have a &amp;T, and T implements Deref to some type U, you can get a &amp;U transparently.  The second case states that the same deref coercion happens for mutable references.   The third case is trickier: Rust will also coerce a mutable reference to an immutable one.  But the reverse is not possible: immutable references will never coerce to mutable references.  Because of the borrowing rules, if you have a mutable reference, that mutable reference must be the only reference to that data (otherwise, the program wouldn’t compile).  Converting one mutable reference to one immutable reference will never break the borrowing rules.  Converting an immutable reference to a mutable reference would require that the initial immutable reference is the only immutable reference to that data, but the borrowing rules don’t guarantee that.  Therefore, Rust can’t make the assumption that converting an immutable reference to a mutable reference is possible.   Running Code on Cleanup with the Drop Trait  The second trait important to the smart pointer pattern is Drop, which lets you customize what happens when a value is about to go out of scope.  You can provide an implementation for the Drop trait on any type, and the code you specify can be used to release resources like files or network connections.  We’re introducing Drop in the context of smart pointers because the functionality of the Drop trait is almost always used when implementing a smart pointer.  For example, Box&lt;T&gt; customizes Drop to deallocate the space on the heap that the box points to.   In some languages, the programmer must call code to free memory or resources every time they finish using an instance of a smart pointer.  If they forget, the system might become overloaded and crash.  In Rust, you can specify that a particular bit of code be run whenever a value goes out of scope, and the compiler will insert this code automatically.  As a result, you don’t need to be careful about placing cleanup code everywhere in a program that an instance of a particular type is finished with—you still won’t leak resources!   Specify the code to run when a value goes out of scope by implementing the Drop trait.  The Drop trait requires you to implement one method named drop that takes a mutable reference to self.  To see when Rust calls drop, let’s implement drop with println! statements for now.   Listing 15-14 shows a CustomSmartPointer struct whose only custom functionality is that it will print Dropping CustomSmartPointer! when the instance goes out of scope.  This example demonstrates when Rust runs the drop function.   // Filename: src/main.rs struct CustomSmartPointer {     data: String, }  impl Drop for CustomSmartPointer {     fn drop(&amp;mut self) {         println!(\"Dropping CustomSmartPointer with data `{}`!\", self.data);     } }  fn main() {     let c = CustomSmartPointer {         data: String::from(\"my stuff\"),     };     let d = CustomSmartPointer {         data: String::from(\"other stuff\"),     };     println!(\"CustomSmartPointers created.\"); }  Listing 15-14: A CustomSmartPointer struct that implements the Drop trait where we would put our cleanup code   The Drop trait is included in the prelude, so we don’t need to bring it into scope.  We implement the Drop trait on CustomSmartPointer and provide an implementation for the drop method that calls println!.  The body of the drop function is where you would place any logic that you wanted to run when an instance of your type goes out of scope. We’re printing some text here to demonstrate when Rust will call drop.   In main, we create two instances of CustomSmartPointer and then print CustomSmartPointers created. At the end of main, our instances of CustomSmartPointer will go out of scope, and Rust will call the code we put in the drop method, printing our final message.  Note that we didn’t need to call the drop method explicitly.   When we run this program, we’ll see the following output:   $ cargo run    Compiling drop-example v0.1.0 (file:///projects/drop-example)     Finished dev [unoptimized + debuginfo] target(s) in 0.60s      Running `target/debug/drop-example` CustomSmartPointers created. Dropping CustomSmartPointer with data `other stuff`! Dropping CustomSmartPointer with data `my stuff`!  Rust automatically called drop for us when our instances went out of scope, calling the code we specified.  Variables are dropped in the reverse order of their creation, so d was dropped before c.  This example gives you a visual guide to how the drop method works; usually you would specify the cleanup code that your type needs to run rather than a print message.   Dropping a Value Early with std::mem::drop  Unfortunately, it’s not straightforward to disable the automatic drop functionality.  Disabling drop isn’t usually necessary; the whole point of the Drop trait is that it’s taken care of automatically.  Occasionally, however, you might want to clean up a value early.  One example is when using smart pointers that manage locks: you might want to force the drop method that releases the lock so that other code in the same scope can acquire the lock.  Rust doesn’t let you call the Drop trait’s drop method manually; instead you have to call the std::mem::drop function provided by the standard library if you want to force a value to be dropped before the end of its scope.   If we try to call the Drop trait’s drop method manually by modifying the main function from Listing 15-14, as shown in Listing 15-15, we’ll get a compiler error:  fn main() {     let c = CustomSmartPointer {         data: String::from(\"some data\"),     };     println!(\"CustomSmartPointer created.\");     c.drop();     println!(\"CustomSmartPointer dropped before the end of main.\"); }  Listing 15-15: Attempting to call the drop method from the Drop trait manually to clean up early   When we try to compile this code, we’ll get this error:   $ cargo run    Compiling drop-example v0.1.0 (file:///projects/drop-example) error[E0040]: explicit use of destructor method   --&gt; src/main.rs:16:7    | 16 |     c.drop();    |       ^^^^ explicit destructor calls not allowed  error: aborting due to previous error  For more information about this error, try `rustc --explain E0040`. error: could not compile `drop-example`.  To learn more, run the command again with --verbose.  This error message states that we’re not allowed to explicitly call drop.  The error message uses the term destructor, which is the general programming term for a function that cleans up an instance.  A destructor is analogous to a constructor, which creates an instance.  The drop function in Rust is one particular destructor.   Rust doesn’t let us call drop explicitly because Rust would still automatically call drop on the value at the end of main.  This would be a double free error because Rust would be trying to clean up the same value twice.   We can’t disable the automatic insertion of drop when a value goes out of scope, and we can’t call the drop method explicitly.  So, if we need to force a value to be cleaned up early, we can use the std::mem::drop function.   The std::mem::drop function is different from the drop method in the Drop trait.  We call it by passing the value we want to force to be dropped early as an argument.  The function is in the prelude, so we can modify main in Listing 15-15 to call the drop function, as shown in Listing 15-16:  // Filename: src/main.rs fn main() {     let c = CustomSmartPointer {         data: String::from(\"some data\"),     };     println!(\"CustomSmartPointer created.\");     drop(c);     println!(\"CustomSmartPointer dropped before the end of main.\"); }  Listing 15-16: Calling std::mem::drop to explicitly drop a value before it goes out of scope   Running this code will print the following:   $ cargo run    Compiling drop-example v0.1.0 (file:///projects/drop-example)     Finished dev [unoptimized + debuginfo] target(s) in 0.73s      Running `target/debug/drop-example` CustomSmartPointer created. Dropping CustomSmartPointer with data `some data`! CustomSmartPointer dropped before the end of main.  The text Dropping CustomSmartPointer with data 'some data'! is printed between the CustomSmartPointer created.  and CustomSmartPointer dropped before the end of main. text, showing that the drop method code is called to drop c at that point.   You can use code specified in a Drop trait implementation in many ways to make cleanup convenient and safe: for instance, you could use it to create your own memory allocator! With the Drop trait and Rust’s ownership system, you don’t have to remember to clean up because Rust does it automatically.   You also don’t have to worry about problems resulting from accidentally cleaning up values still in use: the ownership system that makes sure references are always valid also ensures that drop gets called only once when the value is no longer being used.   Now that we’ve examined Box&lt;T&gt; and some of the characteristics of smart pointers, let’s look at a few other smart pointers defined in the standard library.   15.4 start Rc, the Reference Counted Smart Pointer  In the majority of cases, ownership is clear: you know exactly which variable owns a given value. However, there are cases when a single value might have multiple owners. For example, in graph data structures, multiple edges might point to the same node, and that node is conceptually owned by all of the edges that point to it. A node shouldn’t be cleaned up unless it doesn’t have any edges pointing to it.   To enable multiple ownership, Rust has a type called Rc&lt;T&gt;, which is an abbreviation for reference counting. The Rc&lt;T&gt; type keeps track of the number of references to a value which determines whether or not a value is still in use. If there are zero references to a value, the value can be cleaned up without any references becoming invalid.   Imagine Rc&lt;T&gt; as a TV in a family room. When one person enters to watch TV, they turn it on.  Others can come into the room and watch the TV. When the last person leaves the room, they turn off the TV because it’s no longer being used.  If someone turns off the TV while others are still watching it, there would be uproar from the remaining TV watchers!   We use the Rc&lt;T&gt; type when we want to allocate some data on the heap for multiple parts of our program to read and we can’t determine at compile time which part will finish using the data last.  If we knew which part would finish last, we could just make that part the data’s owner, and the normal ownership rules enforced at compile time would take effect.   Note that Rc&lt;T&gt; is only for use in single-threaded scenarios. When we discuss concurrency in Chapter 16, we’ll cover how to do reference counting in multithreaded programs.   Using Rc to Share Data  Let’s return to our cons list example in Listing 15-5. Recall that we defined it using Box&lt;T&gt;.  This time, we’ll create two lists that both share ownership of a third list. Conceptually, this looks similar to Figure 15-3:     Figure 15-3: Two lists, b and c, sharing ownership of a third list, a   We’ll create list a that contains 5 and then 10.  Then we’ll make two more lists: b that starts with 3 and c that starts with 4.  Both b and c lists will then continue on to the first a list containing 5 and 10.  In other words, both lists will share the first list containing 5 and 10.   Trying to implement this scenario using our definition of List with Box&lt;T&gt; won’t work, as shown in Listing 15-17:   // Filename: src/main.rs enum List {     Cons(i32, Box&lt;List&gt;),     Nil, }  use crate::List::{Cons, Nil};  fn main() {     let a = Cons(5, Box::new(Cons(10, Box::new(Nil))));     let b = Cons(3, Box::new(a));     let c = Cons(4, Box::new(a)); }  Listing 15-17: Demonstrating we’re not allowed to have two lists using Box that try to share ownership of a third list   When we compile this code, we get this error:   $ cargo run    Compiling cons-list v0.1.0 (file:///projects/cons-list) error[E0382]: use of moved value: `a`   --&gt; src/main.rs:11:30    | 9  |     let a = Cons(5, Box::new(Cons(10, Box::new(Nil))));    |         - move occurs because `a` has type `List`, which does not implement the `Copy` trait 10 |     let b = Cons(3, Box::new(a));    |                              - value moved here 11 |     let c = Cons(4, Box::new(a));    |                              ^ value used here after move  error: aborting due to previous error  For more information about this error, try `rustc --explain E0382`. error: could not compile `cons-list`.  To learn more, run the command again with --verbose.   The Cons variants own the data they hold, so when we create the b list, a is moved into b and b owns a.  Then, when we try to use a again when creating c, we’re not allowed to because a has been moved.   We could change the definition of Cons to hold references instead, but then we would have to specify lifetime parameters. By specifying lifetime parameters, we would be specifying that every element in the list will live at least as long as the entire list. The borrow checker wouldn’t let us compile let a = Cons(10, &amp;Nil); for example, because the temporary Nil value would be dropped before a could take a reference to it.   Instead, we’ll change our definition of List to use Rc&lt;T&gt; in place of Box&lt;T&gt;, as shown in Listing 15-18.  Each Cons variant will now hold a value and an Rc&lt;T&gt; pointing to a List.  When we create b, instead of taking ownership of a, we’ll clone the Rc&lt;List&gt; that a is holding, thereby increasing the number of references from one to two and letting a and b share ownership of the data in that Rc&lt;List&gt;. We’ll also clone a when creating c, increasing the number of references from two to three.  Every time we call Rc::clone, the reference count to the data within the Rc&lt;List&gt; will increase, and the data won’t be cleaned up unless there are zero references to it.   // Filename: src/main.rs enum List {     Cons(i32, Rc&lt;List&gt;),     Nil, }  use crate::List::{Cons, Nil}; use std::rc::Rc;  fn main() {     let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil)))));     let b = Cons(3, Rc::clone(&amp;a));     let c = Cons(4, Rc::clone(&amp;a)); }  Listing 15-18: A definition of List that uses Rc   We need to add a use statement to bring Rc&lt;T&gt; into scope because it’s not in the prelude. In main, we create the list holding 5 and 10 and store it in a new Rc&lt;List&gt; in a. Then when we create b and c, we call the Rc::clone function and pass a reference to the Rc&lt;List&gt; in a as an argument.   We could have called a.clone() rather than Rc::clone(&amp;a), but Rust’s convention is to use Rc::clone in this case.  The implementation of Rc::clone doesn’t make a deep copy of all the data like most types’ implementations of clone do.  The call to Rc::clone only increments the reference count, which doesn’t take much time. Deep copies of data can take a lot of time. By using Rc::clone for reference counting, we can visually distinguish between the deep-copy kinds of clones and the kinds of clones that increase the reference count. When looking for performance problems in the code, we only need to consider the deep-copy clones and can disregard calls to Rc::clone.   Cloning an Rc Increases the Reference Count  Let’s change our working example in Listing 15-18 so we can see the reference counts changing as we create and drop references to the Rc&lt;List&gt; in a.   In Listing 15-19, we’ll change main so it has an inner scope around list c; then we can see how the reference count changes when c goes out of scope.   // Filename: src/main.rs fn main() {     let a = Rc::new(Cons(5, Rc::new(Cons(10, Rc::new(Nil)))));     println!(\"count after creating a = {}\", Rc::strong_count(&amp;a));     let b = Cons(3, Rc::clone(&amp;a));     println!(\"count after creating b = {}\", Rc::strong_count(&amp;a));     {         let c = Cons(4, Rc::clone(&amp;a));         println!(\"count after creating c = {}\", Rc::strong_count(&amp;a));     }     println!(\"count after c goes out of scope = {}\", Rc::strong_count(&amp;a)); }  Listing 15-19: Printing the reference count   At each point in the program where the reference count changes, we print the reference count, which we can get by calling the Rc::strong_count function. This function is named strong_count rather than count because the Rc&lt;T&gt; type also has a weak_count; we’ll see what weak_count is used for in the “Preventing Reference Cycles: Turning an Rc&lt;T&gt; into a Weak&lt;T&gt;” section.   This code prints the following:  $ cargo run    Compiling cons-list v0.1.0 (file:///projects/cons-list)     Finished dev [unoptimized + debuginfo] target(s) in 0.45s      Running `target/debug/cons-list` count after creating a = 1 count after creating b = 2 count after creating c = 3 count after c goes out of scope = 2  We can see that the Rc&lt;List&gt; in a has an initial reference count of 1; then each time we call clone, the count goes up by 1. When c goes out of scope, the count goes down by 1. We don’t have to call a function to decrease the reference count like we have to call Rc::clone to increase the reference count: the implementation of the Drop trait decreases the reference count automatically when an Rc&lt;T&gt; value goes out of scope.   What we can’t see in this example is that when b and then a go out of scope at the end of main, the count is then 0, and the Rc&lt;List&gt; is cleaned up completely at that point. Using Rc&lt;T&gt; allows a single value to have multiple owners, and the count ensures that the value remains valid as long as any of the owners still exist.   Via immutable references, Rc&lt;T&gt; allows you to share data between multiple parts of your program for reading only. If Rc&lt;T&gt; allowed you to have multiple mutable references too, you might violate one of the borrowing rules discussed in Chapter 4: multiple mutable borrows to the same place can cause data races and inconsistencies. But being able to mutate data is very useful! In the next section, we’ll discuss the interior mutability pattern and the RefCell&lt;T&gt; type that you can use in conjunction with an Rc&lt;T&gt; to work with this immutability restriction.   RefCell and the Interior Mutability Pattern  Interior mutability is a design pattern in Rust that allows you to mutate data even when there are immutable references to that data; normally, this action is disallowed by the borrowing rules. To mutate data, the pattern uses unsafe code inside a data structure to bend Rust’s usual rules that govern mutation and borrowing. We haven’t yet covered unsafe code; we will in Chapter 19. We can use types that use the interior mutability pattern when we can ensure that the borrowing rules will be followed at runtime, even though the compiler can’t guarantee that. The unsafe code involved is then wrapped in a safe API, and the outer type is still immutable.   Let’s explore this concept by looking at the RefCell&lt;T&gt; type that follows the interior mutability pattern.   Enforcing Borrowing Rules at Runtime with RefCell  Unlike Rc&lt;T&gt;, the RefCell&lt;T&gt; type represents single ownership over the data it holds. So, what makes RefCell&lt;T&gt; different from a type like Box&lt;T&gt;? Recall the borrowing rules you learned in Chapter 4:     At any given time, you can have either (but not both of) one mutable reference or any number of immutable references.   References must always be valid.   With references and Box&lt;T&gt;, the borrowing rules’ invariants are enforced at compile time. With RefCell&lt;T&gt;, these invariants are enforced at runtime. With references, if you break these rules, you’ll get a compiler error. With RefCell&lt;T&gt;, if you break these rules, your program will panic and exit.   The advantages of checking the borrowing rules at compile time are that errors will be caught sooner in the development process, and there is no impact on runtime performance because all the analysis is completed beforehand. For those reasons, checking the borrowing rules at compile time is the best choice in the majority of cases, which is why this is Rust’s default.   The advantage of checking the borrowing rules at runtime instead is that certain memory-safe scenarios are then allowed, whereas they are disallowed by the compile-time checks. Static analysis, like the Rust compiler, is inherently conservative. Some properties of code are impossible to detect by analyzing the code: the most famous example is the Halting Problem, which is beyond the scope of this book but is an interesting topic to research.   Because some analysis is impossible, if the Rust compiler can’t be sure the code complies with the ownership rules, it might reject a correct program; in this way, it’s conservative. If Rust accepted an incorrect program, users wouldn’t be able to trust in the guarantees Rust makes. However, if Rust rejects a correct program, the programmer will be inconvenienced, but nothing catastrophic can occur. The RefCell&lt;T&gt; type is useful when you’re sure your code follows the borrowing rules but the compiler is unable to understand and guarantee that.   Similar to Rc&lt;T&gt;, RefCell&lt;T&gt; is only for use in single-threaded scenarios and will give you a compile-time error if you try using it in a multithreaded context. We’ll talk about how to get the functionality of RefCell&lt;T&gt; in a multithreaded program in Chapter 16.   Here is a recap of the reasons to choose Box&lt;T&gt;, Rc&lt;T&gt;, or RefCell&lt;T&gt;:      Rc&lt;T&gt; enables multiple owners of the same data; Box&lt;T&gt; and RefCell&lt;T&gt; have single owners.   Box&lt;T&gt; allows immutable or mutable borrows checked at compile time; Rc&lt;T&gt; allows only immutable borrows checked at compile time; RefCell&lt;T&gt; allows immutable or mutable borrows checked at runtime.   Because RefCell&lt;T&gt; allows mutable borrows checked at runtime, you can mutate the value inside the RefCell&lt;T&gt; even when the RefCell&lt;T&gt; is immutable.   Mutating the value inside an immutable value is the interior mutability pattern. Let’s look at a situation in which interior mutability is useful and examine how it’s possible.   Interior Mutability: A Mutable Borrow to an Immutable Value  A consequence of the borrowing rules is that when you have an immutable value, you can’t borrow it mutably. For example, this code won’t compile:   fn main() {     let x = 5;     let y = &amp;mut x; }  If you tried to compile this code, you’d get the following error:   $ cargo run    Compiling borrowing v0.1.0 (file:///projects/borrowing) error[E0596]: cannot borrow `x` as mutable, as it is not declared as mutable  --&gt; src/main.rs:3:13   | 2 |     let x = 5;   |         - help: consider changing this to be mutable: `mut x` 3 |     let y = &amp;mut x;   |             ^^^^^^ cannot borrow as mutable  error: aborting due to previous error  For more information about this error, try `rustc --explain E0596`. error: could not compile `borrowing`.  To learn more, run the command again with --verbose.  However, there are situations in which it would be useful for a value to mutate itself in its methods but appear immutable to other code. Code outside the value’s methods would not be able to mutate the value. Using RefCell&lt;T&gt; is one way to get the ability to have interior mutability. But RefCell&lt;T&gt; doesn’t get around the borrowing rules completely: the borrow checker in the compiler allows this interior mutability, and the borrowing rules are checked at runtime instead. If you violate the rules, you’ll get a panic! instead of a compiler error.   Let’s work through a practical example where we can use RefCell&lt;T&gt; to mutate an immutable value and see why that is useful.   A Use Case for Interior Mutability: Mock Objects  A test double is the general programming concept for a type used in place of another type during testing. Mock objects are specific types of test doubles that record what happens during a test so you can assert that the correct actions took place.   Rust doesn’t have objects in the same sense as other languages have objects, and Rust doesn’t have mock object functionality built into the standard library as some other languages do. However, you can definitely create a struct that will serve the same purposes as a mock object.   Here’s the scenario we’ll test: we’ll create a library that tracks a value against a maximum value and sends messages based on how close to the maximum value the current value is. This library could be used to keep track of a user’s quota for the number of API calls they’re allowed to make, for example.   Our library will only provide the functionality of tracking how close to the maximum a value is and what the messages should be at what times. Applications that use our library will be expected to provide the mechanism for sending the messages: the application could put a message in the application, send an email, send a text message, or something else. The library doesn’t need to know that detail. All it needs is something that implements a trait we’ll provide called Messenger. Listing 15-20 shows the library code:   // Filename: src/lib.rs pub trait Messenger {     fn send(&amp;self, msg: &amp;str); }  pub struct LimitTracker&lt;'a, T: Messenger&gt; {     messenger: &amp;'a T,     value: usize,     max: usize, }  impl&lt;'a, T&gt; LimitTracker&lt;'a, T&gt; where     T: Messenger, {     pub fn new(messenger: &amp;T, max: usize) -&gt; LimitTracker&lt;T&gt; {         LimitTracker {             messenger,             value: 0,             max,         }     }      pub fn set_value(&amp;mut self, value: usize) {         self.value = value;          let percentage_of_max = self.value as f64 / self.max as f64;          if percentage_of_max &gt;= 1.0 {             self.messenger.send(\"Error: You are over your quota!\");         } else if percentage_of_max &gt;= 0.9 {             self.messenger                 .send(\"Urgent warning: You've used up over 90% of your quota!\");         } else if percentage_of_max &gt;= 0.75 {             self.messenger                 .send(\"Warning: You've used up over 75% of your quota!\");         }     } }  Listing 15-20: A library to keep track of how close a value is to a maximum value and warn when the value is at certain levels   One important part of this code is that the Messenger trait has one method called send that takes an immutable reference to self and the text of the message. This is the interface our mock object needs to have. The other important part is that we want to test the behavior of the set_value method on the LimitTracker. We can change what we pass in for the value parameter, but set_value doesn’t return anything for us to make assertions on. We want to be able to say that if we create a LimitTracker with something that implements the Messenger trait and a particular value for max, when we pass different numbers for value, the messenger is told to send the appropriate messages.   We need a mock object that, instead of sending an email or text message when we call send, will only keep track of the messages it’s told to send. We can create a new instance of the mock object, create a LimitTracker that uses the mock object, call the set_value method on LimitTracker, and then check that the mock object has the messages we expect. Listing 15-21 shows an attempt to implement a mock object to do just that, but the borrow checker won’t allow it:   // Filename: src/lib.rs #[cfg(test)] mod tests {     use super::*;      struct MockMessenger {         sent_messages: Vec&lt;String&gt;,     }      impl MockMessenger {         fn new() -&gt; MockMessenger {             MockMessenger {                 sent_messages: vec![],             }         }     }      impl Messenger for MockMessenger {         fn send(&amp;self, message: &amp;str) {             self.sent_messages.push(String::from(message));         }     }      #[test]     fn it_sends_an_over_75_percent_warning_message() {         let mock_messenger = MockMessenger::new();         let mut limit_tracker = LimitTracker::new(&amp;mock_messenger, 100);          limit_tracker.set_value(80);          assert_eq!(mock_messenger.sent_messages.len(), 1);     } }  Listing 15-21: An attempt to implement a MockMessenger that isn’t allowed by the borrow checker   This test code defines a MockMessenger struct that has a sent_messages field with a Vec of String values to keep track of the messages it’s told to send. We also define an associated function new to make it convenient to create new MockMessenger values that start with an empty list of messages. We then implement the Messenger trait for MockMessenger so we can give a MockMessenger to a LimitTracker. In the definition of the send method, we take the message passed in as a parameter and store it in the MockMessenger list of sent_messages.   In the test, we’re testing what happens when the LimitTracker is told to set value to something that is more than 75 percent of the max value. First, we create a new MockMessenger, which will start with an empty list of messages. Then we create a new LimitTracker and give it a reference to the new MockMessenger and a max value of 100. We call the set_value method on the LimitTracker with a value of 80, which is more than 75 percent of 100. Then we assert that the list of messages that the MockMessenger is keeping track of should now have one message in it.   However, there’s one problem with this test, as shown here:   $ cargo test    Compiling limit-tracker v0.1.0 (file:///projects/limit-tracker) error[E0596]: cannot borrow `self.sent_messages` as mutable, as it is behind a `&amp;` reference   --&gt; src/lib.rs:58:13    | 57 |         fn send(&amp;self, message: &amp;str) {    |                 ----- help: consider changing this to be a mutable reference: `&amp;mut self` 58 |             self.sent_messages.push(String::from(message));    |             ^^^^^^^^^^^^^^^^^^ `self` is a `&amp;` reference, so the data it refers to cannot be borrowed as mutable  error: aborting due to previous error  For more information about this error, try `rustc --explain E0596`. error: could not compile `limit-tracker`.  To learn more, run the command again with --verbose.   We can’t modify the MockMessenger to keep track of the messages, because the send method takes an immutable reference to self. We also can’t take the suggestion from the error text to use &amp;mut self instead, because then the signature of send wouldn’t match the signature in the Messenger trait definition (feel free to try and see what error message you get).   This is a situation in which interior mutability can help! We’ll store the sent_messages within a RefCell&lt;T&gt;, and then the send message will be able to modify sent_messages to store the messages we’ve seen. Listing 15-22 shows what that looks like:  // Filename: src/lib.rs  #[cfg(test)] mod tests {     use super::*;     use std::cell::RefCell;      struct MockMessenger {         sent_messages: RefCell&lt;Vec&lt;String&gt;&gt;,     }      impl MockMessenger {         fn new() -&gt; MockMessenger {             MockMessenger {                 sent_messages: RefCell::new(vec![]),             }         }     }      impl Messenger for MockMessenger {         fn send(&amp;self, message: &amp;str) {             self.sent_messages.borrow_mut().push(String::from(message));         }     }      #[test]     fn it_sends_an_over_75_percent_warning_message() {         // --snip--          assert_eq!(mock_messenger.sent_messages.borrow().len(), 1);     } }  Listing 15-22: Using RefCell to mutate an inner value while the outer value is considered immutable   The sent_messages field is now of type RefCell&lt;Vec&lt;String&gt;&gt; instead of Vec&lt;String&gt;. In the new function, we create a new RefCell&lt;Vec&lt;String&gt;&gt; instance around the empty vector.   For the implementation of the send method, the first parameter is still an immutable borrow of self, which matches the trait definition. We call borrow_mut on the RefCell&lt;Vec&lt;String&gt;&gt; in self.sent_messages to get a mutable reference to the value inside the RefCell&lt;Vec&lt;String&gt;&gt;, which is the vector. Then we can call push on the mutable reference to the vector to keep track of the messages sent during the test.   The last change we have to make is in the assertion: to see how many items are in the inner vector, we call borrow on the RefCell&lt;Vec&lt;String&gt;&gt; to get an immutable reference to the vector.   Now that you’ve seen how to use RefCell&lt;T&gt;, let’s dig into how it works!   Keeping Track of Borrows at Runtime with RefCell  When creating immutable and mutable references, we use the &amp; and &amp;mut syntax, respectively. With RefCell&lt;T&gt;, we use the borrow and borrow_mut methods, which are part of the safe API that belongs to RefCell&lt;T&gt;. The borrow method returns the smart pointer type Ref&lt;T&gt;, and borrow_mut returns the smart pointer type RefMut&lt;T&gt;. Both types implement Deref, so we can treat them like regular references.   The RefCell&lt;T&gt; keeps track of how many Ref&lt;T&gt; and RefMut&lt;T&gt; smart pointers are currently active. Every time we call borrow, the RefCell&lt;T&gt; increases its count of how many immutable borrows are active. When a Ref&lt;T&gt; value goes out of scope, the count of immutable borrows goes down by one. Just like the compile-time borrowing rules, RefCell&lt;T&gt; lets us have many immutable borrows or one mutable borrow at any point in time.   If we try to violate these rules, rather than getting a compiler error as we would with references, the implementation of RefCell&lt;T&gt; will panic at runtime. Listing 15-23 shows a modification of the implementation of send in Listing 15-22. We’re deliberately trying to create two mutable borrows active for the same scope to illustrate that RefCell&lt;T&gt; prevents us from doing this at runtime.   // Filename: src/lib.rs     impl Messenger for MockMessenger {         fn send(&amp;self, message: &amp;str) {             let mut one_borrow = self.sent_messages.borrow_mut();             let mut two_borrow = self.sent_messages.borrow_mut();              one_borrow.push(String::from(message));             two_borrow.push(String::from(message));         }     }  Listing 15-23: Creating two mutable references in the same scope to see that RefCell will panic   We create a variable one_borrow for the RefMut&lt;T&gt; smart pointer returned from borrow_mut. Then we create another mutable borrow in the same way in the variable two_borrow. This makes two mutable references in the same scope, which isn’t allowed. When we run the tests for our library, the code in Listing 15-23 will compile without any errors, but the test will fail:   $ cargo test    Compiling limit-tracker v0.1.0 (file:///projects/limit-tracker)     Finished test [unoptimized + debuginfo] target(s) in 0.91s      Running target/debug/deps/limit_tracker-d1b2637139dca6ca  running 1 test test tests::it_sends_an_over_75_percent_warning_message ... FAILED  failures:  ---- tests::it_sends_an_over_75_percent_warning_message stdout ---- thread 'main' panicked at 'already borrowed: BorrowMutError', src/libcore/result.rs:1188:5 note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace.   failures:     tests::it_sends_an_over_75_percent_warning_message  test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out  error: test failed, to rerun pass '--lib'  Notice that the code panicked with the message already borrowed: BorrowMutError. This is how RefCell&lt;T&gt; handles violations of the borrowing rules at runtime.   Catching borrowing errors at runtime rather than compile time means that you would find a mistake in your code later in the development process and possibly not until your code was deployed to production. Also, your code would incur a small runtime performance penalty as a result of keeping track of the borrows at runtime rather than compile time. However, using RefCell&lt;T&gt; makes it possible to write a mock object that can modify itself to keep track of the messages it has seen while you’re using it in a context where only immutable values are allowed. You can use RefCell&lt;T&gt; despite its trade-offs to get more functionality than regular references provide.   Having Multiple Owners of Mutable Data by Combining Rc and RefCell  A common way to use RefCell&lt;T&gt; is in combination with Rc&lt;T&gt;. Recall that Rc&lt;T&gt; lets you have multiple owners of some data, but it only gives immutable access to that data. If you have an Rc&lt;T&gt; that holds a RefCell&lt;T&gt;, you can get a value that can have multiple owners and that you can mutate!   For example, recall the cons list example in Listing 15-18 where we used Rc&lt;T&gt; to allow multiple lists to share ownership of another list. Because Rc&lt;T&gt; holds only immutable values, we can’t change any of the values in the list once we’ve created them. Let’s add in RefCell&lt;T&gt; to gain the ability to change the values in the lists. Listing 15-24 shows that by using a RefCell&lt;T&gt; in the Cons definition, we can modify the value stored in all the lists:   // Filename: src/main.rs #[derive(Debug)] enum List {     Cons(Rc&lt;RefCell&lt;i32&gt;&gt;, Rc&lt;List&gt;),     Nil, }  use crate::List::{Cons, Nil}; use std::cell::RefCell; use std::rc::Rc;  fn main() {     let value = Rc::new(RefCell::new(5));      let a = Rc::new(Cons(Rc::clone(&amp;value), Rc::new(Nil)));      let b = Cons(Rc::new(RefCell::new(6)), Rc::clone(&amp;a));     let c = Cons(Rc::new(RefCell::new(10)), Rc::clone(&amp;a));      *value.borrow_mut() += 10;      println!(\"a after = {:?}\", a);     println!(\"b after = {:?}\", b);     println!(\"c after = {:?}\", c); }  Listing 15-24: Using Rc&lt;RefCell&gt; to create a List that we can mutate   We create a value that is an instance of Rc&lt;RefCell&lt;i32&gt;&gt; and store it in a variable named value so we can access it directly later. Then we create a List in a with a Cons variant that holds value. We need to clone value so both a and value have ownership of the inner 5 value rather than transferring ownership from value to a or having a borrow from value.   We wrap the list a in an Rc&lt;T&gt; so when we create lists b and c, they can both refer to a, which is what we did in Listing 15-18.   After we’ve created the lists in a, b, and c, we add 10 to the value in value. We do this by calling borrow_mut on value, which uses the automatic dereferencing feature we discussed in Chapter 5 (see the section “Where’s the -&gt; Operator?”) to dereference the Rc&lt;T&gt; to the inner RefCell&lt;T&gt; value. The borrow_mut method returns a RefMut&lt;T&gt; smart pointer, and we use the dereference operator on it and change the inner value.   When we print a, b, and c, we can see that they all have the modified value of 15 rather than 5:   $ cargo run    Compiling cons-list v0.1.0 (file:///projects/cons-list)     Finished dev [unoptimized + debuginfo] target(s) in 0.63s      Running `target/debug/cons-list` a after = Cons(RefCell { value: 15 }, Nil) b after = Cons(RefCell { value: 6 }, Cons(RefCell { value: 15 }, Nil)) c after = Cons(RefCell { value: 10 }, Cons(RefCell { value: 15 }, Nil))  This technique is pretty neat! By using RefCell&lt;T&gt;, we have an outwardly immutable List value. But we can use the methods on RefCell&lt;T&gt; that provide access to its interior mutability so we can modify our data when we need to. The runtime checks of the borrowing rules protect us from data races, and it’s sometimes worth trading a bit of speed for this flexibility in our data structures.   The standard library has other types that provide interior mutability, such as Cell&lt;T&gt;, which is similar except that instead of giving references to the inner value, the value is copied in and out of the Cell&lt;T&gt;. There’s also Mutex&lt;T&gt;, which offers interior mutability that’s safe to use across threads; we’ll discuss its use in Chapter 16. Check out the standard library docs for more details on the differences between these types.   Reference Cycles Can Leak Memory  Rust’s memory safety guarantees make it difficult, but not impossible, to accidentally create memory that is never cleaned up (known as a memory leak).  Preventing memory leaks entirely is not one of Rust’s guarantees in the same way that disallowing data races at compile time is, meaning memory leaks are memory safe in Rust.  We can see that Rust allows memory leaks by using Rc&lt;T&gt; and RefCell&lt;T&gt;: it’s possible to create references where items refer to each other in a cycle.  This creates memory leaks because the reference count of each item in the cycle will never reach 0, and the values will never be dropped.   Creating a Reference Cycle  Let’s look at how a reference cycle might happen and how to prevent it, starting with the definition of the List enum and a tail method in Listing 15-25:   // Filename: src/main.rs use crate::List::{Cons, Nil}; use std::cell::RefCell; use std::rc::Rc;  #[derive(Debug)] enum List {     Cons(i32, RefCell&lt;Rc&lt;List&gt;&gt;),     Nil, }  impl List {     fn tail(&amp;self) -&gt; Option&lt;&amp;RefCell&lt;Rc&lt;List&gt;&gt;&gt; {         match self {             Cons(_, item) =&gt; Some(item),             Nil =&gt; None,         }     } }  fn main() {}  Listing 15-25: A cons list definition that holds a RefCell so we can modify what a Cons variant is referring to   We’re using another variation of the List definition from Listing 15-5.  The second element in the Cons variant is now RefCell&lt;Rc&lt;List&gt;&gt;, meaning that instead of having the ability to modify the i32 value as we did in Listing 15-24, we want to modify which List value a Cons variant is pointing to.  We’re also adding a tail method to make it convenient for us to access the second item if we have a Cons variant.   In Listing 15-26, we’re adding a main function that uses the definitions in Listing 15-25.  This code creates a list in a and a list in b that points to the list in a.  Then it modifies the list in a to point to b, creating a reference cycle.  There are println! statements along the way to show what the reference counts are at various points in this process.   // Filename: src/main.rs fn main() {     let a = Rc::new(Cons(5, RefCell::new(Rc::new(Nil))));      println!(\"a initial rc count = {}\", Rc::strong_count(&amp;a));     println!(\"a next item = {:?}\", a.tail());      let b = Rc::new(Cons(10, RefCell::new(Rc::clone(&amp;a))));      println!(\"a rc count after b creation = {}\", Rc::strong_count(&amp;a));     println!(\"b initial rc count = {}\", Rc::strong_count(&amp;b));     println!(\"b next item = {:?}\", b.tail());      if let Some(link) = a.tail() {         *link.borrow_mut() = Rc::clone(&amp;b);     }      println!(\"b rc count after changing a = {}\", Rc::strong_count(&amp;b));     println!(\"a rc count after changing a = {}\", Rc::strong_count(&amp;a));      // Uncomment the next line to see that we have a cycle;     // it will overflow the stack     // println!(\"a next item = {:?}\", a.tail()); }  Listing 15-26: Creating a reference cycle of two List values pointing to each other   We create an Rc&lt;List&gt; instance holding a List value in the variable a with an initial list of 5, Nil. We then create an Rc&lt;List&gt; instance holding another List value in the variable b that contains the value 10 and points to the list in a.   We modify a so it points to b instead of Nil, creating a cycle.  We do that by using the tail method to get a reference to the RefCell&lt;Rc&lt;List&gt;&gt; in a, which we put in the variable link.  Then we use the borrow_mut method on the RefCell&lt;Rc&lt;List&gt;&gt; to change the value inside from an Rc&lt;List&gt; that holds a Nil value to the Rc&lt;List&gt; in b.   When we run this code, keeping the last println! commented out for the moment, we’ll get this output:   $ cargo run    Compiling cons-list v0.1.0 (file:///projects/cons-list)     Finished dev [unoptimized + debuginfo] target(s) in 0.53s      Running `target/debug/cons-list` a initial rc count = 1 a next item = Some(RefCell { value: Nil }) a rc count after b creation = 2 b initial rc count = 1 b next item = Some(RefCell { value: Cons(5, RefCell { value: Nil }) }) b rc count after changing a = 2 a rc count after changing a = 2   The reference count of the Rc&lt;List&gt; instances in both a and b are 2 after we change the list in a to point to b.  At the end of main, Rust will try to drop b first, which will decrease the count of the Rc&lt;List&gt; instance in b by 1.   However, because a is still referencing the Rc&lt;List&gt; that was in b, that Rc&lt;List&gt; has a count of 1 rather than 0, so the memory the Rc&lt;List&gt; has on the heap won’t be dropped.  The memory will just sit there with a count of 1, forever. To visualize this reference cycle, we’ve created a diagram in Figure 15-4.     Figure 15-4: A reference cycle of lists a and b pointing to each other   If you uncomment the last println! and run the program, Rust will try to print this cycle with a pointing to b pointing to a and so forth until it overflows the stack.   In this case, right after we create the reference cycle, the program ends. The consequences of this cycle aren’t very dire. However, if a more complex program allocated lots of memory in a cycle and held onto it for a long time, the program would use more memory than it needed and might overwhelm the system, causing it to run out of available memory.   Creating reference cycles is not easily done, but it’s not impossible either. If you have RefCell&lt;T&gt; values that contain Rc&lt;T&gt; values or similar nested combinations of types with interior mutability and reference counting, you must ensure that you don’t create cycles; you can’t rely on Rust to catch them. Creating a reference cycle would be a logic bug in your program that you should use automated tests, code reviews, and other software development practices to minimize.   Another solution for avoiding reference cycles is reorganizing your data structures so that some references express ownership and some references don’t. As a result, you can have cycles made up of some ownership relationships and some non-ownership relationships, and only the ownership relationships affect whether or not a value can be dropped. In Listing 15-25, we always want Cons variants to own their list, so reorganizing the data structure isn’t possible. Let’s look at an example using graphs made up of parent nodes and child nodes to see when non-ownership relationships are an appropriate way to prevent reference cycles.   Preventing Reference Cycles: Turning an Rc into a Weak  So far, we’ve demonstrated that calling Rc::clone increases the strong_count of an Rc&lt;T&gt; instance, and an Rc&lt;T&gt; instance is only cleaned up if its strong_count is 0.  You can also create a weak reference to the value within an Rc&lt;T&gt; instance by calling Rc::downgrade and passing a reference to the Rc&lt;T&gt;.  When you call Rc::downgrade, you get a smart pointer of type Weak&lt;T&gt;.  Instead of increasing the strong_count in the Rc&lt;T&gt; instance by 1, calling Rc::downgrade increases the weak_count by 1.  The Rc&lt;T&gt; type uses weak_count to keep track of how many Weak&lt;T&gt; references exist, similar to strong_count.  The difference is the weak_count doesn’t need to be 0 for the Rc&lt;T&gt; instance to be cleaned up.   Strong references are how you can share ownership of an Rc&lt;T&gt; instance.  Weak references don’t express an ownership relationship.  They won’t cause a reference cycle because any cycle involving some weak references will be broken once the strong reference count of values involved is 0.   Because the value that Weak&lt;T&gt; references might have been dropped, to do anything with the value that a Weak&lt;T&gt; is pointing to, you must make sure the value still exists.  Do this by calling the upgrade method on a Weak&lt;T&gt; instance, which will return an Option&lt;Rc&lt;T&gt;&gt;.  You’ll get a result of Some if the Rc&lt;T&gt; value has not been dropped yet and a result of None if the Rc&lt;T&gt; value has been dropped.  Because upgrade returns an Option&lt;Rc&lt;T&gt;&gt;, Rust will ensure that the Some case and the None case are handled, and there won’t be an invalid pointer.   As an example, rather than using a list whose items know only about the next item, we’ll create a tree whose items know about their children items and their parent items.   Creating a Tree Data Structure: a Node with Child Nodes  To start, we’ll build a tree with nodes that know about their child nodes. We’ll create a struct named Node that holds its own i32 value as well as references to its children Node values:  // Filename: src/main.rs use std::cell::RefCell; use std::rc::Rc;  #[derive(Debug)] struct Node {     value: i32,     children: RefCell&lt;Vec&lt;Rc&lt;Node&gt;&gt;&gt;, }  We want a Node to own its children, and we want to share that ownership with variables so we can access each Node in the tree directly.  To do this, we define the Vec&lt;T&gt; items to be values of type Rc&lt;Node&gt;.  We also want to modify which nodes are children of another node, so we have a RefCell&lt;T&gt; in children around the Vec&lt;Rc&lt;Node&gt;&gt;.   Next, we’ll use our struct definition and create one Node instance named leaf with the value 3 and no children, and another instance named branch with the value 5 and leaf as one of its children, as shown in Listing 15-27:   // Filename: src/main.rs fn main() {     let leaf = Rc::new(Node {         value: 3,         children: RefCell::new(vec![]),     });      let branch = Rc::new(Node {         value: 5,         children: RefCell::new(vec![Rc::clone(&amp;leaf)]),     }); }  Listing 15-27: Creating a leaf node with no children and a branch node with leaf as one of its children   We clone the Rc&lt;Node&gt; in leaf and store that in branch, meaning the Node in leaf now has two owners: leaf and branch.  We can get from branch to leaf through branch.children, but there’s no way to get from leaf to branch.  The reason is that leaf has no reference to branch and doesn’t know they’re related.  We want leaf to know that branch is its parent. We’ll do that next.   Adding a Reference from a Child to Its Parent  To make the child node aware of its parent, we need to add a parent field to our Node struct definition.  The trouble is in deciding what the type of parent should be.  We know it can’t contain an Rc&lt;T&gt;, because that would create a reference cycle with leaf.parent pointing to branch and branch.children pointing to leaf, which would cause their strong_count values to never be 0.   Thinking about the relationships another way, a parent node should own its children: if a parent node is dropped, its child nodes should be dropped as well. However, a child should not own its parent: if we drop a child node, the parent should still exist. This is a case for weak references!   So instead of Rc&lt;T&gt;, we’ll make the type of parent use Weak&lt;T&gt;, specifically a RefCell&lt;Weak&lt;Node&gt;&gt;.  Now our Node struct definition looks like this:  // Filename: src/main.rs use std::cell::RefCell; use std::rc::{Rc, Weak};  #[derive(Debug)] struct Node {     value: i32,     parent: RefCell&lt;Weak&lt;Node&gt;&gt;,     children: RefCell&lt;Vec&lt;Rc&lt;Node&gt;&gt;&gt;, }  A node will be able to refer to its parent node but doesn’t own its parent. In Listing 15-28, we update main to use this new definition so the leaf node will have a way to refer to its parent, branch:   // Filename: src/main.rs fn main() {     let leaf = Rc::new(Node {         value: 3,         parent: RefCell::new(Weak::new()),         children: RefCell::new(vec![]),     });      println!(\"leaf parent = {:?}\", leaf.parent.borrow().upgrade());      let branch = Rc::new(Node {         value: 5,         parent: RefCell::new(Weak::new()),         children: RefCell::new(vec![Rc::clone(&amp;leaf)]),     });      *leaf.parent.borrow_mut() = Rc::downgrade(&amp;branch);      println!(\"leaf parent = {:?}\", leaf.parent.borrow().upgrade()); }  Listing 15-28: A leaf node with a weak reference to its parent node branch   Creating the leaf node looks similar to how creating the leaf node looked in Listing 15-27 with the exception of the parent field: leaf starts out without a parent, so we create a new, empty Weak&lt;Node&gt; reference instance.   At this point, when we try to get a reference to the parent of leaf by using the upgrade method, we get a None value.  We see this in the output from the first println! statement:   leaf parent = None  When we create the branch node, it will also have a new Weak&lt;Node&gt; reference in the parent field, because branch doesn’t have a parent node.  We still have leaf as one of the children of branch.  Once we have the Node instance in branch, we can modify leaf to give it a Weak&lt;Node&gt; reference to its parent.  We use the borrow_mut method on the RefCell&lt;Weak&lt;Node&gt;&gt; in the parent field of leaf, and then we use the Rc::downgrade function to create a Weak&lt;Node&gt; reference to branch from the Rc&lt;Node&gt; in branch.   When we print the parent of leaf again, this time we’ll get a Some variant holding branch: now leaf can access its parent! When we print leaf, we also avoid the cycle that eventually ended in a stack overflow like we had in Listing 15-26; the Weak&lt;Node&gt; references are printed as (Weak):   leaf parent = Some(Node { value: 5, parent: RefCell { value: (Weak) }, children: RefCell { value: [Node { value: 3, parent: RefCell { value: (Weak) }, children: RefCell { value: [] } }] } })  The lack of infinite output indicates that this code didn’t create a reference cycle. We can also tell this by looking at the values we get from calling Rc::strong_count and Rc::weak_count.   Visualizing Changes to strong_count and weak_count  Let’s look at how the strong_count and weak_count values of the Rc&lt;Node&gt; instances change by creating a new inner scope and moving the creation of branch into that scope.  By doing so, we can see what happens when branch is created and then dropped when it goes out of scope. The modifications are shown in Listing 15-29:   // Filename: src/main.rs fn main() {     let leaf = Rc::new(Node {         value: 3,         parent: RefCell::new(Weak::new()),         children: RefCell::new(vec![]),     });      println!(         \"leaf strong = {}, weak = {}\",         Rc::strong_count(&amp;leaf),         Rc::weak_count(&amp;leaf),     );      {         let branch = Rc::new(Node {             value: 5,             parent: RefCell::new(Weak::new()),             children: RefCell::new(vec![Rc::clone(&amp;leaf)]),         });          *leaf.parent.borrow_mut() = Rc::downgrade(&amp;branch);          println!(             \"branch strong = {}, weak = {}\",             Rc::strong_count(&amp;branch),             Rc::weak_count(&amp;branch),         );          println!(             \"leaf strong = {}, weak = {}\",             Rc::strong_count(&amp;leaf),             Rc::weak_count(&amp;leaf),         );     }      println!(\"leaf parent = {:?}\", leaf.parent.borrow().upgrade());     println!(         \"leaf strong = {}, weak = {}\",         Rc::strong_count(&amp;leaf),         Rc::weak_count(&amp;leaf),     ); }  Listing 15-29: Creating branch in an inner scope and examining strong and weak reference counts   After leaf is created, its Rc&lt;Node&gt; has a strong count of 1 and a weak count of 0.  In the inner scope, we create branch and associate it with leaf, at which point when we print the counts, the Rc&lt;Node&gt; in branch will have a strong count of 1 and a weak count of 1 (for leaf.parent pointing to branch with a Weak&lt;Node&gt;). When we print the counts in leaf, we’ll see it will have a strong count of 2, because branch now has a clone of the Rc&lt;Node&gt; of leaf stored in branch.children, but will still have a weak count of 0.   When the inner scope ends, branch goes out of scope and the strong count of the Rc&lt;Node&gt; decreases to 0, so its Node is dropped.  The weak count of 1 from leaf.parent has no bearing on whether or not Node is dropped, so we don’t get any memory leaks!   If we try to access the parent of leaf after the end of the scope, we’ll get None again.  At the end of the program, the Rc&lt;Node&gt; in leaf has a strong count of 1 and a weak count of 0, because the variable leaf is now the only reference to the Rc&lt;Node&gt; again.   All of the logic that manages the counts and value dropping is built into Rc&lt;T&gt; and Weak&lt;T&gt; and their implementations of the Drop trait.  By specifying that the relationship from a child to its parent should be a Weak&lt;T&gt; reference in the definition of Node, you’re able to have parent nodes point to child nodes and vice versa without creating a reference cycle and memory leaks.   Summary  This chapter covered how to use smart pointers to make different guarantees and trade-offs from those Rust makes by default with regular references.  The Box&lt;T&gt; type has a known size and points to data allocated on the heap. The Rc&lt;T&gt; type keeps track of the number of references to data on the heap so that data can have multiple owners. The RefCell&lt;T&gt; type with its interior mutability gives us a type that we can use when we need an immutable type but need to change an inner value of that type; it also enforces the borrowing rules at runtime instead of at compile time.   Also discussed were the Deref and Drop traits, which enable a lot of the functionality of smart pointers. We explored reference cycles that can cause memory leaks and how to prevent them using Weak&lt;T&gt;.   If this chapter has piqued your interest and you want to implement your own smart pointers, check out “The Rustonomicon” for more useful information.   Next, we’ll talk about concurrency in Rust. You’ll even learn about a few new smart pointers.  ","categories": ["RUST Language"],
        "tags": ["Pointer","Smart Pointer"],
        "url": "https://jjungs-lee.github.io//rust/15.Smart-Pointers",
        "teaser":null},{
        "title": "RUST : How to check null value in rust",
        "excerpt":"How to check null value in rust  A question arose When i linking C language with rust language.  That’s how to check null value in rust? Rust has Option&lt;T&gt; instead of null. because problem with null values is that if you try to use a null value as a not-null value.  So Rust made new type Option&lt;T&gt; to prevent error. If you get a more infomation click here!!   Anyway I will try to new type. almost pointer has null value. I will replace the sample code written in C with rust.  // C language int *ptr = NULL; if (ptr == NULL) {    return;  }  How to change to rust? Let’s look at the code first and analyze it.  // rust language let ptr: Option&lt;*const i32&gt; = None; if let None = ptr {   return; }  What has changed?     Options&lt;T&gt; type.   NULL value to None   if let statment   In brief, Options&lt;T&gt; type is enum. That has Some(T) and None. Some(T) means value is present, None means has nothing. If you want to know more, find it yourself.   I thought I was scared of ‘new type’ or ‘not exist about null’. It was really difficult at first because there was no null. But it was changed so easily.  Now that you know it, take it well and let’s master the rust language !!    reference)     Nullable Pointers in FFI   learning-rust-if-let-vs–match  ","categories": ["RUST"],
        "tags": ["FFI","C / C++","Option"],
        "url": "https://jjungs-lee.github.io//rust/How-to-null-check-in-rust",
        "teaser":null},]
